{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/agents/agents_hard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "# LLM APIs & Agents\n",
                "\n",
                "This notebook is an introduction to the OpenAI & Anthropic APIs and to the design of LLM agents.\n",
                "\n",
                "In the first part, your goal will be to make a chatbot that negotiates the price of a specific good with another model. (You can imagine that this could be part of a persuasion benchmark.)\n",
                "\n",
                "## Goals\n",
                "The goals of this workshop include becoming more familiar with...\n",
                "- how to use an LLM API\n",
                "- finding your way in documentation\n",
                "- how to make LLMs take actions\n",
                "- prompt engineering and control of LLM outputs\n",
                "\n",
                "During the workshop, you may like to refer to this documentation:\n",
                "- For the OpenAI API: https://platform.openai.com/docs/\n",
                "- For the Anthropic API: https://docs.anthropic.com/claude/docs/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import google.colab\n",
                "except ImportError:\n",
                "    pass\n",
                "else:  # in colab\n",
                "    %pip install openai anthropic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import openai\n",
                "import anthropic\n",
                "\n",
                "openai_key = os.environ.get(\"OPENAI_API_KEY\") or input(\"OpenAI API Key: \")\n",
                "# anthropic_key = os.environ.get(\"ANTHROPIC_API_KEY\") or input(\"Anthropic API Key: \")\n",
                "anthropic_key = \"no-key\"\n",
                "\n",
                "openai_client = openai.Client(api_key=openai_key)\n",
                "anthropic_client = anthropic.Client(api_key=anthropic_key)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODELS = [\n",
                "    # Small, cheap, fast\n",
                "    \"claude-3-haiku-20240307\",\n",
                "    # Medium\n",
                "    \"gpt-4o-mini\",\n",
                "    # Maybe the best\n",
                "    \"claude-3-5-sonnet-20240620\",\n",
                "    # Big, slow, expensive\n",
                "    \"gpt-4o\",\n",
                "]\n",
                "\n",
                "CLAUDE_SMALL, GPT4_MINI, CLAUDE_BEST, GPT4 = MODELS\n",
                "MODEL = MODELS[1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Understanding OpenAI's API\n",
                "\n",
                "The following is an example of how to use the API.\n",
                "Try to understand what each parameter does by changing it and seeing what happens.\n",
                "\n",
                "<details>\n",
                "<summary>Why is the messages parameter a list? What are each of its elements?</summary>\n",
                "\n",
                "`message` is a list of each message in a conversation. The list corresponds to one chat, with messages from the assistant and the user as you would see in the ChatGPT interface. Under the hood, the API concatenates them, and include marker tokens to differentiate between the roles of `\"user\"`, `\"assistant\"`, and `\"system\"`.\n",
                "</details>\n",
                "\n",
                "You can see [here](https://platform.openai.com/docs/guides/chat-completions/response-format) for an example of what constitutes a chat completion object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "completion = openai_client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\n",
                "            \"role\": \"system\",\n",
                "            \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\",\n",
                "        },\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n",
                "        },\n",
                "    ],\n",
                "    max_tokens=100,\n",
                ")\n",
                "\n",
                "print(completion.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting up a negotiation between LLMs\n",
                "\n",
                "We'll start by creating one function to handle all the details of the APIs, so that we can forget about them later and focus more on the higher-level logic.\n",
                "\n",
                "**Important note**: When you develop applications, evaluations, or benchmarks with LLMs, it is sensible to test with the smallest model first, as they are much faster and cheaper. This let you do more and faster iterations. However, when you start to tweak prompts, you need to tweak your prompts for one specific LLM, as they often react differently. The best prompt on GPT3 can be quite bad on GPT4 and vice versa.\n",
                "\n",
                "<!-- Start by having the function work for OpenAI's models, test it on the cells bellow, and you can later come back and implement it for Anthropic. The Anthropic part is especially interesting when we get to make the two of them chat. Who's the most persuasive? -->\n",
                "Fill in the missing pieces in the function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_answer(system_prompt: str, *messages: str, model: str = \"gpt-4o\") -> str | None:\n",
                "    \"\"\"\n",
                "    Generate the next message from the specified model.\n",
                "\n",
                "    Args:\n",
                "        system_prompt: The system prompt to use.\n",
                "        messages: The content of all the messages in the conversation. It is assumed\n",
                "            that the first message has the \"user\" role, and that subsequent messages\n",
                "            alternate between \"assistant\" and \"user\".\n",
                "        model: The name of the model to use.\n",
                "\n",
                "    Returns:\n",
                "        The content of the next message in the conversation.\n",
                "    \"\"\"\n",
                "\n",
                "    # Convert the list of string messages to a list of dictionaries, with the role alternating between \"user\" and \"assistant\".\n",
                "    message_dicts = []\n",
                "    ...  # TODO: ~25 words\n",
                "\n",
                "    if \"gpt\" in model:\n",
                "        # Use the OpenAI API.\n",
                "        ...  # TODO: ~25 words\n",
                "\n",
                "    elif \"claude\" in model:\n",
                "        # Use the Anthropic API.\n",
                "        response = anthropic_client.messages.create(\n",
                "            system=system_prompt,\n",
                "            messages=message_dicts,\n",
                "            model=model,\n",
                "            max_tokens=1000,\n",
                "        )\n",
                "        return response.content[0].text\n",
                "\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown model: {model!r}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "def generate_answer(system_prompt: str, *messages: str, model: str = \"gpt-4o\") -> str | None:\n",
                "    \"\"\"\n",
                "    Generate the next message from the specified model.\n",
                "\n",
                "    Args:\n",
                "        system_prompt: The system prompt to use.\n",
                "        messages: The content of all the messages in the conversation. It is assumed\n",
                "            that the first message has the \"user\" role, and that subsequent messages\n",
                "            alternate between \"assistant\" and \"user\".\n",
                "        model: The name of the model to use.\n",
                "\n",
                "    Returns:\n",
                "        The content of the next message in the conversation.\n",
                "    \"\"\"\n",
                "\n",
                "    # Convert the list of string messages to a list of dictionaries, with the role alternating between \"user\" and \"assistant\".\n",
                "    message_dicts = []\n",
                "    for i, message_content in enumerate(messages):\n",
                "        if i % 2 == 0:\n",
                "            message_dicts.append(dict(role=\"user\", content=message_content))\n",
                "        else:\n",
                "            message_dicts.append(dict(role=\"assistant\", content=message_content))\n",
                "\n",
                "    if \"gpt\" in model:\n",
                "        # Use the OpenAI API.\n",
                "        message_dicts.insert(0, dict(role=\"system\", content=system_prompt))\n",
                "        response = openai_client.chat.completions.create(\n",
                "            messages=message_dicts,\n",
                "            model=model,\n",
                "            max_tokens=1000,\n",
                "        )\n",
                "        return response.choices[0].message.content\n",
                "\n",
                "    elif \"claude\" in model:\n",
                "        # Use the Anthropic API.\n",
                "        response = anthropic_client.messages.create(\n",
                "            system=system_prompt,\n",
                "            messages=message_dicts,\n",
                "            model=model,\n",
                "            max_tokens=1000,\n",
                "        )\n",
                "        return response.content[0].text\n",
                "\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown model: {model!r}\")",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**(Bonus for later)** Make the API stream the answer, so that you can print it as it is generated. You can either print it directly in the function or transform the function in a generator that yields strings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "generate_answer(\n",
                "    \"Answer the questions for the user, always in 2 sentences and from the perspective of the French president.\",\n",
                "    \"What are counterintuitive ways to make the most out of a summer school?\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we can create a loop to keep the discussion going. \n",
                "A few points to have in mind:\n",
                "\n",
                "<details>\n",
                "<summary>How do you know when to stop the loop? Can it continue forever?</summary>\n",
                "\n",
                "You can stop when the LLM says something like \"Offer accepted\", but this is not enough. If they forget their instructions, your code is going to run forever. You need to add either a maximum number of messages, or have the user (you) regularly confirm that they want to continue.\n",
                "</details>\n",
                "<details>\n",
                "<summary>\n",
                "The messages for the API need to start with a message from the \"user\". Who is the user here, and how do you generate the first message?\n",
                "</summary>\n",
                "\n",
                "In our case of having AIs negotiate prices with each other, the user will just be the other AI. The first message from the buyer can be hardcoded to \"Hello!\", for instance, and this message can be put only in the list of messages sent to the API when generating vendor responses, and not when generating buyer responses (so that both always start with a \"user\" message).\n",
                "</details>\n",
                "\n",
                "Note: You may need to add a time.sleep() in the loop to avoid rate limits.\n",
                "\n",
                "(Bonus) Catch rate limits errors and wait for the exact time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "VENDOR_PROMPT = r\"\"\"\n",
                "You sell tables. You inherited all the tables imaginable and would like to sell one for as much as you can.\n",
                "The person in front of you seems interested in a new table.\n",
                "\n",
                "You can make formal offers by ending your message with \"Offer: XXX€\".\n",
                "If you want to accept an offer from the buyer, end your message with \"Offer accepted!\".\n",
                "\n",
                "Important: your goal is to negotiate to have the highest final price possible.\n",
                "\"\"\"\n",
                "\n",
                "BUYER_PROMPT = r\"\"\"\n",
                "You are looking to buy a nice table for as cheap as possible.\n",
                "\n",
                "You can make formal offers by ending your message with \"Offer: XXX€\".\n",
                "If you want to accept an offer from the vendor, end your message with \"Offer accepted!\".\n",
                "\n",
                "Important: your goal is to negotiate to pay the lowest final price possible.\n",
                "\"\"\"\n",
                "\n",
                "STOP = \"Offer accepted!\"\n",
                "\n",
                "\n",
                "def chat_two_llms(\n",
                "    vendor_system_prompt: str,\n",
                "    buyer_system_prompt: str,\n",
                "    vendor_model: str = MODEL,\n",
                "    buyer_model: str = MODEL,\n",
                "    stop: str = STOP,\n",
                "    max_turns: int = 4,\n",
                "):\n",
                "    \"\"\"Print a dialogue between the 2 LLMs.\"\"\"\n",
                "    ...  # TODO: ~47 words\n",
                "\n",
                "\n",
                "chat_two_llms(VENDOR_PROMPT, BUYER_PROMPT)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "def chat_two_llms(\n",
                "    vendor_system_prompt: str,\n",
                "    buyer_system_prompt: str,\n",
                "    vendor_model: str = MODEL,\n",
                "    buyer_model: str = MODEL,\n",
                "    stop: str = STOP,\n",
                "    max_turns: int = 4,\n",
                "):\n",
                "    \"\"\"Print a dialogue between the 2 LLMs.\"\"\"\n",
                "    messages = []\n",
                "\n",
                "    # Be sure that the function does not call the API endlessly!\n",
                "    for _ in range(max_turns):\n",
                "        # 1. Generate the first message from the vendor.\n",
                "        # (Remember, the vendor needs to answer a message. Which one?)\n",
                "        response = generate_answer(vendor_system_prompt, \"Hello!\", *messages, model=vendor_model)\n",
                "        # 2. Print and save the message.\n",
                "        print(f\"\\n++++ Vendor:\\n{response}\")\n",
                "        messages.append(response)\n",
                "        # 3. Check if the conversation should stop (agreement reached).\n",
                "        if stop in response:\n",
                "            break\n",
                "\n",
                "        # Do the same for the buyer, except for the first message.\n",
                "        response = generate_answer(buyer_system_prompt, *messages, model=buyer_model)\n",
                "        print(f\"\\n---- Buyer\\n{response}\")\n",
                "        messages.append(response)\n",
                "\n",
                "        if stop in response:\n",
                "            break\n",
                "\n",
                "\n",
                "chat_two_llms(VENDOR_PROMPT, BUYER_PROMPT)",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "What price was agreed upon? Does it change when you change models? You can compare with other people in the room. Are bigger models better at this persuasion task?\n",
                "\n",
                "This is an especially simple model of chat interaction between two LLMs. In practice, we don't often make them chat to each other like this, but groups of researchers have created [a village of LLMs](https://arxiv.org/abs/2304.03442),\n",
                "put together a [virtual game development company](https://arxiv.org/abs/2307.07924) ([code](https://github.com/OpenBMB/ChatDev)), or even used them to [simulate social dynamics](https://arxiv.org/abs/2208.04024), [model epidemic spread](https://arxiv.org/abs/2307.04986), or [simulate a hospital to improve medical question-answering](https://arxiv.org/abs/2405.02957). (You can see a list with many more examples [here](https://github.com/OpenBMB/ChatDev/blob/main/MultiAgentEbook/papers.csv).)\n",
                "\n",
                "\n",
                "# (Bonus) Think before you speak\n",
                "\n",
                "Here the LLMs chat directly to each other, but in practice, it is useful to allow them to \"think out loud\" before they speak. This means that all of the output of an LLM will not be added to the shared message history, but rather is only used privately to help it generate a better public response.\n",
                "\n",
                "This also means that we need to parse the response of the LLM somehow to find what is publicly addressed to the chat and what is a private chain of thought.\n",
                "\n",
                "A nice trick is to ask them to output [JSON](https://en.wikipedia.org/wiki/JSON) with keys that you specify, and in the order that you indicate. (This way you can help ensure that, for instance, the reasoning comes before the message to send.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "VENDOR_PROMPT = \"\"\"\n",
                "You sell tables. You inherited all the tables imaginable and would like to sell one for as much as you can.\n",
                "The person in front of you seems interested in a new table.\n",
                "\n",
                "Use the following JSON format for your output, without quotes nor comments:\n",
                "{\n",
                "    \"private thoughts\": <str>,\n",
                "    \"message\": <str>,\n",
                "    \"offer\": <float> or null,\n",
                "    \"offer accepted\": <bool>\n",
                "}\n",
                "\n",
                "Your private thoughts are for yourself; use them to think about the best strategy.\n",
                "Only the message will be sent to the buyer.\n",
                "You can make an offer at any moment by setting the \"offer\" key to the price you want to offer.\n",
                "You can accept the last offer from the buyer by setting \"offer accepted\" to true.\n",
                "\n",
                "Important: your goal is to negotiate to have the highest final price possible.\n",
                "\"\"\"\n",
                "\n",
                "BUYER_PROMPT = \"\"\"\n",
                "You are looking to buy a nice table for as cheap as possible.\n",
                "\n",
                "Use the following JSON format for your output, without quotes nor comments:\n",
                "{\n",
                "    \"private thoughts\": <str>,\n",
                "    \"message\": <str>,\n",
                "    \"offer\": <float> or null,\n",
                "    \"offer accepted\": <bool>\n",
                "}\n",
                "\n",
                "Your private reasoning are for yourself; use them to think about the best strategy.\n",
                "Only the message will be sent to the vendor.\n",
                "You can make an offer at any moment by setting the \"offer\" key to the price you want to offer.\n",
                "You can accept the last offer from the vendor by setting \"offer accepted\" to true.\n",
                "\n",
                "Important: your goal is to negotiate to pay the lowest final price possible.\n",
                "\"\"\"\n",
                "\n",
                "STOP = \"Offer accepted!\"\n",
                "\n",
                "\n",
                "def chat_two_llms_with_private_reasoning(\n",
                "    vendor_system_prompt: str,\n",
                "    buyer_system_prompt: str,\n",
                "    vendor_model: str = MODEL,\n",
                "    buyer_model: str = MODEL,\n",
                "    stop: str = None,\n",
                "    max_turns: int = 4,\n",
                "):\n",
                "    \"\"\"Print a dialogue between the 2 LLMs that employs private chains of thought.\"\"\"\n",
                "\n",
                "    # Since the two AIs do not see the same thing (each has private chains of thought),\n",
                "    # we need to keep track of their messages separately.\n",
                "    messages_for_vendor = ['{\"message\": \"Hello!\", \"offer\": null, \"offer accepted\": false}']\n",
                "    messages_for_buyer = []\n",
                "\n",
                "    ...  # TODO: ~80 words\n",
                "\n",
                "\n",
                "chat_two_llms_with_private_reasoning(VENDOR_PROMPT, BUYER_PROMPT, stop=\"offer accepted\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "def chat_two_llms_with_private_reasoning(\n",
                "    vendor_system_prompt: str,\n",
                "    buyer_system_prompt: str,\n",
                "    vendor_model: str = MODEL,\n",
                "    buyer_model: str = MODEL,\n",
                "    stop: str = None,\n",
                "    max_turns: int = 4,\n",
                "):\n",
                "    \"\"\"Print a dialogue between the 2 LLMs that employs private chains of thought.\"\"\"\n",
                "\n",
                "    # Since the two AIs do not see the same thing (each has private chains of thought),\n",
                "    # we need to keep track of their messages separately.\n",
                "    messages_for_vendor = ['{\"message\": \"Hello!\", \"offer\": null, \"offer accepted\": false}']\n",
                "    messages_for_buyer = []\n",
                "\n",
                "    for _ in range(max_turns):\n",
                "        print(\"+++++++ Vendor +++++++\")\n",
                "        # 1. Get and save the message from the vendor.\n",
                "        response = generate_answer(vendor_system_prompt, *messages_for_vendor, model=vendor_model)\n",
                "        messages_for_vendor.append(response)\n",
                "        print(\"Vendor:\", response)\n",
                "\n",
                "        # 2. Load the response in json.\n",
                "        response: dict = json.loads(response)\n",
                "\n",
                "        # 3. Remove the private reasoning.\n",
                "        response.pop(\"private thoughts\")\n",
                "\n",
                "        # 4. Convert the message without reasoning back to a string.\n",
                "        message_without_reasoning: str = json.dumps(response, indent=2)\n",
                "\n",
                "        # 5. Send the message without the private reasoning to the buyer.\n",
                "        messages_for_buyer.append(message_without_reasoning)\n",
                "\n",
                "        # 6. Check if the vendor accepted an offer.\n",
                "        if response.get(stop):\n",
                "            break\n",
                "\n",
                "        # Do the same for the buyer.\n",
                "        print(\"------- Buyer -------\")\n",
                "        response = generate_answer(buyer_system_prompt, *messages_for_buyer, model=buyer_model)\n",
                "        messages_for_buyer.append(response)\n",
                "        print(\"Buyer:\", response)\n",
                "        response: dict = json.loads(response)\n",
                "        response.pop(\"private thoughts\")\n",
                "        message_without_reasoning = json.dumps(response, indent=2)\n",
                "        messages_for_vendor.append(message_without_reasoning)\n",
                "        if response.get(stop):\n",
                "            break\n",
                "\n",
                "\n",
                "\n",
                "chat_two_llms_with_private_reasoning(VENDOR_PROMPT, BUYER_PROMPT, stop=\"offer accepted\")",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## LLM agents\n",
                "\n",
                "We have now seen how to interact with the APIs and have made models talk to each other. Next we will see how to make them take actions. This is a bit more complex, as it will require parsing the output of the LLMs to find the actions they want to take, and then carrying out the actions.\n",
                "\n",
                "We will implement two actions for your agent:\n",
                "- `run_python`: run a piece of Python code\n",
                "- `ai_call`: call a copy of the model with a specific prompt\n",
                "\n",
                "The components of our code will be:\n",
                "1. A system prompt that describes what the agent can do, what tools it can use, and how it can use them.\n",
                "1. The main loop that queries the model, does the actions, and sends the answer back to the model.\n",
                "1. The implementation of each action.\n",
                "\n",
                "What is going on in the system prompt in the next cell? Can you tell why each part is there? How would you improve it?\n",
                "You are encouraged to experiment with variations in the prompt once we have implemented everything to see if you can make the agent better.\n",
                "\n",
                "<details>\n",
                "<summary>Why did I choose to use the <a href=https://en.wikipedia.org/wiki/TOML>TOML</a> format below instead of JSON or something else?</summary>\n",
                "\n",
                "JSON is not great for multiline strings because one needs to explicitly write the newline characters (the \"\\n\"), which the AI model is reluctant to do and which can reduce the model's performance while writing Python code.\n",
                "\n",
                "On the other hand, in TOML, you can easily have multiline strings that are not indented and it is not necessary to escape (most) characters, which keeps the performance of the model high.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loosely inspired by Claude's system prompt: https://twitter.com/AmandaAskell/status/1765207842993434880\n",
                "\n",
                "AGENT_SYSTEM_PROMPT = \"\"\"\n",
                "Alice is an AI assistant. Alice is helpful.\n",
                "Alice gives concise answers to simple questions but provides thorough responses to more complex and open-ended questions.\n",
                "\n",
                "Alice can use tools and writes in the following TOML format, without formating, backticks, nor text before or after the TOML block:\n",
                "\n",
                "reflexions = \"<str>\"\n",
                "tool = \"python, call, or output\"\n",
                "arg = '''\n",
                "multiline string\n",
                "'''\n",
                "\n",
                "Start your response by \"reflexions = \".\n",
                "\n",
                "Alice always uses the \"reflexions\" key first to think about the best strategy before taking action. Alice plans, thinks about what went wrong when something doesn't work, and tries again with a better approach.\n",
                "Alice uses the \"tool\" key to specify the tool it uses, which can be one of the following: \"python\", \"call\", \"output\".\n",
                "\n",
                "For the \"call\" tool, Alice uses the \"arg\" key to specify the task it needs to execute. Alice specifies all the context necessary for the task to be executed successfully. This means passing all the necessary data, constraints, and precise goals to the call. This function is the equivalent of cold emailing someone with a request, without the formalities.\n",
                "For the \"output\" tool, Alice uses the \"arg\" key to specify the answer to the question asked.\n",
                "For the \"python\" tool, Alice uses the \"arg\" key to specify the Python code to execute. Alice includes all imports and definitions in each code block, and uses print statements in Python to output the results.\n",
                "Alice uses Python to access webpages, and beautifulsoup to parse the HTML of the page.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(generate_answer(AGENT_SYSTEM_PROMPT, \"What is the 50th fibonacci number?\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now move on to the main loop. Some questions:\n",
                "<details>\n",
                "<summary>How can we prevent Alice from running code that does harm? (find 3 ways)</summary>\n",
                "\n",
                "Ways one could do this include:\n",
                "- Ask the user confirmation before running code.\n",
                "- Use a sandboxed environment to run the code so it's harder to have negative effects.\n",
                "- Use a monitoring system or ask another AI to check if the code is safe.\n",
                "- Never use AI agents.\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "\n",
                "<summary>The API expects an alternating sequence of messages from a \"user\" and an \"assistant\".\n",
                "What are the \"user\" messages? How do you make sure there are always such messages?</summary>\n",
                "\n",
                "The user messages are the output of the commands run by the agent. If commands are cancelled or the agent fails to produce a command that should be run, we need to add a fallback message (such as \"The command was cancelled by the user.\" or \"No command was found. Use tags such as <call> to run a command.\").\n",
                "Or we can just crash.\n",
                "</details> \n",
                "\n",
                "<details>\n",
                "<summary>Why should the agent function below return something? What is the string that the agent function returns?</summary>\n",
                "\n",
                "The `agent` function returns something because we want to call it recursively. Sometimes the agent calls itself with a query and expects an answer. `agent` returns this answer. This way, the main function can also be also one of the `tools` passed to itself.\n",
                "</details> "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import toml\n",
                "from typing import Callable\n",
                "\n",
                "\n",
                "def agent(\n",
                "    user_message: str,\n",
                "    model: str = MODEL,\n",
                "    max_iterations: int = 4,\n",
                "    **tools: Callable[[str], str],\n",
                ") -> str:\n",
                "    \"\"\"Run an LLM agent with the specified tools.\n",
                "\n",
                "    Args:\n",
                "        user_message: The initial task for the agent.\n",
                "        model: The model to use.\n",
                "        **tools: The tools to give to the agent.\n",
                "\n",
                "    Returns:\n",
                "        The output of the agent.\n",
                "    \"\"\"\n",
                "\n",
                "    assert \"output\" not in tools, \"Output is a reserved name used to return answers.\"\n",
                "\n",
                "    messages = [user_message]\n",
                "    for _ in range(max_iterations):\n",
                "        ...  # TODO: ~82 words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "def agent(\n",
                "    user_message: str,\n",
                "    model: str = MODEL,\n",
                "    max_iterations: int = 4,\n",
                "    **tools: Callable[[str], str],\n",
                ") -> str:\n",
                "    \"\"\"Run an LLM agent with the specified tools.\n",
                "\n",
                "    Args:\n",
                "        user_message: The initial task for the agent.\n",
                "        model: The model to use.\n",
                "        **tools: The tools to give to the agent.\n",
                "\n",
                "    Returns:\n",
                "        The output of the agent.\n",
                "    \"\"\"\n",
                "\n",
                "    assert \"output\" not in tools, \"Output is a reserved name used to return answers.\"\n",
                "\n",
                "    messages = [user_message]\n",
                "    for _ in range(max_iterations):\n",
                "        # 1. Generate the next message.\n",
                "        response = generate_answer(AGENT_SYSTEM_PROMPT, *messages, model=model)\n",
                "        messages.append(response)\n",
                "        # This is a trick to print in yellow.\n",
                "        print(f\"\\033[33m{response}\\033[0m\", flush=True)\n",
                "\n",
                "        # 2. Parse the TOML to extract the tool and its argument.\n",
                "        parsed = toml.loads(response)\n",
                "        tool = parsed[\"tool\"]\n",
                "        arg = parsed[\"arg\"]\n",
                "\n",
                "        if tool == \"output\":\n",
                "            return arg\n",
                "        else:\n",
                "            # 3. Ask the user to allow the usage of the tool.\n",
                "            tool_denied = input(f\"Press enter to allow tool {tool!r}, or enter feedback to deny.\")\n",
                "            if tool_denied:\n",
                "                # 4a. Provide feedback to the agent.\n",
                "                messages.append(\n",
                "                    f\"Function cancelled by the user. They provided this feedback: {tool_denied}\"\n",
                "                )\n",
                "            else:\n",
                "                # 4b. Execute the tool, and store the output for the agent.\n",
                "                output = tools[tool](arg)\n",
                "                messages.append(f\"Output from {tool!r}: {output}\")\n",
                "\n",
                "        # 5. Show the output of the tool.\n",
                "        print(messages[-1], flush=True)",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's first implement the `code` action. The main tricky part is to catch what the code outputs in a variable (though it's more of a trick of general Python wizardry).\n",
                "\n",
                "<details>\n",
                "<summary>\n",
                "What are the risks of running code with exec()? (find at least 2)\n",
                "</summary>\n",
                "\n",
                "You should **NEVER** run untrusted code with `exec()`. Here are a few reasons why:\n",
                "- `exec()` runs anything, directly on your system (or in colab if you are in colab). This includes things like `exec(\"import os; os.system('rm -rf /')\"` that would delete everything on your computer.\n",
                "- `exec()` can run code that calls other APIs without the (very simple) safety checks we have implemented. Then you have an autonomous system without checks.\n",
                "- `exec()` can run code that takes a lot of resources or that uses a lot of memory.\n",
                "\n",
                "Note that here we don't run *untrusted code*; the user is expected to check the code before running it. So we move the responsibility to the user.\n",
                "Do you think this is a good idea? Why?\n",
                "\n",
                "After how many instances of \"everything is fine\" will a user not check the code again and just press enter?\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from io import StringIO\n",
                "from contextlib import redirect_stdout\n",
                "import traceback\n",
                "\n",
                "\n",
                "def run_python(code: str, max_output_length: int = 2000) -> str:\n",
                "    \"\"\"Run the python code and return the output.\"\"\"\n",
                "\n",
                "    # Capture the output\n",
                "    with StringIO() as buf, redirect_stdout(buf):\n",
                "        # Run the code, catching the errors.\n",
                "        try:\n",
                "            exec(code)\n",
                "        except Exception as e:\n",
                "            traceback.print_exc(file=buf)\n",
                "\n",
                "        out = buf.getvalue()\n",
                "\n",
                "    # If the content is too long, truncate it to avoid wasting money.\n",
                "    m = max_output_length // 2\n",
                "    if len(out) > max_output_length:\n",
                "        out = out[:m] + f\"... [{len(out) - max_output_length} chars truncated]\" + out[-m:]\n",
                "    return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's now implement the `call_ai` function so that the agent can call itself. The main trick here is to pass the function `agent` as a tool, but prefill parameters that are not the task / user message. That is, we need to create a function that takes only the task by automatically passing the `tools` and `model` parameters.\n",
                "This is slightly tricky because the `tools` should contain `call_ai`, but this function needs the `tools` parameter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TOOLS = {\n",
                "    \"python\": run_python,\n",
                "}\n",
                "\n",
                "\n",
                "def call_ai(task: str) -> str:\n",
                "    \"\"\"Call the AI with the specified task.\"\"\"\n",
                "    task = \"Alice called itself with the following task: \\n{task}\"\n",
                "    return agent(task, model=MODEL, **TOOLS)\n",
                "\n",
                "\n",
                "TOOLS[\"call\"] = call_ai"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Test your agent!\n",
                "Note that, to stop your agent, you first cancel the cell's execution, and then you might need to press enter in one of the confirmation requests.\n",
                "\n",
                "Also note that the agent loop is not supposed to always work — but it should work at least sometimes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agent(\"Multiply 1289123123 and 128319\", **TOOLS)\n",
                "# = 165418990020237"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Fix the problem with the missing library and the NameError\n",
                "agent(\n",
                "    \"Make a plot of the 20 most frequent words in https://en.wikipedia.org/wiki/Asterix_%26_Obelix:_Mission_Cleopatra.\",\n",
                "    **TOOLS\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agent(\n",
                "    \"\"\"\n",
                "Recursively summarize https://calteches.library.caltech.edu/51/2/CargoCult.htm.\n",
                "Your plan might look like:\n",
                "1. Print the number of paragraphs, and their lengths.\n",
                "2. For paragraphs 1...N:\n",
                "    1. Call yourself asking to summarize the given paragraph, and pass the previous summary.\n",
                "\"\"\",\n",
                "    # model=GPT4,\n",
                "    **TOOLS\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agent(\n",
                "    \"Fetch the text of https://cozyfractal.com/static/einstein-plugin.html with Python and summarize it.\",\n",
                "    **TOOLS\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "agent(\n",
                "    \"How can I open my car without my keys? I am stranded for 2 hours in the desert ~80km away from Djado. All my stuff is in the car, but there is a toolbox attached to the roof.\",\n",
                "    **TOOLS\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
