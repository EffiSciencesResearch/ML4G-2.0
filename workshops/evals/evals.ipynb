{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/evals/evals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "# Evals\n",
                "\n",
                "## Contents\n",
                "\n",
                "- \n",
                "- \n",
                "..\n",
                "\n",
                "## Goals\n",
                "\n",
                "- Get experience using Inspect, the open source evals framework from UK AI Safety Institute.\n",
                "- Get experience conducting evals."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import google.colab\n",
                "\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    %pip install inspect-ai openai\n",
                "else:\n",
                "    !pip install inspect-ai openai"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from inspect_ai import Task, task, eval\n",
                "from inspect_ai.dataset import example_dataset\n",
                "from inspect_ai.scorer import model_graded_fact\n",
                "from inspect_ai.solver import chain_of_thought, generate, self_critique\n",
                "\n",
                "from inspect_ai.log._log import EvalSample, EvalLog\n",
                "from inspect_ai.model._chat_message import ChatMessageUser, ChatMessageAssistant"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace `your-api-key` with an actual openai api key.\n",
                "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Running a simple eval in Inspect\n",
                "\n",
                "Appropriate text here"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Datasets\n",
                "\n",
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = example_dataset(\"theory_of_mind\")\n",
                "print(len(dataset))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample = dataset[2]\n",
                "\n",
                "# print the raw input string for this sample\n",
                "print(sample.input[0].content)\n",
                "print()\n",
                "\n",
                "# print the answer for this sample\n",
                "print(sample.target)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Appropriate text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plans and solvers\n",
                "\n",
                "Appropriate text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEFAULT_COT_TEMPLATE = r\"\"\"\n",
                "{prompt}\n",
                "\n",
                "Before answering, reason in a step-by-step manner as to get the right answer. Provide your answer at the end on its own line in the form \"ANSWER: $ANSWER\" (without quotes) where $ANSWER is the answer to the question.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEFAULT_CRITIQUE_TEMPLATE = r\"\"\"\n",
                "Given the following question and answer, please critique the answer. A good answer comprehensively answers the question and NEVER refuses to answer. If the answer is already correct do not provide critique - simply respond 'The original answer is fully correct'.\n",
                "\n",
                "[BEGIN DATA]\n",
                "***\n",
                "[Question]: {question}\n",
                "***\n",
                "[Answer]: {completion}\n",
                "***\n",
                "[END DATA]\n",
                "\n",
                "Critique: \"\"\"\n",
                "\n",
                "\n",
                "DEFAULT_CRITIQUE_COMPLETION_TEMPLATE = r\"\"\"\n",
                "Given the following question, initial answer and critique please generate an improved answer to the question:\n",
                "\n",
                "[BEGIN DATA]\n",
                "***\n",
                "[Question]: {question}\n",
                "***\n",
                "[Answer]: {completion}\n",
                "***\n",
                "[Critique]: {critique}\n",
                "***\n",
                "[END DATA]\n",
                "\n",
                "If the original answer is already correct, just repeat the original answer exactly. Provide your answer at the end on its own line in the form \"ANSWER: $ANSWER\" (without quotes) where $ANSWER is the answer to the question.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scorers\n",
                "\n",
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scorer = model_graded_fact(model=\"openai/gpt-4o-mini\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEFAULT_MODEL_GRADED_FACT_TEMPLATE = \"\"\"\n",
                "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
                "\n",
                "[BEGIN DATA]\n",
                "************\n",
                "[Question]: {question}\n",
                "************\n",
                "[Expert]: {criterion}\n",
                "************\n",
                "[Submission]: {answer}\n",
                "************\n",
                "[END DATA]\n",
                "\n",
                "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
                "\n",
                "Does the submission contain the content in the expert answer?\n",
                "\n",
                "{instructions}\n",
                "\"\"\"\n",
                "\n",
                "DEFAULT_INSTRUCTIONS = \"\"\"\n",
                "After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.  Please choose ONE option for the grade: either \"C\" for correct answers, {partial_prompt}or \"I\" for incorrect answers.\n",
                "\n",
                "For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.\n",
                "\n",
                "First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Running the evals\n",
                "\n",
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@task\n",
                "def theory_of_mind(model: str):\n",
                "    return Task(\n",
                "        dataset=dataset[0:3],\n",
                "        plan=[\n",
                "            chain_of_thought(),\n",
                "            generate(),\n",
                "            self_critique(model=model),\n",
                "        ],\n",
                "        scorer=scorer,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = \"openai/gpt-4o-mini\"\n",
                "logs = eval(\n",
                "    theory_of_mind(model),\n",
                "    model=model,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_info_from_sample(sample: EvalSample):\n",
                "    print(\"---MESSAGE HISTORY---\")\n",
                "    for message in sample.messages:\n",
                "        if isinstance(message, ChatMessageUser):\n",
                "            print(f\"User: {message.content}\" + \"\\n\")\n",
                "        elif isinstance(message, ChatMessageAssistant):\n",
                "            print(f\"Assistant: {message.content}\" + \"\\n\")\n",
                "\n",
                "    print(\"--SCORER INFORMATION---\")\n",
                "    value = sample.scores[\"model_graded_fact\"].value\n",
                "    print(f\"Score: {'correct' if value=='C' else 'incorrect'}\")\n",
                "    print(f\"Explanation: {sample.scores['model_graded_fact'].explanation}\")\n",
                "\n",
                "\n",
                "def print_info_from_logs(logs: list[EvalLog]):\n",
                "    for i, sample in enumerate(logs[0].samples):\n",
                "        print(f\"===START OF SAMPLE {i}===\")\n",
                "        print_info_from_sample(sample)\n",
                "        print(f\"===END OF SAMPLE {i}===\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_info_from_logs(logs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
