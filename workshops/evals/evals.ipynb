{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/evals/evals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "# Evals\n",
                "\n",
                "## Contents\n",
                "\n",
                "- \n",
                "- \n",
                "..\n",
                "\n",
                "## Goals\n",
                "\n",
                "- Get experience using Inspect, the open source evals framework from UK AI Safety Institute.\n",
                "- Get experience conducting evals.\n",
                "- Hard version of notebook: doing various 'side tasks' that research can involve, e.g. exploring a new code base, finding the history of a research idea, etc."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import google.colab\n",
                "\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    %pip install inspect-ai openai\n",
                "else:\n",
                "    !pip install inspect-ai openai"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from inspect_ai import Task, task, eval\n",
                "from inspect_ai.dataset import example_dataset\n",
                "from inspect_ai.scorer import model_graded_fact\n",
                "from inspect_ai.solver import chain_of_thought, generate, self_critique\n",
                "\n",
                "from inspect_ai.log._log import EvalSample, EvalLog\n",
                "from inspect_ai.model._chat_message import ChatMessageUser, ChatMessageAssistant"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Replace `your-api-key` with an actual openai api key.\n",
                "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Running a simple eval in Inspect\n",
                "\n",
                "The first exercise introduces the main components of the Inspect framework:\n",
                "\n",
                "- Datasets\n",
                "  - These are sets of (labeled) samples.\n",
                "A vanilla dataset would consist of a list of input-target pairs, where inputs are prompts to be given to an LLM and targets somehow indicate what an LLM's output should be (or should not, in the case of detecting unwanted behaviour).\n",
                "- Plans and solvers\n",
                "  - The core solver is `generate()` which gets the LLM to generate text based on the current message history.\n",
                "In the simplest case, the message history will consist of one user message containing the input from the dataset sample.\n",
                "Other solvers can add scaffolding by changing the message history, e.g. so that the LLM uses chain of thought.\n",
                "A plan is a sequence of solvers that provide the full scaffolding for the LLM.\n",
                "- Scorers\n",
                "  - These score the outputs of the LLMs, often involving some kind of comparison with the 'target'.\n",
                "You can do hard-coded scoring (e.g. using regex to detect answer to a multiple choice question) or use another LLM to do the scoring.\n",
                "- Tasks\n",
                "  - A task is a combination of a dataset, plan and a scorer. A task is then run on an LLM you want to evaluate.\n",
                "\n",
                "The example below is based on Inspect's documentation https://inspect.ai-safety-institute.org.uk/#getting-started and https://inspect.ai-safety-institute.org.uk/workflow.html.\n",
                "Some adjustments are made to work in a (colab) notebook - Inspect is designed to be *run* via a CLI but they do recommend using a notebook while exploring and developing eval workflows."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Datasets\n",
                "\n",
                "Inspect includes some in-built datasets.\n",
                "We will use a dataset that evaluates an LLMs understanding of theory of mind.\n",
                "\n",
                "You can view the raw jsonl file where the dataset is stored on Inspect's GitHub repo at the link\n",
                "- https://github.com/UKGovernmentBEIS/inspect_ai/blob/main/src/inspect_ai/dataset/_examples/theory_of_mind.jsonl\n",
                "\n",
                "For those interested in the history of ideas, this dataset's history can be traced through these three repos:\n",
                "- https://github.com/openai/evals/tree/main/evals/registry/data/theory_of_mind/tomi\n",
                "- https://github.com/facebookresearch/ToMi\n",
                "- https://github.com/kayburns/tom-qa-dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = example_dataset(\"theory_of_mind\")\n",
                "print(len(dataset))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We see this dataset has 100 samples.\n",
                "Let us look at an individual sample."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample = dataset[2]\n",
                "\n",
                "# print the raw input string for this sample\n",
                "# Hide: hard\n",
                "print(sample.input[0].content)\n",
                "print()\n",
                "# Hide: none\n",
                "\n",
                "# print the answer string for this sample\n",
                "# Hide: hard\n",
                "print(sample.target)\n",
                "# Hide: none\n",
                "\n",
                "# Hide: all\n",
                "5\n",
                "# Hide: none"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From the above, we see that this dataset is straightforward, and is essentially a list of 100 question-answer pairs."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plans and solvers\n",
                "\n",
                "Appropriate text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEFAULT_COT_TEMPLATE = r\"\"\"\n",
                "{prompt}\n",
                "\n",
                "Before answering, reason in a step-by-step manner as to get the right answer. Provide your answer at the end on its own line in the form \"ANSWER: $ANSWER\" (without quotes) where $ANSWER is the answer to the question.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEFAULT_CRITIQUE_TEMPLATE = r\"\"\"\n",
                "Given the following question and answer, please critique the answer. A good answer comprehensively answers the question and NEVER refuses to answer. If the answer is already correct do not provide critique - simply respond 'The original answer is fully correct'.\n",
                "\n",
                "[BEGIN DATA]\n",
                "***\n",
                "[Question]: {question}\n",
                "***\n",
                "[Answer]: {completion}\n",
                "***\n",
                "[END DATA]\n",
                "\n",
                "Critique: \"\"\"\n",
                "\n",
                "\n",
                "DEFAULT_CRITIQUE_COMPLETION_TEMPLATE = r\"\"\"\n",
                "Given the following question, initial answer and critique please generate an improved answer to the question:\n",
                "\n",
                "[BEGIN DATA]\n",
                "***\n",
                "[Question]: {question}\n",
                "***\n",
                "[Answer]: {completion}\n",
                "***\n",
                "[Critique]: {critique}\n",
                "***\n",
                "[END DATA]\n",
                "\n",
                "If the original answer is already correct, just repeat the original answer exactly. Provide your answer at the end on its own line in the form \"ANSWER: $ANSWER\" (without quotes) where $ANSWER is the answer to the question.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scorers\n",
                "\n",
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scorer = model_graded_fact(model=\"openai/gpt-4o-mini\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DEFAULT_MODEL_GRADED_FACT_TEMPLATE = \"\"\"\n",
                "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
                "\n",
                "[BEGIN DATA]\n",
                "************\n",
                "[Question]: {question}\n",
                "************\n",
                "[Expert]: {criterion}\n",
                "************\n",
                "[Submission]: {answer}\n",
                "************\n",
                "[END DATA]\n",
                "\n",
                "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
                "\n",
                "Does the submission contain the content in the expert answer?\n",
                "\n",
                "{instructions}\n",
                "\"\"\"\n",
                "\n",
                "DEFAULT_INSTRUCTIONS = \"\"\"\n",
                "After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.  Please choose ONE option for the grade: either \"C\" for correct answers, {partial_prompt}or \"I\" for incorrect answers.\n",
                "\n",
                "For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.\n",
                "\n",
                "First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Running the evals\n",
                "\n",
                "Appropriate text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@task\n",
                "def theory_of_mind(model: str):\n",
                "    return Task(\n",
                "        dataset=dataset[0:3],\n",
                "        plan=[\n",
                "            chain_of_thought(),\n",
                "            generate(),\n",
                "            self_critique(model=model),\n",
                "        ],\n",
                "        scorer=scorer,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = \"openai/gpt-4o-mini\"\n",
                "logs = eval(\n",
                "    theory_of_mind(model),\n",
                "    model=model,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_info_from_sample(sample: EvalSample):\n",
                "    print(\"---MESSAGE HISTORY---\")\n",
                "    for message in sample.messages:\n",
                "        if isinstance(message, ChatMessageUser):\n",
                "            print(f\"User: {message.content}\" + \"\\n\")\n",
                "        elif isinstance(message, ChatMessageAssistant):\n",
                "            print(f\"Assistant: {message.content}\" + \"\\n\")\n",
                "\n",
                "    print(\"--SCORER INFORMATION---\")\n",
                "    value = sample.scores[\"model_graded_fact\"].value\n",
                "    print(f\"Score: {'correct' if value=='C' else 'incorrect'}\")\n",
                "    print(f\"Explanation: {sample.scores['model_graded_fact'].explanation}\")\n",
                "\n",
                "\n",
                "def print_info_from_logs(logs: list[EvalLog]):\n",
                "    for i, sample in enumerate(logs[0].samples):\n",
                "        print(f\"===START OF SAMPLE {i}===\")\n",
                "        print_info_from_sample(sample)\n",
                "        print(f\"===END OF SAMPLE {i}===\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_info_from_logs(logs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
