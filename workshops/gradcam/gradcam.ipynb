{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/gradcam/gradcam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Visualization of CNN: Grad-CAM\n",
    "\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we will introduce the Grad-CAM which visualizes the heatmap of input images by highlighting the important region for visual question answering(VQA) task.\n",
    "\n",
    "* NB: if `PIL` is not installed, try `pip install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install jaxtyping einops -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from jaxtyping import Float, Int\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Question Answering problem\n",
    "Given an image and a question in natural language, the model choose the most likely answer from 3 000 classes according to the content of image. The VQA task is indeed a multi-classificaition problem.\n",
    "<img src=\"https://github.com/EffiSciencesResearch/ML4G/blob/main/days/w1d4/gradCam/vqa_model.PNG?raw=1\">\n",
    "\n",
    "We provide you a pretrained model `vqa_resnet` for VQA tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary files\n",
    "%cd /content\n",
    "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1TtuK5ZFDcnkK5qvF-ZBuY3PmAots6e7c' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1TtuK5ZFDcnkK5qvF-ZBuY3PmAots6e7c\" -O gradCam.zip && rm -rf /tmp/cookies.txt\n",
    "! unzip -o gradCam.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "%cd /content/gradCam\n",
    "from load_model import load_model\n",
    "\n",
    "vqa_resnet = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixes a strange bug. Ideally, we would run the model in eval mode though.\n",
    "vqa_resnet.train()\n",
    "# Dropout of 0.5 is too big, for deterministic behavior, remove the dropout\n",
    "\n",
    "# Loop through all the modules in the model\n",
    "for module in vqa_resnet.modules():\n",
    "    if isinstance(module, nn.Dropout):\n",
    "        # Update the dropout probability for each dropout layer\n",
    "        module.p = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses to sets of tokens, which are all words, one for the questions and one for the answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"2017-08-04_00.55.19.pth\"\n",
    "saved_state = torch.load(checkpoint, map_location=device)\n",
    "# reading vocabulary from saved model\n",
    "vocab = saved_state[\"vocab\"]\n",
    "print(\"Vocab:\", set(vocab.keys()))\n",
    "\n",
    "# reading word tokens from saved model\n",
    "question_word_to_index = vocab[\"question\"]\n",
    "print(\"Tokens for questions:\", question_word_to_index)\n",
    "\n",
    "# reading answers from saved model\n",
    "answer_word_to_index = vocab[\"answer\"]\n",
    "print(\"Tokens for answers:\", answer_word_to_index)\n",
    "\n",
    "num_tokens = len(question_word_to_index) + 1\n",
    "print(f\"{num_tokens=}\")\n",
    "\n",
    "# Mapping from integer to token string\n",
    "index_to_answer_word = [\"unk\"] * len(answer_word_to_index)\n",
    "for w, idx in answer_word_to_index.items():\n",
    "    index_to_answer_word[idx] = w\n",
    "\n",
    "print(index_to_answer_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "In order to use the pretrained model, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(448, 448)`. You can call the function `image_to_features` to achieve image preprocessing. For input question, the function `encode_question` is provided to encode the question into a vector of indices. You can also use `preprocess` function for both image and question preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    target_size = 448\n",
    "    central_fraction = 1.0\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(int(target_size / central_fraction)),\n",
    "            transforms.CenterCrop(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(question: str) -> Int[Tensor, \"nb_words\"]:\n",
    "    \"\"\"Turn a question into a vector of tokens.\"\"\"\n",
    "    # For this model, tokens are lowercase words, so we split on whitespace\n",
    "    words = question.lower().split()\n",
    "    # Then map each word to its index in the dictionary\n",
    "    return torch.tensor([question_word_to_index[word] for word in words], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dir_path: str, question: str):\n",
    "    \"\"\"\n",
    "    Load the image at `dir_path` and process it to be a suitable input for vqa_resnet.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(question)\n",
    "\n",
    "    img = Image.open(dir_path).convert(\"RGB\")\n",
    "    img_transformed = get_transform()(img).unsqueeze(0).to(device)\n",
    "\n",
    "    q_len = torch.tensor(tokens.shape, device=device)\n",
    "\n",
    "    inputs = (img_transformed, tokens.unsqueeze(0), q_len.unsqueeze(0))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answers(img_path: str, question: str, topk=10):\n",
    "    \"\"\"Show the top `topk` answers of the model for a given question.\"\"\"\n",
    "    inputs = preprocess(img_path, question)\n",
    "    logits = vqa_resnet(*inputs)\n",
    "    probas = F.softmax(logits.squeeze(), dim=0)\n",
    "    values, tokens_indices = torch.topk(probas, k=topk)\n",
    "\n",
    "    print(\"Output probablities:\")\n",
    "    for token, value in zip(tokens_indices, values):\n",
    "        print(f\"- {index_to_answer_word[token]!r: >10} \\t-> {value:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you two pictures and questions. Is the model doing great?\n",
    "If not, make a hypothesis for why it makes an eroneous prediction. What feature in the image did it pick up?\n",
    "\n",
    "This is the question that GradCam tries to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_cat_path = \"dog_cat.png\"\n",
    "dog_cat_question = \"What animal\"\n",
    "check_answers(dog_cat_path, dog_cat_question)\n",
    "Image.open(dog_cat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrant_path = \"hydrant.png\"\n",
    "hydrant_question = \"What color\"\n",
    "check_answers(hydrant_path, hydrant_question)\n",
    "\n",
    "Image.open(hydrant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks in  pytorch\n",
    "\n",
    "The goal of this exercise is to familiarize yourself with the hook system in pytorch. The hooks are not used to manipulate the weights but to **manipulate the activations** of the model on a given input. You can read, and even modify the hidden activations of the model\n",
    "\n",
    "1. Use hooks to log information about the inner working of the model. Here we will just print the shapes of the activations.\n",
    "2. But we can also view more interesting information. In the second exercise we plot the norm of each channel. The norm of a channel is a (bad) proxy for how much information there is in a channel.\n",
    "3. We actually modify the activation to flip the sign of the output of a convolution. This should change the output of our model: we are butchering through it! (this is also a completely meaningless operation, but let's see what it does...)\n",
    "\n",
    "Hooks in pytorch are not super pleasant to work with:\n",
    "- Once you add a hook to a module, it stays there until you remove it, using `handle.remove()`. But for this, you need to\n",
    "have saved the handle in the first place. \n",
    "- Errors: if your hook function throws an error, it you will need to remove it, since they are not removed automatically. The best way to do this is to always wrap calls with hooks in a `try`/`finally` block, and remove the hook in the `finally` block. This way, you are garanteed that the hook will be removed after one pass through the model.\n",
    "- If you mess up, you can always reload the model, the hooks will be gone.\n",
    "- They encorage the use of `global` state, which can lead to the usual drawbacks of global state. \n",
    "\n",
    "We do our interventions on the last convolution of the resnet, defined below.\n",
    "\n",
    "You may need to read the hook tuto https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last convolution\n",
    "module_to_hook = vqa_resnet.resnet_layer4.r_model.layer4[2].conv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Show the shapes of the activations\n",
    "\n",
    "You need to print the shapes of the input and output of the second to last convolution.\n",
    "What are their shape? What does each dimension represent?\n",
    "\n",
    "Hint: both of them are 4D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_shapes_hook(module, inputs, output):\n",
    "    # What's the type of `inputs`?\n",
    "    ...\n",
    "\n",
    "\n",
    "hook_handle = module_to_hook.register_forward_hook(show_shapes_hook)\n",
    "\n",
    "try:\n",
    "    check_answers(dog_cat_path, dog_cat_question)\n",
    "finally:\n",
    "    hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    \n",
    "```python\n",
    "def show_shapes_hook(module, inputs, output):\n",
    "    # Note: inputs is a tuple of one element (= all the inputs of the module)\n",
    "    print(f\"{inputs[0].shape=}\")\n",
    "    print(f\"{output.shape=}\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "### Exercise 2: Plotting in a hook\n",
    "\n",
    "Goal: figuring out which channels have the highest norm (â‰ˆ are the most used).\n",
    "You just need to compute the per-channel norm.\n",
    "\n",
    "What does the plot tells you about the inner workings of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_highest_output_norm(module, inputs, output):\n",
    "    norms = ...\n",
    "\n",
    "    norms.squeeze_()\n",
    "    assert norms.shape == (2048,)\n",
    "    plt.plot(norms.detach().cpu())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "hook_handle = module_to_hook.register_forward_hook(plot_highest_output_norm)\n",
    "\n",
    "try:\n",
    "    check_answers(dog_cat_path, dog_cat_question)\n",
    "finally:\n",
    "    hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "What does the plot tell you about the model? Huh... nothing?\n",
    "    \n",
    "```python\n",
    "def plot_highest_output_norm(self, inputs, output):\n",
    "    # You could also have used matrix_norm here, it's the same result.\n",
    "    norms = torch.linalg.vector_norm(output, dim=(0, 2, 3))\n",
    "\n",
    "    norms.squeeze_()\n",
    "    assert norms.shape == (2048,)\n",
    "    plt.plot(norms.detach().cpu())\n",
    "    plt.show()\n",
    "```\n",
    "</details>\n",
    "\n",
    "### Exercise 3: Modifying the output of a module\n",
    "\n",
    "What would happen if we filp (i.e. multiply by $-1$) the contribution of this convolution to the residual stream?\n",
    "Make a prediction first!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_output(module, inputs, output):\n",
    "    # You need to modify output *in place*\n",
    "    ...\n",
    "\n",
    "\n",
    "hook_handle = module_to_hook.register_forward_hook(flip_output)\n",
    "\n",
    "try:\n",
    "    check_answers(dog_cat_path, dog_cat_question)\n",
    "finally:\n",
    "    hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    \n",
    "```python\n",
    "def flip_output(module, inputs, output):\n",
    "    # You need to modify output *in place*\n",
    "    output *= -1\n",
    "```\n",
    "</details>\n",
    "\n",
    "### Exercise 4: Save the output activations so we can re-use them later.\n",
    "\n",
    "There is no code to complete in this exercise, but you need to understand what is going on.\n",
    "- What is `global`?\n",
    "- Why do we use it, and what would happen if we did not?\n",
    "- What are the drawbacks of using `global` variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_output = None\n",
    "saved_output_grad = None\n",
    "\n",
    "\n",
    "def forward_hook(module, inputs, output):\n",
    "    global saved_output\n",
    "    saved_output = output\n",
    "    print(\"Saved output of shape:\", output.shape)\n",
    "\n",
    "\n",
    "def backward_hook(module, grad_input, grad_outputs):\n",
    "    global saved_output_grad\n",
    "    saved_output_grad = grad_outputs[0]\n",
    "    print(\"Saved gradient of shape:\", grad_outputs[0].shape)\n",
    "\n",
    "\n",
    "forward_handle = module_to_hook.register_forward_hook(forward_hook)\n",
    "backward_handle = module_to_hook.register_backward_hook(backward_hook)\n",
    "\n",
    "try:\n",
    "    inputs = preprocess(dog_cat_path, dog_cat_question)\n",
    "    vqa_resnet.zero_grad()\n",
    "    logits = vqa_resnet(*inputs)\n",
    "    # Pretend the loss is the 'giraffe' logit.\n",
    "    loss = logits[0, answer_word_to_index[\"giraffe\"]]\n",
    "    loss.backward()\n",
    "finally:\n",
    "    forward_handle.remove()\n",
    "    backward_handle.remove()\n",
    "\n",
    "print(\"Shape of saved output:\", saved_output.shape)\n",
    "print(\"Shape of saved grad:\", saved_output_grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM\n",
    "* **Overview:** Given an image with a question, and a category (â€˜dogâ€™) as input, we forward propagate the image through the model to obtain the `raw class scores` before softmax. We backpropagate only the logit of the target class. This signal is then backpropagated to the `convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "* We will define a `grad_cam` function visualize each image and its saliency map.\n",
    "\n",
    "* Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(\n",
    "    img_path=\"dog_cat.png\", question=\"What animal\", answer=\"dog\", module_to_hook=module_to_hook\n",
    "):\n",
    "    # Make a figure with 3 subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
    "    # Plot the original image on the left\n",
    "    img = Image.open(img_path)\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Original image\")\n",
    "\n",
    "    inputs = preprocess(img_path, question)\n",
    "\n",
    "    # Add the hooks to store the feature map and its gradient\n",
    "    # in the global variables saved_output and saved_output_grad.\n",
    "    forward_handle = module_to_hook.register_forward_hook(forward_hook)\n",
    "    backward_handle = module_to_hook.register_backward_hook(backward_hook)\n",
    "\n",
    "    try:\n",
    "        # Make sure there are no gradients\n",
    "        vqa_resnet.zero_grad()\n",
    "        # Compute the predictions of the model\n",
    "        logits = vqa_resnet(*inputs)\n",
    "\n",
    "        # Backpropagate just on the logit of the given answer\n",
    "        answer_logit = logits[0, answer_word_to_index[answer]]\n",
    "        answer_logit.backward()\n",
    "    finally:\n",
    "        forward_handle.remove()\n",
    "        backward_handle.remove()\n",
    "\n",
    "    # TODO: compute the gradient camera (equation 1 and 2 of the paper)\n",
    "    mean_gradient = einops.reduce(...)\n",
    "    grad_cam = einops.einsum(...)\n",
    "\n",
    "    grad_cam = grad_cam.clip(min=0)\n",
    "\n",
    "    # Upscale, normalize and convert to RGB\n",
    "    grad_cam = grad_cam.cpu().detach().numpy()\n",
    "    cam = cv2.resize(grad_cam, (224, 224))\n",
    "    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
    "    cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
    "\n",
    "    # Heatmap of activation map. Plot in the center.\n",
    "    activation_heatmap = cv2.applyColorMap(cam, cv2.COLORMAP_HSV)\n",
    "    axs[1].imshow(activation_heatmap)\n",
    "    axs[1].set_title(\"Heatmap of activation map\")\n",
    "\n",
    "    # Overlay heatmap and picture. Plot on the right.\n",
    "    img = cv2.imread(img_path)\n",
    "    org_img = cv2.resize(img, (224, 224))\n",
    "    img_with_heatmap = np.float32(activation_heatmap) + np.float32(org_img)\n",
    "    img_with_heatmap *= 0.99 / np.max(img_with_heatmap)\n",
    "    axs[2].imshow(img_with_heatmap)\n",
    "    axs[2].set_title(\"Heatmap on picture\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "    \n",
    "```python\n",
    "    mean_gradient = einops.reduce(saved_output_grad, \"batch features w h -> features\", \"mean\")\n",
    "    grad_cam = einops.einsum(\n",
    "        mean_gradient, saved_output.squeeze(0), \"features, features w h -> w h\"\n",
    "    )\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam(img_path=\"dog_cat.png\", question=\"What animal\", answer=\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam(img_path=\"dog_cat.png\", question=\"What animal\", answer=\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam(img_path=\"dog_cat.png\", question=\"What animal\", answer=\"giraffe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam(img_path=\"hydrant.png\", question=\"What color\", answer=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam(img_path=\"hydrant.png\", question=\"What color\", answer=\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats the interpretation of those plots?\n",
    "\n",
    "Note: don't try too much to interpret the cat.\n",
    "\n",
    "\n",
    "## Bonus: Safari - hunting the giraffe\n",
    "Note: please don't do this in real life. ðŸ˜˜ðŸ¦’\n",
    "\n",
    "At the start we saw that the model predicts the animal in the cat-dog picture is a giraffe. \n",
    "Using grad cam and hooks, can you find a find the parts of the model that are responsible for this prediction \n",
    "and remove them?\n",
    "\n",
    "This is a very exploratory exercise, you will have to make your own hypothesis and experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
