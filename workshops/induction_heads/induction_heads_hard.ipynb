{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/induction_heads/induction_heads_hard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "## Introduction\n",
                "\n",
                "Thanks to Callum McDougall for creating much of this notebook's content!\n",
                "\n",
                "This exercise should get you familiar with concepts in circuit interpretability in transformers.\n",
                "\n",
                "## Content & Learning Objectives\n",
                "\n",
                "#### 1️⃣ Find Inductive Heads via attention patterns\n",
                "\n",
                "You'll plot attention patterns to check out the theory of Inductive Heads and see if you can find them in the model you're working with.\n",
                "\n",
                "> ##### Learning objectives\n",
                "> \n",
                "> - Use `circuitsvis` to visualize attention heads\n",
                "> - Understand what the theory of inductive heads predicts about attention patterns\n",
                "> - Use attention patterns to identify inductive heads\n",
                "> - Automate this process to find inductive heads in a larger model\n",
                "\n",
                "#### 2️⃣ Logit Attribution\n",
                "\n",
                "Here, you'll learn how to use `TransformerLens` to implement `LogitLens`, a tool for attributing logit values to specific components of the model. You'll also learn how to use this tool to identify basic attention heads that are important for Induction tasks.\n",
                "> ##### Learning objectives\n",
                "> - Perform direct logit attribution to figure out which heads are writing to the residual stream in a significant way\n",
                "> - Crosscheck your earlier results with the results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import google.colab\n",
                "\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    # Install packages\n",
                "    %pip install einops transformer_lens circuitsvis -q\n",
                "\n",
                "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
                "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/induction_heads/*'\n",
                "    !mv --no-clobber ML4G-2.0-master/workshops/induction_heads/* .\n",
                "    !rm -r ML4G-2.0-master\n",
                "\n",
                "    print(\"Imports & installations complete!\")\n",
                "else:\n",
                "    %load_ext autoreload\n",
                "    %autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import circuitsvis as cv\n",
                "import torch\n",
                "import typeguard\n",
                "from typing import Callable\n",
                "import plotly.express as px\n",
                "from huggingface_hub import hf_hub_download\n",
                "from IPython.display import display\n",
                "from jaxtyping import Float, Int, jaxtyped\n",
                "from tests import (\n",
                "    test_average_over_condition,\n",
                "    test_current_attn_detector,\n",
                "    test_first_attn_detector,\n",
                "    test_induction_attn_detector,\n",
                "    test_logit_attribution,\n",
                "    test_prev_attn_detector,\n",
                ")\n",
                "from torch import Tensor\n",
                "from transformer_lens import ActivationCache, HookedTransformer, HookedTransformerConfig\n",
                "\n",
                "# We use @typechecked above a function to make sure the types, and tensor sizes are respected.\n",
                "typechecked = jaxtyped(typechecker=typeguard.typechecked)\n",
                "\n",
                "# Disable the gradients globally. We don't want them for this notebook.\n",
                "torch.set_grad_enabled(False)\n",
                "\n",
                "# Use CUDA if available\n",
                "device = torch.device(\"cpu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and Running Models\n",
                "\n",
                "Today, we will be using two models:\n",
                "\n",
                "1. GPT-2 Small: a smaller version of the GPT-2 model with 12 layers and 80 million parameters.\n",
                "\n",
                "2. A two-layer attention-only transformer model.\n",
                "This toy model is a 2L attention-only transformer trained specifically for today. Some changes to make it easier to interpret:\n",
                "- It has only attention blocks.\n",
                "- The positional embeddings are only added to the residual stream before each key and query vector in the attention layers instead of the token embeddings - i.e. we compute queries as `Q = (resid + pos_embed) @ W_Q + b_Q` and the same for keys, but for values as `V = resid @ W_V + b_V`. This means that **the residual stream can't directly encode positional information**.\n",
                "- This turns out to make it *way* easier for induction heads to form; it happens 2-3x times earlier - [see the comparison of two training runs](https://wandb.ai/mechanistic-interpretability/attn-only/reports/loss_ewma-22-08-24-11-08-83---VmlldzoyNTI0MDMz?accessToken=8ap8ir6y072uqa4f9uinotdtrwmoa8d8k2je4ec0lyasf1jcm3mtdh37ouijgdbm) here. (The bump in each curve is the formation of induction heads.)\n",
                "    - The argument that does this below is `positional_embedding_type=\"shortformer\"`.\n",
                "- It has no MLP layers, no LayerNorms, and no biases.\n",
                "- There are separate embed and unembed matrices (i.e. the weights are not tied).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Don't read, just run\n",
                "\n",
                "# Load GPT-2\n",
                "gpt2: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
                "gpt2.set_use_attn_result(True)\n",
                "\n",
                "# Load the toy transformer\n",
                "cfg = HookedTransformerConfig(\n",
                "    d_model=768,\n",
                "    d_head=64,\n",
                "    n_heads=12,\n",
                "    n_layers=2,\n",
                "    n_ctx=2048,\n",
                "    d_vocab=50278,\n",
                "    attention_dir=\"causal\",\n",
                "    attn_only=True,  # defaults to False\n",
                "    tokenizer_name=\"EleutherAI/gpt-neox-20b\",\n",
                "    seed=398,\n",
                "    use_attn_result=True,\n",
                "    normalization_type=None,  # defaults to \"LN\", i.e. layernorm with weights & biases\n",
                "    positional_embedding_type=\"shortformer\",\n",
                ")\n",
                "\n",
                "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
                "FILENAME = \"attn_only_2L_half.pth\"\n",
                "\n",
                "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
                "\n",
                "model = HookedTransformer(cfg)\n",
                "model.load_state_dict(torch.load(weights_path, map_location=device))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Caching all Activations\n",
                "\n",
                "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out, on the first sentence from the GPT-2 paper.\n",
                "\n",
                "<details>\n",
                "<summary>Aside - a note on <code>remove_batch_dim</code></summary>\n",
                "\n",
                "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and can be annoying, so passing in the `remove_batch_dim=True` keyword removes it. \n",
                "\n",
                "`gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \" I can't repeat enough that ML4Good is the best place to learn about AI safety.\" * 2\n",
                "\n",
                "# Tokenize the text\n",
                "tokens: Int[Tensor, \"batch=1 seq_len\"] = model.to_tokens(text)\n",
                "print(f\"{tokens.shape=}\")\n",
                "\n",
                "# Gather the logits and caches\n",
                "logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you inspect the `gpt_cache` object, you should see that it contains a very large number of keys, each one corresponding to a different activation in the model. You can access the keys by indexing the cache directly, or by using a more convenient indexing shorthand. For instance, the code:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pattern = cache[\"pattern\", 0]\n",
                "print(f\"{pattern.shape=}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>What is this activation [\"pattern\", 0]? Can you explain what the size of the shape?</summary>\n",
                "\n",
                "The activations are attention scores between the pairs of tokens of the sentence. The 0 corresponds to the first layer of the model.\n",
                "Its shape is `(number of heads, query token, key token)`.\n",
                "</details>\n",
                "\n",
                "You can use the following diagram to find the names of activations that you want (click to see it larger)\n",
                "[ ![activations](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-full-updated.png) ](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-full-updated.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing Attention Heads\n",
                "\n",
                "A key insight from the Mathematical Frameworks paper is that we should focus on interpreting the parts of the model that are intrinsically interpretable - the input tokens, the output logits, and the attention patterns. Everything else (the residual stream, keys, queries, values, etc.) are compressed intermediate states when calculating meaningful things. So a natural place to start is classifying heads by their attention patterns on various texts.\n",
                "\n",
                "When doing interpretability, it's always good to begin by visualizing your data rather than relying on summary statistics. Summary statistics can be super misleading! Now that we have visualized the attention patterns, we can create some basic summary statistics and use our visualizations to validate them. Being proficient in web development/data visualization is a surprisingly useful skill set. Neural networks are very high-dimensional objects.\n",
                "Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on Anthropic's PySvelte library). If you did the previous set of exercises, you'll have seen this library before.\n",
                "\n",
                "We will use the function `cv.attention.attention_patterns`, which takes the following arguments:\n",
                "\n",
                "* `attention`: Attention head activations.\n",
                "    * This should be a tensor of shape `[nhead, seq_dest, seq_src]`, i.e. the `[i, :, :]`th element is the grid of attention patterns (probabilities) for the `i`th attention head.\n",
                "    * We get this by indexing our `gpt2_cache` object.\n",
                "* `tokens`: List of tokens (e.g. `[\"A\", \"person\"]`). \n",
                "    * Sequence length must match that inferred from `attention`.\n",
                "    * This is used to label the grid.\n",
                "    * We get this by using the `gpt2_small.to_str_tokens` method.\n",
                "* `attention_head_names`: Optional list of names for the heads.\n",
                "\n",
                "There are also other circuitsvis functions, e.g. `cv.attention.attention_pattern` (for just a single head) or `cv.attention.attention_heads` (which has the same syntax and but presents the information in a different form).\n",
                "\n",
                "<details>\n",
                "<summary>Help - my <code>attention_heads</code> plots are behaving weirdly (e.g. they continually shrink after I plot them).</summary>\n",
                "This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.\n",
                "\n",
                "Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:\n",
                "\n",
                "```python\n",
                "attn_heads = cv.attention.attention_heads(\n",
                "    tokens=gpt2_str_tokens, \n",
                "    attention=attention_pattern,\n",
                "    attention_head_names=[f\"L0H{i}\" for i in range(12)],\n",
                ")\n",
                "\n",
                "path = \"attn_heads.html\"\n",
                "\n",
                "with open(path, \"w\") as f:\n",
                "    f.write(str(attn_heads))\n",
                "\n",
                "webbrowser.open(path)\n",
                "```\n",
                "\n",
                "To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.\n",
                "</details>\n",
                "\n",
                "This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backward, so information can only move forward in the network.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "layer = 1\n",
                "\n",
                "attention_pattern = cache[\"pattern\", layer]\n",
                "str_tokens = model.to_str_tokens(text)\n",
                "\n",
                "cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hover over heads to see the attention patterns; click on a head to lock it. Hover over each token to see which other tokens it attends to (or which other tokens attend to it - you can see this by changing the dropdown to `Destination <- Source`).\n",
                "\n",
                "# Finding induction heads"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "             \n",
                "Use the [diagram at this link](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/small-merm.svg) to remind yourself of the relevant hook names.\n",
                "\n",
                "\n",
                "### Exercise - visualise attention patterns\n",
                "\n",
                "```yaml\n",
                "Difficulty: 🔴🔴⚪⚪⚪\n",
                "Importance: 🔵🔵🔵⚪⚪\n",
                "\n",
                "You should spend up to ~10 minutes on this exercise.\n",
                "\n",
                "It's important to be comfortable using circuitsvis, and the cache object.\n",
                "```\n",
                "\n",
                "*This exercise should be very quick - you can reuse code from the previous section. You should look at the solution if you're still stuck after 5-10 minutes.*\n",
                "\n",
                "Visualise the attention patterns for **both** layers of your model, with the same repeated text.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Convert the text to tokens, both as integers and as strings (for the plot)\n",
                "...  # TODO: ~8 words\n",
                "\n",
                "# 2. Pass the tokens through the model, get the cache\n",
                "...  # TODO: ~7 words\n",
                "\n",
                "# 3. For each of the model.cfg.n_layers, display the attention pattern\n",
                "...  # TODO: ~24 words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "# 1. Convert the text to tokens, both as integers and as strings (for the plot)\n",
                "tokens = model.to_tokens(text)\n",
                "str_tokens = model.to_str_tokens(text)\n",
                "\n",
                "# 2. Pass the tokens through the model, get the cache\n",
                "logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
                "\n",
                "# 3. For each of the model.cfg.n_layers, display the attention pattern\n",
                "for layer in range(model.cfg.n_layers):\n",
                "    print(f\"Layer {layer+1}\")\n",
                "    attention_pattern = cache[\"pattern\", layer]\n",
                "    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>What patterns do you see (find at least 3)</summary>\n",
                "We notice that there are three basic patterns which repeat quite frequently:\n",
                "\n",
                "* `prev_token_heads`, which mainly attend to the previous token (e.g. head `0.7`)\n",
                "* `current_token_heads`, which mainly attend to the current token (e.g. head `1.6`)\n",
                "* `first_token_heads`, which attend a lot to the first token (e.g. heads `0.3` or `1.4`, although these are a bit less clear-cut than the other two)\n",
                "The `prev_token_heads` and `current_token_heads` are perhaps unsurprising because words that are close together in a sequence probably have a lot more mutual information (i.e. we could get quite far using bigram or trigram prediction). \n",
                "\n",
                "The `first_token_heads` are a bit more surprising. The basic intuition here is that the first token in a sequence is often used as a resting or null position for heads that only sometimes activate (since our attention probabilities always have to add up to 1).\n",
                "</details>\n",
                "\n",
                "Now that we've observed our three basic attention patterns, it's time to make detectors for those patterns!\n",
                "### Exercise - Write Your Own Detectors\n",
                "\n",
                "```yaml\n",
                "Difficulty: 🔴🔴⚪⚪⚪\n",
                "Importance: 🔵🔵🔵⚪⚪\n",
                "\n",
                "You shouldn't spend more than 15-20 minutes on these exercises.\n",
                "These exercises shouldn't be too challenging if you understand attention patterns.\n",
                "```\n",
                "\n",
                "You should fill in the functions below, which act as detectors for particular types of heads. Validate your detectors by comparing these results to the visual attention patterns above. Summary statistics on their own can be unreliable, but are much more reliable if you can validate them by directly engaging with the data.\n",
                "\n",
                "Tasks like this are useful because we need to be able to interpret our observations/intuitions about what a model is doing and translate these into quantitative measures. As the exercises proceed, we'll be creating some much more interesting tools and detectors!\n",
                "\n",
                "Note - there's no objectively correct answer for which heads are performing which tasks, and which detectors can spot them. You should try to come up with something plausible that identifies the kind of behavior you're looking for.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "THRESHOLD = 0.35\n",
                "\n",
                "\n",
                "@typechecked\n",
                "def average_over_condition(\n",
                "    tensor: Float[Tensor, \"head query key\"],\n",
                "    condition: Callable[[int, int], bool],\n",
                ") -> Float[Tensor, \"head\"]:\n",
                "    \"\"\"\n",
                "    Return the average of the values in the tensor whose indices satisfy the condition.\n",
                "\n",
                "    Args:\n",
                "        tensor: A 3D tensor of shape (n_head, n_query, n_key).\n",
                "        condition: A function that takes two indices, for the query and key, and determines whether to consider them.\n",
                "\n",
                "    Returns:\n",
                "        A 1D tensor of shape (n_head,) with the average of the values in the tensor that satisfy the condition.\n",
                "    \"\"\"\n",
                "    n_head, n_query, n_key = tensor.shape\n",
                "\n",
                "    # Make a 2D tensor with True where the condition is met, and use it to index the tensor\n",
                "    ...  # TODO: ~33 words\n",
                "\n",
                "\n",
                "def heads_with_high_average_pattern(\n",
                "    cache, condition: Callable[[int, int], bool], threshold: float\n",
                ") -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list of \"L{layer}H{head}\" which have an average pattern value over the threshold\n",
                "    on the entries whose indices satisfy the condition.\n",
                "    \"\"\"\n",
                "    return_values = []\n",
                "\n",
                "    for layer, pattern in enumerate(cache.stack_activation(\"pattern\")):\n",
                "        ...  # TODO: ~21 words\n",
                "    return return_values\n",
                "\n",
                "\n",
                "def current_attn_detector(cache: ActivationCache, threshold=THRESHOLD) -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be the current-token heads\n",
                "    \"\"\"\n",
                "\n",
                "    def is_attn_to_self(query: int, key: int): ...  # TODO: ~3 words\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_self, threshold=threshold)\n",
                "\n",
                "\n",
                "def prev_attn_detector(cache: ActivationCache, threshold=THRESHOLD) -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
                "    \"\"\"\n",
                "\n",
                "    def is_attn_to_prev_token(query: int, key: int): ...  # TODO: ~4 words\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_prev_token, threshold=threshold)\n",
                "\n",
                "\n",
                "def first_attn_detector(cache: ActivationCache, threshold=THRESHOLD) -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
                "    \"\"\"\n",
                "\n",
                "    def is_attn_to_first_token(query: int, key: int): ...  # TODO: ~3 words\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_first_token, threshold=threshold)\n",
                "\n",
                "\n",
                "def find_repeating_rows(tensor):\n",
                "    \"\"\"\n",
                "    Finds repeating rows (vectors) in a 2D torch tensor.\n",
                "\n",
                "    Args:\n",
                "        tensor (torch.Tensor): A 2D torch tensor.\n",
                "\n",
                "    Returns:\n",
                "        dict: A dictionary where keys are the indices of repeating rows,\n",
                "            and values are the indices where those rows last occurred.\n",
                "    \"\"\"\n",
                "    last_occurrence = {}\n",
                "    repeats = {}\n",
                "\n",
                "    for pos, token in enumerate(tensor[0]):\n",
                "        id = token.item()\n",
                "\n",
                "        if id in last_occurrence:\n",
                "            repeats[pos] = last_occurrence[id]\n",
                "        last_occurrence[id] = pos\n",
                "\n",
                "    return repeats\n",
                "\n",
                "\n",
                "@typechecked\n",
                "def induction_attn_detector(\n",
                "    cache: ActivationCache, tokens: Int[Tensor, \"batch seq\"], off_by_one=True, threshold=THRESHOLD\n",
                ") -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be induction heads\n",
                "\n",
                "    Args:\n",
                "        off_by_one: If True, the heads attending to the token after the previous occurrence are returned,\n",
                "            otherwise the heads attending directly to the previous occurrence are returned.\n",
                "    \"\"\"\n",
                "    repeat_dict = find_repeating_rows(tokens)\n",
                "\n",
                "    def is_attn_to_last_occurrence(query, key):\n",
                "        if query not in repeat_dict.keys():\n",
                "            return False\n",
                "        to_add = 1 if off_by_one else 0\n",
                "        return repeat_dict[query] + to_add == key\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_last_occurrence, threshold=threshold)\n",
                "\n",
                "\n",
                "def print_head_detection_report(cache, tokens, threshold=THRESHOLD):\n",
                "    print(\"Heads attending to the current token:\", *current_attn_detector(cache, threshold))\n",
                "\n",
                "    print(\"Heads attending to the first token:\", *first_attn_detector(cache, threshold))\n",
                "    print(\n",
                "        \"Heads attending to previous occurrence:\",\n",
                "        *induction_attn_detector(cache, tokens, off_by_one=False, threshold=threshold),\n",
                "    )\n",
                "    print(\n",
                "        \"Heads attending to one after previous occurrence:\",\n",
                "        *induction_attn_detector(cache, tokens, off_by_one=True, threshold=threshold),\n",
                "    )\n",
                "\n",
                "\n",
                "test_average_over_condition(average_over_condition)\n",
                "test_current_attn_detector(current_attn_detector, model)\n",
                "test_first_attn_detector(first_attn_detector, model)\n",
                "test_induction_attn_detector(induction_attn_detector, model)\n",
                "test_prev_attn_detector(prev_attn_detector, model)\n",
                "print()\n",
                "\n",
                "print_head_detection_report(cache, tokens, threshold=0.35)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "THRESHOLD = 0.35\n",
                "\n",
                "\n",
                "@typechecked\n",
                "def average_over_condition(\n",
                "    tensor: Float[Tensor, \"head query key\"],\n",
                "    condition: Callable[[int, int], bool],\n",
                ") -> Float[Tensor, \"head\"]:\n",
                "    \"\"\"\n",
                "    Return the average of the values in the tensor whose indices satisfy the condition.\n",
                "\n",
                "    Args:\n",
                "        tensor: A 3D tensor of shape (n_head, n_query, n_key).\n",
                "        condition: A function that takes two indices, for the query and key, and determines whether to consider them.\n",
                "\n",
                "    Returns:\n",
                "        A 1D tensor of shape (n_head,) with the average of the values in the tensor that satisfy the condition.\n",
                "    \"\"\"\n",
                "    n_head, n_query, n_key = tensor.shape\n",
                "\n",
                "    # Make a 2D tensor with True where the condition is met, and use it to index the tensor\n",
                "    condition_tensor = torch.tensor(\n",
                "        [[condition(j, k) for k in range(n_key)] for j in range(n_query)]\n",
                "    )\n",
                "    assert condition_tensor.shape == (n_query, n_key)\n",
                "\n",
                "    # For each head, average over the condition\n",
                "    return torch.stack([tensor[head][condition_tensor].mean() for head in range(n_head)])\n",
                "\n",
                "\n",
                "def heads_with_high_average_pattern(\n",
                "    cache, condition: Callable[[int, int], bool], threshold: float\n",
                ") -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list of \"L{layer}H{head}\" which have an average pattern value over the threshold\n",
                "    on the entries whose indices satisfy the condition.\n",
                "    \"\"\"\n",
                "    return_values = []\n",
                "\n",
                "    for layer, pattern in enumerate(cache.stack_activation(\"pattern\")):\n",
                "        scores = average_over_condition(pattern, condition)\n",
                "        for head, score in enumerate(scores):\n",
                "            if score > threshold:\n",
                "                return_values.append(f\"L{layer+1}H{head}\")\n",
                "    return return_values\n",
                "\n",
                "\n",
                "def current_attn_detector(cache: ActivationCache, threshold=THRESHOLD) -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be the current-token heads\n",
                "    \"\"\"\n",
                "\n",
                "    def is_attn_to_self(query: int, key: int):\n",
                "        return query == key\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_self, threshold=threshold)\n",
                "\n",
                "\n",
                "def prev_attn_detector(cache: ActivationCache, threshold=THRESHOLD) -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
                "    \"\"\"\n",
                "\n",
                "    def is_attn_to_prev_token(query: int, key: int):\n",
                "        return query == key + 1\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_prev_token, threshold=threshold)\n",
                "\n",
                "\n",
                "def first_attn_detector(cache: ActivationCache, threshold=THRESHOLD) -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
                "    \"\"\"\n",
                "\n",
                "    def is_attn_to_first_token(query: int, key: int):\n",
                "        return key == 0\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_first_token, threshold=threshold)\n",
                "\n",
                "\n",
                "def find_repeating_rows(tensor):\n",
                "    \"\"\"\n",
                "    Finds repeating rows (vectors) in a 2D torch tensor.\n",
                "\n",
                "    Args:\n",
                "        tensor (torch.Tensor): A 2D torch tensor.\n",
                "\n",
                "    Returns:\n",
                "        dict: A dictionary where keys are the indices of repeating rows,\n",
                "            and values are the indices where those rows last occurred.\n",
                "    \"\"\"\n",
                "    last_occurrence = {}\n",
                "    repeats = {}\n",
                "\n",
                "    for pos, token in enumerate(tensor[0]):\n",
                "        id = token.item()\n",
                "\n",
                "        if id in last_occurrence:\n",
                "            repeats[pos] = last_occurrence[id]\n",
                "        last_occurrence[id] = pos\n",
                "\n",
                "    return repeats\n",
                "\n",
                "\n",
                "@typechecked\n",
                "def induction_attn_detector(\n",
                "    cache: ActivationCache, tokens: Int[Tensor, \"batch seq\"], off_by_one=True, threshold=THRESHOLD\n",
                ") -> list[str]:\n",
                "    \"\"\"\n",
                "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be induction heads\n",
                "\n",
                "    Args:\n",
                "        off_by_one: If True, the heads attending to the token after the previous occurrence are returned,\n",
                "            otherwise the heads attending directly to the previous occurrence are returned.\n",
                "    \"\"\"\n",
                "    repeat_dict = find_repeating_rows(tokens)\n",
                "\n",
                "    def is_attn_to_last_occurrence(query, key):\n",
                "        if query not in repeat_dict.keys():\n",
                "            return False\n",
                "        to_add = 1 if off_by_one else 0\n",
                "        return repeat_dict[query] + to_add == key\n",
                "\n",
                "    return heads_with_high_average_pattern(cache, is_attn_to_last_occurrence, threshold=threshold)\n",
                "\n",
                "\n",
                "def print_head_detection_report(cache, tokens, threshold=THRESHOLD):\n",
                "    print(\"Heads attending to the current token:\", *current_attn_detector(cache, threshold))\n",
                "\n",
                "    print(\"Heads attending to the first token:\", *first_attn_detector(cache, threshold))\n",
                "    print(\n",
                "        \"Heads attending to previous occurrence:\",\n",
                "        *induction_attn_detector(cache, tokens, off_by_one=False, threshold=threshold),\n",
                "    )\n",
                "    print(\n",
                "        \"Heads attending to one after previous occurrence:\",\n",
                "        *induction_attn_detector(cache, tokens, off_by_one=True, threshold=threshold),\n",
                "    )\n",
                "\n",
                "\n",
                "test_average_over_condition(average_over_condition)\n",
                "test_current_attn_detector(current_attn_detector, model)\n",
                "test_first_attn_detector(first_attn_detector, model)\n",
                "test_induction_attn_detector(induction_attn_detector, model)\n",
                "test_prev_attn_detector(prev_attn_detector, model)\n",
                "print()\n",
                "\n",
                "print_head_detection_report(cache, tokens, threshold=0.35)",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Compare the printouts to your attention visualizations above. Do they seem to make sense?\n",
                "\n",
                "You can now do the same thing with GPT-2 and find all the induction heads in the model!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "layer = 8  # @param {type:\"slider\", min:1, max:12}\n",
                "\n",
                "# 1. Convert the text to tokens, both as integers and as strings (for the plot)\n",
                "tokens = gpt2.to_tokens(text)\n",
                "str_tokens = gpt2.to_str_tokens(text)\n",
                "\n",
                "# 2. Pass the tokens through the model, get the cache\n",
                "logits, cache = gpt2.run_with_cache(tokens, remove_batch_dim=True)\n",
                "\n",
                "# 3. Display the attention pattern for the selected layer\n",
                "print(f\"Layer {layer}\")\n",
                "attention_pattern = cache[\"pattern\", layer - 1]\n",
                "display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
                "\n",
                "print_head_detection_report(cache, tokens, threshold=0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Logit Attribution\n",
                "\n",
                "We now implement a second method to identify induction heads, which will hopefully give similar results to the first method. This method is based on the idea of **logit attribution**.\n",
                "\n",
                "A consequence of the residual stream is that the output logits are the sum of the contributions of each layer, and thus the sum of the results of each head. This means we can decompose the output logits into a term coming from each head and directly do attribution like this!\n",
                "\n",
                "<details>\n",
                "<summary>A concrete example</summary>\n",
                "\n",
                "Let's say that our model knows that the token Harry is followed by the token Potter, and we want to figure out how it does this. The logits on Harry are `residual @ W_U`. But this is a linear map, and the residual stream is the sum of all previous layers `residual = embed + attn_out_0 + attn_out_1`. So `logits = (embed @ W_U) + (attn_out @ W_U) + (attn_out_1 @ W_U)`\n",
                "\n",
                "We can be even more specific, and *just* look at the logit of the Potter token - this corresponds to a column of `W_U`, and so a direction in the residual stream - our logit is now a single number that is the sum of `(embed @ potter_U) + (attn_out_0 @ potter_U) + (attn_out_1 @ potter_U)`. Even better, we can decompose each attention layer output into the sum of the result of each head, and use this to get many terms.\n",
                "</details>\n",
                "\n",
                "Your mission here is to write a function to look at how much each component contributes to the correct logit. Your components are:\n",
                "\n",
                "* The direct path (i.e. the residual connections from the embedding to unembedding),\n",
                "* Each layer 0 head (via the residual connection and skipping layer 1)\n",
                "* Each layer 1 head\n",
                "\n",
                "To emphasize, these are not paths from the start to the end of the model, these are paths from the output of some component directly to the logits - we make no assumptions about how each path was calculated!\n",
                "A few important notes for this exercise:\n",
                "\n",
                "* Here we are only examining the DIRECT effect on the logits, i.e. the information that this component writes/embeds into the residual stream - we will not capture interactions where heads combine with other heads to influence logits, or suppress logits for other tokens to enhance the correct one.\n",
                "* When focusing only on the logits corresponding to the correct token, our data is lower-dimensional because we can disregard all other tokens except the correct one (Handling a 50K vocab size can be tedious!). However, this approach may overlook more nuanced effects, such as a head suppressing other plausible logits to elevate the log probability of the correct one.\n",
                "    * There are other situations where our job might be easier. For instance, in the IOI task (which we'll discuss shortly) we're just comparing the logits of the indirect object to the logits of the direct object, meaning we can use the **difference between these logits**, and ignore all the other logits.\n",
                "* When calculating correct output logits, we will get tensors with a dimension `(position - 1,)`, not `(position,)` - we remove the final element of the output (logits), and the first element of labels (tokens). This is because we're predicting the *next* token, and we don't know the token after the final token, so we ignore it.\n",
                "\n",
                "<details>\n",
                "\n",
                "<summary>Question - why don't we do this to the log probs instead?</summary>\n",
                "\n",
                "Because log probs aren't linear, they go through `log_softmax`, a non-linear function.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Exercise - build logit attribution tool\n",
                "\n",
                "\n",
                "\n",
                "You should implement the `logit_attribution` function below. This should return the contribution of each component in the \"correct direction\". \n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_attribution_pattern(attribution_scores: Float[Tensor, \"layers heads\"]):\n",
                "    num_layers, num_heads = attribution_scores.shape\n",
                "\n",
                "    fig = px.imshow(\n",
                "        attribution_scores,\n",
                "        labels=dict(x=\"Heads\", y=\"Layers\"),\n",
                "        color_continuous_scale=\"Viridis\",\n",
                "        x=[f\"Head {i+1}\" for i in range(num_heads)],\n",
                "        y=[f\"Layer {i+1}\" for i in range(num_layers)],\n",
                "    )\n",
                "\n",
                "    fig.update_layout(\n",
                "        title={\"text\": \"Attribution Scores\", \"font\": {\"size\": 16}},\n",
                "        xaxis_title=\"Heads\",\n",
                "        yaxis_title=\"Layers\",\n",
                "        xaxis={\"tickfont\": {\"size\": 12}, \"tickangle\": -45},\n",
                "        yaxis={\"tickfont\": {\"size\": 12}},\n",
                "        coloraxis_colorbar={\"title\": \"\", \"tickfont\": {\"size\": 12}},\n",
                "    )\n",
                "\n",
                "    fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def logit_attribution(\n",
                "    tokens: Int[Tensor, \"batch seq\"],\n",
                "    model: HookedTransformer,\n",
                "    cache: ActivationCache,\n",
                "    token_position: int,\n",
                ") -> Float[Tensor, \"layers heads\"]:\n",
                "    \"\"\"\n",
                "    Computes the logit attribution for a specific token position in the input sequence.\n",
                "\n",
                "    Args:\n",
                "        tokens (Int[Tensor, \"batch seq\"]): The input token IDs tensor with shape (batch_size, sequence_length).\n",
                "        model (HookedTransformer): The HookedTransformer model instance.\n",
                "        cache (ActivationCache): The activation cache containing the intermediate results.\n",
                "        token_position (int): The position of the token in the input sequence for which to compute the attribution.\n",
                "\n",
                "    Returns:\n",
                "        Float[Tensor, \"layers heads\"]: The logit attribution tensor with shape (num_layers, num_heads).\n",
                "\n",
                "    Description:\n",
                "        This function computes the logit attribution for a specific token position in the input sequence.\n",
                "        It unembeds the output of each attention head in each layer, and sees what upweight it gives on the correct next token.\n",
                "\n",
                "    Note:\n",
                "        - The input `tokens` tensor is assumed to have a batch size of 1.\n",
                "        - The `token_position` is zero-indexed, meaning the first token in the sequence has a position of 0.\n",
                "        - The returned attention pattern has shape (num_layers, num_heads), representing the attribution scores\n",
                "          for each layer and attention head.\n",
                "    \"\"\"\n",
                "    # Retrieve the attention results from the activation cache for each transformer block\n",
                "    ...  # TODO: ~14 words\n",
                "\n",
                "    # Stack the attention results along the layer dimension\n",
                "    ...  # TODO: ~6 words\n",
                "\n",
                "    # Select the attention results corresponding to the specified token position\n",
                "    ...  # TODO: ~3 words\n",
                "\n",
                "    # Pass the selected attention results through the model's unembed function to obtain the logits\n",
                "    ...  # TODO: ~4 words\n",
                "\n",
                "    # Get the ID of the next token in the sequence\n",
                "    ...  # TODO: ~5 words\n",
                "\n",
                "    # Extract the logits corresponding to the next token ID\n",
                "    ...  # TODO: ~3 words\n",
                "\n",
                "    return attributions\n",
                "\n",
                "\n",
                "test_logit_attribution(logit_attribution, model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "def logit_attribution(\n",
                "    tokens: Int[Tensor, \"batch seq\"],\n",
                "    model: HookedTransformer,\n",
                "    cache: ActivationCache,\n",
                "    token_position: int,\n",
                ") -> Float[Tensor, \"layers heads\"]:\n",
                "    \"\"\"\n",
                "    Computes the logit attribution for a specific token position in the input sequence.\n",
                "\n",
                "    Args:\n",
                "        tokens (Int[Tensor, \"batch seq\"]): The input token IDs tensor with shape (batch_size, sequence_length).\n",
                "        model (HookedTransformer): The HookedTransformer model instance.\n",
                "        cache (ActivationCache): The activation cache containing the intermediate results.\n",
                "        token_position (int): The position of the token in the input sequence for which to compute the attribution.\n",
                "\n",
                "    Returns:\n",
                "        Float[Tensor, \"layers heads\"]: The logit attribution tensor with shape (num_layers, num_heads).\n",
                "\n",
                "    Description:\n",
                "        This function computes the logit attribution for a specific token position in the input sequence.\n",
                "        It unembeds the output of each attention head in each layer, and sees what upweight it gives on the correct next token.\n",
                "\n",
                "    Note:\n",
                "        - The input `tokens` tensor is assumed to have a batch size of 1.\n",
                "        - The `token_position` is zero-indexed, meaning the first token in the sequence has a position of 0.\n",
                "        - The returned attention pattern has shape (num_layers, num_heads), representing the attribution scores\n",
                "          for each layer and attention head.\n",
                "    \"\"\"\n",
                "    # Retrieve the attention results from the activation cache for each transformer block\n",
                "    results = [cache[f\"blocks.{i}.attn.hook_result\"] for i in range(len(model.blocks))]\n",
                "\n",
                "    # Stack the attention results along the layer dimension\n",
                "    results = torch.stack(results, dim=1)\n",
                "\n",
                "    # Select the attention results corresponding to the specified token position\n",
                "    results = results[token_position, :, :, :]\n",
                "\n",
                "    # Pass the selected attention results through the model's unembed function to obtain the logits\n",
                "    logits = model.unembed(results)\n",
                "\n",
                "    # Get the ID of the next token in the sequence\n",
                "    next_token_id = tokens[0, token_position + 1]\n",
                "\n",
                "    # Extract the logits corresponding to the next token ID\n",
                "    attributions = logits[:, :, next_token_id]\n",
                "\n",
                "    return attributions\n",
                "\n",
                "\n",
                "test_logit_attribution(logit_attribution, model)",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that you have the tool to see which heads have which effects on the logits, run some experiments to see which heads are useful for induction.\n",
                "Are they the same as those that have the pattern you identified earlier?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "repeat_seq = gpt2.to_tokens(text)\n",
                "\n",
                "...  # TODO: ~32 words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "repeat_seq = gpt2.to_tokens(text)\n",
                "\n",
                "gpt_logits, gpt_cache = gpt2.run_with_cache(repeat_seq, remove_batch_dim=True)\n",
                "toymodel_logits, toymodel_cache = model.run_with_cache(repeat_seq, remove_batch_dim=True)\n",
                "\n",
                "token_position = 14\n",
                "attribution_scores = logit_attribution(repeat_seq, model, toymodel_cache, token_position)\n",
                "plot_attribution_pattern(attribution_scores)\n",
                "\n",
                "attribution_scores = logit_attribution(repeat_seq, gpt2, gpt_cache, token_position)\n",
                "plot_attribution_pattern(attribution_scores)\n",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
