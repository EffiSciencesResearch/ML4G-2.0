{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating high dimensional tensors\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/tensors/tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Thanks to Callum McDougall for this Notebook that was adapted from the prerequisites of [ARENA 3.0](https://arena3-chapter0-fundamentals.streamlit.app/[0.0]_Prerequisites).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/pres2.png\" width=\"350\">\n",
    "\n",
    "This notebook contains important prerequisites for the rest of the technical material.\n",
    "\n",
    "\n",
    "> ### Learning objectives\n",
    ">\n",
    "> - Be more familiar manipulating tensors with multiple dimensions\n",
    "> - Understand the basics of Einstein summation convention\n",
    "> - Learn how to use `einops` to perform basic tensor rearrangement, and `einsum` to to perform standard linear algebra operations on tensors\n",
    "> - Remember: the goal is not to pass the exercises but to understand what happens and why.\n",
    "\n",
    "**Note**: this section contains a large number of exercises. You should generally feel free to skim through them (skipping some or most) if you feel comfortable with the basic ideas.\n",
    "A good rule is to stop once you are familiar enough so that you know that you will be able to solve the other exercises without too much effort. However this point is not the same for everyone, that's why we have a lot of exercises for those who want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (don't read, just run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    %pip install einops\n",
    "\n",
    "    # Code to make sure output widgets display\n",
    "    from google.colab import output\n",
    "\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
    "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/tensors/*'\n",
    "    !mv --no-clobber ML4G-2.0-master/workshops/tensors/* .\n",
    "    !rm -r ML4G-2.0-master\n",
    "\n",
    "    print(\"Imports & installations complete!\")\n",
    "\n",
    "else:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import einops\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from utils import show_array_as_img, show_target_image\n",
    "import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einops\n",
    "\n",
    "### Reading\n",
    "\n",
    "* Read about the benefits of the `einops` library [here](https://www.blopig.com/blog/2022/05/einops-powerful-library-for-tensor-operations-in-deep-learning/).\n",
    "* If you haven't already, then review the [Einops basics tutorial](https://einops.rocks/1-einops-basics/) (up to the \"fancy examples\" section). Try not to look it up too much while you're doing the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"numbers.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`arr` is a 4D numpy array. The first axes corresponds to the number from 0 to 5, and the next three axes are channels (i.e. RGB), height and width respectively. You have the function `show_array_as_img` which takes in a numpy array and displays it as an image. There are two possible ways this function can be run:\n",
    "\n",
    "* If the input is three-dimensional, the dimensions are interpreted as `(channel, height, width)` - in other words, as an RGB image.\n",
    "* If the input is two-dimensional, the dimensions are interpreted as `(height, width)` - i.e. a monochrome image.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand tensors and their transformation, printing their shape is very useful\n",
    "print(arr.shape)\n",
    "# Here, ploting 3D slices will also be useful\n",
    "show_array_as_img(arr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of images follow below, which have been created using `einops` functions performed on `arr`. You should work through these and try to produce each of the images yourself. This page also includes solutions, but you should only look at them after you've tried for at least five minutes.\n",
    "\n",
    "> *Note - if you find you're comfortable with the first ~half of these, you can skip to later sections if you'd prefer, since these aren't particularly conceptually important.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einops exercises - images\n",
    "\n",
    "```c\n",
    "Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "\n",
    "You should spend up to ~45 minutes on these exercises collectively.\n",
    "\n",
    "If you think you get the general idea, then you can skip to the next section.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr1\n",
    "arr1 = ...\n",
    "show_array_as_img(arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr1 = einops.rearrange(arr, \"number channel height width -> channel height (number width)\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr2\n",
    "show_array_as_img(arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr2 = einops.repeat(arr[0], \"channel height width -> channel (2 height) width\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr3\n",
    "show_array_as_img(arr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr3 = einops.repeat(arr[0:2], \"b c h w -> c (b h) (2 w)\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr4\n",
    "show_array_as_img(arr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr4 = einops.repeat(arr[0], \"c h w -> c (h 2) w\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr5\n",
    "show_array_as_img(arr5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr5 = einops.rearrange(arr[0], \"c h w -> h (c w)\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr6\n",
    "show_array_as_img(arr6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr6 = einops.rearrange(arr, \"(b1 b2) c h w -> c (b1 h) (b2 w)\", b1=2)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr7\n",
    "show_array_as_img(arr7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr7 = einops.reduce(arr, \"b c h w -> h (b w)\", \"max\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr8\n",
    "show_array_as_img(arr8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr8 = einops.reduce(arr, \"b c h w -> h w\", \"min\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr9\n",
    "show_array_as_img(arr9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr9 = einops.rearrange(arr[1], \"c h w -> c w h\")\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_target_image(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - define arr10\n",
    "show_array_as_img(arr10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "arr10 = einops.reduce(arr, \"(b1 b2) c (h h2) (w w2) -> c (b1 h) (b2 w)\", \"max\", h2=2, w2=2, b1=2)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einops exercises - operations\n",
    "\n",
    "```c\n",
    "Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "\n",
    "You should spend up to ~45 minutes on these exercises collectively.\n",
    "\n",
    "If you think you get the general idea, then you can skip to the next section.\n",
    "```\n",
    "\n",
    "Next, we have a series of functions which you should implement using `einops`. In the dropdown below these exercises, you can find solutions to all of them.\n",
    "\n",
    "First, let's define some functions to help us test our solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_all_equal(actual: Tensor, expected: Tensor) -> None:\n",
    "    assert actual.shape == expected.shape, f\"Shape mismatch, got: {actual.shape}\"\n",
    "    assert (actual == expected).all(), f\"Value mismatch, got: {actual}\"\n",
    "    print(\"Passed!\")\n",
    "\n",
    "\n",
    "def assert_all_close(actual: Tensor, expected: Tensor, rtol=1e-05, atol=0.0001) -> None:\n",
    "    assert actual.shape == expected.shape, f\"Shape mismatch, got: {actual.shape}\"\n",
    "    assert torch.allclose(actual, expected, rtol=rtol, atol=atol)\n",
    "    print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A.1 - rearrange (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_1() -> Tensor:\n",
    "    \"\"\"Return the following tensor using only torch.arange and einops.rearrange:\n",
    "\n",
    "    [[3, 4],\n",
    "     [5, 6],\n",
    "     [7, 8]]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "expected = torch.tensor([[3, 4], [5, 6], [7, 8]])\n",
    "assert_all_equal(rearrange_1(), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def rearrange_1() -> Tensor:\n",
    "    \"\"\"Return the following tensor using only torch.arange and einops.rearrange:\n",
    "\n",
    "    [[3, 4],\n",
    "     [5, 6],\n",
    "     [7, 8]]\n",
    "    \"\"\"\n",
    "    return einops.rearrange(torch.arange(3, 9), \"(h w) -> h w\", h=3, w=2)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A.2 - rearrange (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_2() -> Tensor:\n",
    "    \"\"\"Return the following tensor using only torch.arange and einops.rearrange:\n",
    "\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "assert_all_equal(rearrange_2(), torch.tensor([[1, 2, 3], [4, 5, 6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def rearrange_2() -> Tensor:\n",
    "    \"\"\"Return the following tensor using only torch.arange and einops.rearrange:\n",
    "\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    "    \"\"\"\n",
    "    return einops.rearrange(torch.arange(1, 7), \"(h w) -> h w\", h=2, w=3)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise A.3 - rearrange (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_3() -> Tensor:\n",
    "    \"\"\"Return the following tensor using only torch.arange and einops.rearrange:\n",
    "\n",
    "    [[[1], [2], [3], [4], [5], [6]]]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "assert_all_equal(rearrange_3(), torch.tensor([[[1], [2], [3], [4], [5], [6]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def rearrange_3() -> Tensor:\n",
    "    \"\"\"Return the following tensor using only torch.arange and einops.rearrange:\n",
    "\n",
    "    [[[1], [2], [3], [4], [5], [6]]]\n",
    "    \"\"\"\n",
    "    return einops.rearrange(torch.arange(1, 7), \"a -> 1 a 1\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise B.1 - temperature average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperatures_average(temps: Tensor) -> Tensor:\n",
    "    \"\"\"Return the average temperature for each week.\n",
    "\n",
    "    temps: a 1D temperature containing temperatures for each day.\n",
    "    Length will be a multiple of 7 and the first 7 days are for the first week, second 7 days for the second week, etc.\n",
    "\n",
    "    You can do this with a single call to reduce.\n",
    "    \"\"\"\n",
    "    assert len(temps) % 7 == 0\n",
    "    pass\n",
    "\n",
    "\n",
    "temps = Tensor([71, 72, 70, 75, 71, 72, 70, 68, 65, 60, 68, 60, 55, 59, 75, 80, 85, 80, 78, 72, 83])\n",
    "expected = Tensor([71.5714, 62.1429, 79.0])\n",
    "assert_all_close(temperatures_average(temps), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def temperatures_average(temps: Tensor) -> Tensor:\n",
    "    \"\"\"Return the average temperature for each week.\n",
    "\n",
    "    temps: a 1D temperature containing temperatures for each day.\n",
    "    Length will be a multiple of 7 and the first 7 days are for the first week, second 7 days for the second week, etc.\n",
    "\n",
    "    You can do this with a single call to reduce.\n",
    "    \"\"\"\n",
    "    assert len(temps) % 7 == 0\n",
    "    return einops.reduce(temps, \"(h 7) -> h\", \"mean\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise B.2 - temperature difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperatures_differences(temps: Tensor) -> Tensor:\n",
    "    \"\"\"For each day, subtract the average for the week the day belongs to.\n",
    "\n",
    "    temps: as above\n",
    "    \"\"\"\n",
    "    assert len(temps) % 7 == 0\n",
    "    pass\n",
    "\n",
    "\n",
    "expected = Tensor(\n",
    "    [\n",
    "        -0.5714,\n",
    "        0.4286,\n",
    "        -1.5714,\n",
    "        3.4286,\n",
    "        -0.5714,\n",
    "        0.4286,\n",
    "        -1.5714,\n",
    "        5.8571,\n",
    "        2.8571,\n",
    "        -2.1429,\n",
    "        5.8571,\n",
    "        -2.1429,\n",
    "        -7.1429,\n",
    "        -3.1429,\n",
    "        -4.0,\n",
    "        1.0,\n",
    "        6.0,\n",
    "        1.0,\n",
    "        -1.0,\n",
    "        -7.0,\n",
    "        4.0,\n",
    "    ]\n",
    ")\n",
    "actual = temperatures_differences(temps)\n",
    "assert_all_close(actual, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def temperatures_differences(temps: Tensor) -> Tensor:\n",
    "    \"\"\"For each day, subtract the average for the week the day belongs to.\n",
    "\n",
    "    temps: as above\n",
    "    \"\"\"\n",
    "    assert len(temps) % 7 == 0\n",
    "    avg = einops.repeat(temperatures_average(temps), \"w -> (w 7)\")\n",
    "\treturn temps - avg\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise B.3 - temperature normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperatures_normalized(temps: Tensor) -> Tensor:\n",
    "    \"\"\"For each day, subtract the weekly average and divide by the weekly standard deviation.\n",
    "\n",
    "    temps: as above\n",
    "\n",
    "    Pass torch.std to reduce.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "expected = Tensor(\n",
    "    [\n",
    "        -0.3326,\n",
    "        0.2494,\n",
    "        -0.9146,\n",
    "        1.9954,\n",
    "        -0.3326,\n",
    "        0.2494,\n",
    "        -0.9146,\n",
    "        1.1839,\n",
    "        0.5775,\n",
    "        -0.4331,\n",
    "        1.1839,\n",
    "        -0.4331,\n",
    "        -1.4438,\n",
    "        -0.6353,\n",
    "        -0.8944,\n",
    "        0.2236,\n",
    "        1.3416,\n",
    "        0.2236,\n",
    "        -0.2236,\n",
    "        -1.5652,\n",
    "        0.8944,\n",
    "    ]\n",
    ")\n",
    "actual = temperatures_normalized(temps)\n",
    "assert_all_close(actual, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def temperatures_normalized(temps: Tensor) -> Tensor:\n",
    "    \"\"\"For each day, subtract the weekly average and divide by the weekly standard deviation.\n",
    "\n",
    "    temps: as above\n",
    "\n",
    "    Pass torch.std to reduce.\n",
    "    \"\"\"\n",
    "\tavg = einops.repeat(temperatures_average(temps), \"w -> (w 7)\")\n",
    "\tstd = einops.repeat(einops.reduce(temps, \"(h 7) -> h\", torch.std), \"w -> (w 7)\")\n",
    "\treturn (temps - avg) / std\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise C - identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_matrix(n: int) -> Tensor:\n",
    "    \"\"\"Return the identity matrix of size nxn.\n",
    "\n",
    "    Don't use torch.eye or similar.\n",
    "\n",
    "    Hint: you can do it with arange, rearrange, and ==.\n",
    "    Bonus: find a different way to do it.\n",
    "    \"\"\"\n",
    "    assert n >= 0\n",
    "    pass\n",
    "\n",
    "\n",
    "assert_all_equal(identity_matrix(3), Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n",
    "assert_all_equal(identity_matrix(0), torch.zeros((0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def identity_matrix(n: int) -> Tensor\n",
    "    \"\"\"Return the identity matrix of size nxn.\n",
    "\n",
    "    Don't use torch.eye or similar.\n",
    "\n",
    "    Hint: you can do it with arange, rearrange, and ==.\n",
    "    Bonus: find a different way to do it.\n",
    "    \"\"\"\n",
    "    assert n >= 0\n",
    "    return (einops.rearrange(torch.arange(n), \"i->i 1\") == torch.arange(n)).float()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Exercise D - sample distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distribution(probs: Tensor, n: int) -> Tensor:\n",
    "    \"\"\"Return n random samples from probs, where probs is a normalized probability distribution.\n",
    "\n",
    "    probs: shape (k,) where probs[i] is the probability of event i occurring.\n",
    "    n: number of random samples\n",
    "\n",
    "    Return: shape (n,) where out[i] is an integer indicating which event was sampled.\n",
    "\n",
    "    Use torch.rand and torch.cumsum to do this without any explicit loops.\n",
    "\n",
    "    Note: if you think your solution is correct but the test is failing, try increasing the value of n.\n",
    "    \"\"\"\n",
    "    assert abs(probs.sum() - 1.0) < 0.001\n",
    "    assert (probs >= 0).all()\n",
    "    pass\n",
    "\n",
    "\n",
    "n = 10000000\n",
    "probs = Tensor([0.05, 0.1, 0.1, 0.2, 0.15, 0.4])\n",
    "freqs = torch.bincount(sample_distribution(probs, n)) / n\n",
    "assert_all_close(freqs, probs, rtol=0.001, atol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def sample_distribution(probs: Tensorn: int) -> TeTensor\n",
    "    \"\"\"Return n random samples from probs, where probs is a normalized probability distribution.\n",
    "\n",
    "    probs: shape (k,) where probs[i] is the probability of event i occurring.\n",
    "    n: number of random samples\n",
    "\n",
    "    Return: shape (n,) where out[i] is an integer indicating which event was sampled.\n",
    "\n",
    "    Use torch.rand and torch.cumsum to do this without any explicit loops.\n",
    "\n",
    "    Note: if you think your solution is correct but the test is failing, try increasing the value of n.\n",
    "    \"\"\"\n",
    "    assert abs(probs.sum() - 1.0) < 0.001\n",
    "    assert (probs >= 0).all()\n",
    "    return (torch.rand(n, 1) > torch.cumsum(probs, dim=0)).sum(dim=-1)\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise E - classifier accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_accuracy(scores: Tensor, true_classes: Tensor) -> Tensor:\n",
    "    \"\"\"Return the fraction of inputs for which the maximum score corresponds to the true class for that input.\n",
    "\n",
    "    scores: shape (batch, n_classes). A higher score[b, i] means that the classifier thinks class i is more likely.\n",
    "    true_classes: shape (batch, ). true_classes[b] is an integer from [0...n_classes).\n",
    "\n",
    "    Use torch.argmax.\n",
    "    \"\"\"\n",
    "    assert true_classes.max() < scores.shape[1]\n",
    "    pass\n",
    "\n",
    "\n",
    "scores = Tensor([[0.75, 0.5, 0.25], [0.1, 0.5, 0.4], [0.1, 0.7, 0.2]])\n",
    "true_classes = Tensor([0, 1, 0])\n",
    "expected = 2.0 / 3.0\n",
    "assert classifier_accuracy(scores, true_classes) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def classifier_accuracy(scores: Tensortrue_classes: TeTensor TensTensor\n",
    "    \"\"\"Return the fraction of inputs for which the maximum score corresponds to the true class for that input.\n",
    "\n",
    "    scores: shape (batch, n_classes). A higher score[b, i] means that the classifier thinks class i is more likely.\n",
    "    true_classes: shape (batch, ). true_classes[b] is an integer from [0...n_classes).\n",
    "\n",
    "    Use torch.argmax.\n",
    "    \"\"\"\n",
    "    assert true_classes.max() < scores.shape[1]\n",
    "    return (scores.argmax(dim=1) == true_classes).float().mean()\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise F.1 - total price indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_price_indexing(prices: Tensor, items: Tensor) -> float:\n",
    "    \"\"\"Given prices for each kind of item and a tensor of items purchased, return the total price.\n",
    "\n",
    "    prices: shape (k, ). prices[i] is the price of the ith item.\n",
    "    items: shape (n, ). A 1D tensor where each value is an item index from [0..k).\n",
    "\n",
    "    Use integer array indexing. The below document describes this for NumPy but it's the same in PyTorch:\n",
    "\n",
    "    https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n",
    "    \"\"\"\n",
    "    assert items.max() < prices.shape[0]\n",
    "    pass\n",
    "\n",
    "\n",
    "prices = Tensor([0.5, 1, 1.5, 2, 2.5])\n",
    "items = Tensor([0, 0, 1, 1, 4, 3, 2])\n",
    "assert total_price_indexing(prices, items) == 9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def total_price_indexing(prices: Tensoritems: TeTensor float:\n",
    "    \"\"\"Given prices for each kind of item and a tensor of items purchased, return the total price.\n",
    "\n",
    "    prices: shape (k, ). prices[i] is the price of the ith item.\n",
    "    items: shape (n, ). A 1D tensor where each value is an item index from [0..k).\n",
    "\n",
    "    Use integer array indexing. The below document describes this for NumPy but it's the same in PyTorch:\n",
    "\n",
    "    https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n",
    "    \"\"\"\n",
    "    assert items.max() < prices.shape[0]\n",
    "    return prices[items].sum().item()\n",
    "```\n",
    "</details>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise F.2 - gather 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_2d(matrix: Tensor, indexes: Tensor) -> Tensor:\n",
    "    \"\"\"Perform a gather operation along the second dimension.\n",
    "\n",
    "    matrix: shape (m, n)\n",
    "    indexes: shape (m, k)\n",
    "\n",
    "    Return: shape (m, k). out[i][j] = matrix[i][indexes[i][j]]\n",
    "\n",
    "    For this problem, the test already passes and it's your job to write at least three asserts relating the arguments and the output. This is a tricky function and worth spending some time to wrap your head around its behavior.\n",
    "\n",
    "    See: https://pytorch.org/docs/stable/generated/torch.gather.html?highlight=gather#torch.gather\n",
    "    \"\"\"\n",
    "    \"TODO: YOUR CODE HERE\"\n",
    "    out = matrix.gather(1, indexes)\n",
    "    \"TODO: YOUR CODE HERE\"\n",
    "    return out\n",
    "\n",
    "\n",
    "matrix = torch.arange(15).view(3, 5)\n",
    "indexes = torch.tensor([[4], [3], [2]])\n",
    "expected = Tensor([[4], [8], [12]])\n",
    "assert_all_equal(gather_2d(matrix, indexes), expected)\n",
    "indexes2 = torch.tensor([[2, 4], [1, 3], [0, 2]])\n",
    "expected2 = Tensor([[2, 4], [6, 8], [10, 12]])\n",
    "assert_all_equal(gather_2d(matrix, indexes2), expected2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def gather_2d(matrix: Tensor, indexes: Tensor) -> Tensor:\n",
    "    \"\"\"Perform a gather operation along the second dimension.\n",
    "\n",
    "    matrix: shape (m, n)\n",
    "    indexes: shape (m, k)\n",
    "\n",
    "    Return: shape (m, k). out[i][j] = matrix[i][indexes[i][j]]\n",
    "\n",
    "    For this problem, the test already passes and it's your job to write at least three asserts relating the arguments and the output. This is a tricky function and worth spending some time to wrap your head around its behavior.\n",
    "\n",
    "    See: https://pytorch.org/docs/stable/generated/torch.gather.html?highlight=gather#torch.gather\n",
    "    \"\"\"\n",
    "\tassert matrix.ndim == indexes.ndim\n",
    "\tassert indexes.shape[0] <= matrix.shape[0]\n",
    "\tout = matrix.gather(1, indexes)\n",
    "\tassert out.shape == indexes.shape\n",
    "\treturn out\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise F.3 - total price gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_price_gather(prices: Tensor, items: Tensor) -> float:\n",
    "    \"\"\"Compute the same as total_price_indexing, but use torch.gather.\"\"\"\n",
    "    assert items.max() < prices.shape[0]\n",
    "    pass\n",
    "\n",
    "\n",
    "prices = Tensor([0.5, 1, 1.5, 2, 2.5])\n",
    "items = Tensor([0, 0, 1, 1, 4, 3, 2])\n",
    "assert total_price_gather(prices, items) == 9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def total_price_gather(prices: Tensoritems: TeTensor float:\n",
    "    \"\"\"Compute the same as total_price_indexing, but use torch.gather.\"\"\"\n",
    "    assert items.max() < prices.shape[0]\n",
    "    return prices.gather(0, items).sum().item()\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise G - indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_array_indexing(matrix: Tensor, coords: Tensor) -> Tensor:\n",
    "    \"\"\"Return the values at each coordinate using integer array indexing.\n",
    "\n",
    "    For details on integer array indexing, see:\n",
    "    https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n",
    "\n",
    "    matrix: shape (d_0, d_1, ..., d_n)\n",
    "    coords: shape (batch, n)\n",
    "\n",
    "    Return: (batch, )\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "mat_2d = torch.arange(15).view(3, 5)\n",
    "coords_2d = Tensor([[0, 1], [0, 4], [1, 4]])\n",
    "actual = integer_array_indexing(mat_2d, coords_2d)\n",
    "assert_all_equal(actual, Tensor([1, 4, 9]))\n",
    "mat_3d = torch.arange(2 * 3 * 4).view((2, 3, 4))\n",
    "coords_3d = Tensor([[0, 0, 0], [0, 1, 1], [0, 2, 2], [1, 0, 3], [1, 2, 0]])\n",
    "actual = integer_array_indexing(mat_3d, coords_3d)\n",
    "assert_all_equal(actual, Tensor([0, 5, 10, 15, 20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def integer_array_indexing(matrix: Tensorcoords: TeTensor TensTensor\n",
    "    \"\"\"Return the values at each coordinate using integer array indexing.\n",
    "\n",
    "    For details on integer array indexing, see:\n",
    "    https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing\n",
    "\n",
    "    matrix: shape (d_0, d_1, ..., d_n)\n",
    "    coords: shape (batch, n)\n",
    "\n",
    "    Return: (batch, )\n",
    "    \"\"\"\n",
    "    return matrix[tuple(coords.T)]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Exercise H.1 - batched logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_logsumexp(matrix: Tensor) -> Tensor:\n",
    "    \"\"\"For each row of the matrix, compute log(sum(exp(row))) in a numerically stable way.\n",
    "\n",
    "    matrix: shape (batch, n)\n",
    "\n",
    "    Return: (batch, ). For each i, out[i] = log(sum(exp(matrix[i]))).\n",
    "\n",
    "    Do this without using PyTorch's logsumexp function.\n",
    "\n",
    "    A couple useful blogs about this function:\n",
    "    - https://leimao.github.io/blog/LogSumExp/\n",
    "    - https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "matrix = Tensor([[-1000, -1000, -1000, -1000], [1000, 1000, 1000, 1000]])\n",
    "expected = Tensor([-1000 + math.log(4), 1000 + math.log(4)])\n",
    "actual = batched_logsumexp(matrix)\n",
    "assert_all_close(actual, expected)\n",
    "matrix2 = torch.randn((10, 20))\n",
    "expected2 = torch.logsumexp(matrix2, dim=-1)\n",
    "actual2 = batched_logsumexp(matrix2)\n",
    "assert_all_close(actual2, expected2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def batched_logsumexp(matrix: Tensor) -> Tensor:\n",
    "    \"\"\"For each row of the matrix, compute log(sum(exp(row))) in a numerically stable way.\n",
    "\n",
    "    matrix: shape (batch, n)\n",
    "\n",
    "    Return: (batch, ). For each i, out[i] = log(sum(exp(matrix[i]))).\n",
    "\n",
    "    Do this without using PyTorch's logsumexp function.\n",
    "\n",
    "    A couple useful blogs about this function:\n",
    "    - https://leimao.github.io/blog/LogSumExp/\n",
    "    - https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
    "    \"\"\"\n",
    "    C = matrix.max(dim=-1).values\n",
    "\texps = torch.exp(matrix - einops.rearrange(C, \"n -> n 1\"))\n",
    "\treturn C + torch.log(torch.sum(exps, dim=-1))\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise H.2 - batched softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_softmax(matrix: Tensor) -> Tensor:\n",
    "    \"\"\"For each row of the matrix, compute softmax(row).\n",
    "\n",
    "    Do this without using PyTorch's softmax function.\n",
    "    Instead, use the definition of softmax: https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "    matrix: shape (batch, n)\n",
    "\n",
    "    Return: (batch, n). For each i, out[i] should sum to 1.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "matrix = torch.arange(1, 6).view((1, 5)).float().log()\n",
    "expected = torch.arange(1, 6).view((1, 5)) / 15.0\n",
    "actual = batched_softmax(matrix)\n",
    "assert_all_close(actual, expected)\n",
    "for i in [0.12, 3.4, -5, 6.7]:\n",
    "    assert_all_close(actual, batched_softmax(matrix + i))\n",
    "matrix2 = torch.rand((10, 20))\n",
    "actual2 = batched_softmax(matrix2)\n",
    "assert actual2.min() >= 0.0\n",
    "assert actual2.max() <= 1.0\n",
    "assert_all_equal(actual2.argsort(), matrix2.argsort())\n",
    "assert_all_close(actual2.sum(dim=-1), torch.ones(matrix2.shape[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def batched_softmax(matrix: Tensor-> TeTensor\n",
    "    \"\"\"For each row of the matrix, compute softmax(row).\n",
    "\n",
    "    Do this without using PyTorch's softmax function.\n",
    "    Instead, use the definition of softmax: https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "    matrix: shape (batch, n)\n",
    "\n",
    "    Return: (batch, n). For each i, out[i] should sum to 1.\n",
    "    \"\"\"\n",
    "    exp = matrix.exp()\n",
    "\treturn exp / exp.sum(dim=-1, keepdim=True)\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise H.3 - batched logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_logsoftmax(matrix: Tensor) -> Tensor:\n",
    "    \"\"\"Compute log(softmax(row)) for each row of the matrix.\n",
    "\n",
    "    matrix: shape (batch, n)\n",
    "\n",
    "    Return: (batch, n). For each i, out[i] should sum to 1.\n",
    "\n",
    "    Do this without using PyTorch's logsoftmax function.\n",
    "    For each row, subtract the maximum first to avoid overflow if the row contains large values.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "matrix = torch.arange(1, 6).view((1, 5)).float()\n",
    "start = 1000\n",
    "matrix2 = torch.arange(start + 1, start + 6).view((1, 5)).float()\n",
    "actual = batched_logsoftmax(matrix2)\n",
    "expected = Tensor([[-4.4519, -3.4519, -2.4519, -1.4519, -0.4519]])\n",
    "assert_all_close(actual, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def batched_logsoftmax(matrix: Tensor) -> Tensor:\n",
    "    \"\"\"Compute log(softmax(row)) for each row of the matrix.\n",
    "\n",
    "    matrix: shape (batch, n)\n",
    "\n",
    "    Return: (batch, n). For each i, out[i] should sum to 1.\n",
    "\n",
    "    Do this without using PyTorch's logsoftmax function.\n",
    "    For each row, subtract the maximum first to avoid overflow if the row contains large values.\n",
    "    \"\"\"\n",
    "    C = matrix.max(dim=1, keepdim=True).values\n",
    "\treturn matrix - C - (matrix - C).exp().sum(dim=1, keepdim=True).log()\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise H.4 - batched cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_cross_entropy_loss(logits: Tensor, true_labels: Tensor) -> Tensor:\n",
    "    \"\"\"Compute the cross entropy loss for each example in the batch.\n",
    "\n",
    "    logits: shape (batch, classes). logits[i][j] is the unnormalized prediction for example i and class j.\n",
    "    true_labels: shape (batch, ). true_labels[i] is an integer index representing the true class for example i.\n",
    "\n",
    "    Return: shape (batch, ). out[i] is the loss for example i.\n",
    "\n",
    "    Hint: convert the logits to log-probabilities using your batched_logsoftmax from above.\n",
    "    Then the loss for an example is just the negative of the log-probability that the model assigned to the true class. Use torch.gather to perform the indexing.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "logits = Tensor([[float(\"-inf\"), float(\"-inf\"), 0], [1 / 3, 1 / 3, 1 / 3], [float(\"-inf\"), 0, 0]])\n",
    "true_labels = Tensor([2, 0, 0])\n",
    "expected = Tensor([0.0, math.log(3), float(\"inf\")])\n",
    "actual = batched_cross_entropy_loss(logits, true_labels)\n",
    "assert_all_close(actual, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def batched_cross_entropy_loss(logits: Tensor, true_labels: Tensor) -> Tensor:\n",
    "    \"\"\"Compute the cross entropy loss for each example in the batch.\n",
    "\n",
    "    logits: shape (batch, classes). logits[i][j] is the unnormalized prediction for example i and class j.\n",
    "    true_labels: shape (batch, ). true_labels[i] is an integer index representing the true class for example i.\n",
    "\n",
    "    Return: shape (batch, ). out[i] is the loss for example i.\n",
    "\n",
    "    Hint: convert the logits to log-probabilities using your batched_logsoftmax from above.\n",
    "    Then the loss for an example is just the negative of the log-probability that the model assigned to the true class. Use torch.gather to perform the indexing.\n",
    "    \"\"\"\n",
    "\tassert logits.shape[0] == true_labels.shape[0]\n",
    "\tassert true_labels.max() < logits.shape[1]\n",
    "\n",
    "    logprobs = batched_logsoftmax(logits)\n",
    "    indices = einops.rearrange(true_labels, \"n -> n 1\")\n",
    "    pred_at_index = logprobs.gather(1, indices)\n",
    "    return - einops.rearrange(pred_at_index, \"n 1 -> n\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise I.1 - collect rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rows(matrix: Tensor, row_indexes: Tensor) -> Tensor:\n",
    "    \"\"\"Return a 2D matrix whose rows are taken from the input matrix in order according to row_indexes.\n",
    "\n",
    "    matrix: shape (m, n)\n",
    "    row_indexes: shape (k,). Each value is an integer in [0..m).\n",
    "\n",
    "    Return: shape (k, n). out[i] is matrix[row_indexes[i]].\n",
    "    \"\"\"\n",
    "    assert row_indexes.max() < matrix.shape[0]\n",
    "    pass\n",
    "\n",
    "\n",
    "matrix = torch.arange(15).view((5, 3))\n",
    "row_indexes = Tensor([0, 2, 1, 0])\n",
    "actual = collect_rows(matrix, row_indexes)\n",
    "expected = Tensor([[0, 1, 2], [6, 7, 8], [3, 4, 5], [0, 1, 2]])\n",
    "assert_all_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def collect_rows(matrix: Tensor, row_indexes: Tensor) -> Tensor:\n",
    "    \"\"\"Return a 2D matrix whose rows are taken from the input matrix in order according to row_indexes.\n",
    "\n",
    "    matrix: shape (m, n)\n",
    "    row_indexes: shape (k,). Each value is an integer in [0..m).\n",
    "\n",
    "    Return: shape (k, n). out[i] is matrix[row_indexes[i]].\n",
    "    \"\"\"\n",
    "    assert row_indexes.max() < matrix.shape[0]\n",
    "    return matrix[row_indexes]\n",
    "```\n",
    "</details>\n",
    "\n",
    "#### Exercise I.2 - collect columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_columns(matrix: Tensor, column_indexes: Tensor) -> Tensor:\n",
    "    \"\"\"Return a 2D matrix whose columns are taken from the input matrix in order according to column_indexes.\n",
    "\n",
    "    matrix: shape (m, n)\n",
    "    column_indexes: shape (k,). Each value is an integer in [0..n).\n",
    "\n",
    "    Return: shape (m, k). out[:, i] is matrix[:, column_indexes[i]].\n",
    "    \"\"\"\n",
    "    assert column_indexes.max() < matrix.shape[1]\n",
    "    pass\n",
    "\n",
    "\n",
    "matrix = torch.arange(15).view((5, 3))\n",
    "column_indexes = Tensor([0, 2, 1, 0])\n",
    "actual = collect_columns(matrix, column_indexes)\n",
    "expected = Tensor([[0, 2, 1, 0], [3, 5, 4, 3], [6, 8, 7, 6], [9, 11, 10, 9], [12, 14, 13, 12]])\n",
    "assert_all_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def collect_columns(matrix: Tensor, column_indexes: Tensor) -> Tensor:\n",
    "    \"\"\"Return a 2D matrix whose columns are taken from the input matrix in order according to column_indexes.\n",
    "\n",
    "    matrix: shape (m, n)\n",
    "    column_indexes: shape (k,). Each value is an integer in [0..n).\n",
    "\n",
    "    Return: shape (m, k). out[:, i] is matrix[:, column_indexes[i]].\n",
    "    \"\"\"\n",
    "    assert column_indexes.max() < matrix.shape[1]\n",
    "    return matrix[:, column_indexes]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einsum\n",
    "\n",
    "Einsum is a very useful function for performing linear operations, which you'll probably be using a lot during this programme.\n",
    "\n",
    "> Note - we'll be using the `einops.einsum` version of the function, which works differently to the more conventional `torch.einsum`:\n",
    ">\n",
    "> * `einops.einsum` has the arrays as the first arguments, and uses spaces to separate dimensions in the string.\n",
    "> * `torch.einsum` has the string as its first argument, and doesn't use spaces to separate dimensions (each dim is represented by a single character).\n",
    ">\n",
    "> For instance, `torch.einsum(\"ij,i->j\", A, b)` is equivalent to `einops.einsum(A, b, \"i j, i -> j\")`. (Note, einops doesn't care whether there are spaces either side of `,` and `->`, so you don't need to match this syntax exactly.)\n",
    "\n",
    "Although there are many different kinds of operations you can perform, they are all derived from three key rules:\n",
    "\n",
    "1. Repeating letters in different inputs means those values will be multiplied, and those products will be in the output.\n",
    "    * For example, `M = einops.einsum(A, B, \"i j, i j -> i j\")` just corresponds to the elementwise product `M = A * B` (because $M_{ij}=A_{ij}B_{ij}$).\n",
    "2. Omitting a letter means that the axis will be summed over.\n",
    "    * For example, if `x` is a 2D array with shape `(I, J)`, then `einops.einsum(x, \"i j -> i\")` will be a 1D array of length `I` containing the row sums of `x` (we're summing along the `j`-index, i.e. across rows).\n",
    "3. We can return the unsummed axes in any order.\n",
    "    * For example, `einops.einsum(x, \"i j k -> k j i\")` does the same thing as `einops.rearrange(x, \"i j k -> k j i\")`.\n",
    "\n",
    "*Note - the einops creators supposedly have plans to support shape rearrangement, e.g. with operations like `einops.einsum(x, y, \"i j, j k l -> i (k l)\")` (i.e. combining the features of rearrange and einsum), so we can all look forward to that day!*\n",
    "\n",
    "\n",
    "### Reading\n",
    "If this intro was too short, you can [read more about it here](https://rockt.github.io/2018/04/30/einsum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einsum exercises\n",
    "\n",
    "```c\n",
    "Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª\n",
    "\n",
    "You should spend up to 15-20 minutes on these exercises collectively.\n",
    "\n",
    "If you think you get the general idea, then you can skip to the next section.\n",
    "```\n",
    "\n",
    "In the following exercises, you'll write simple functions using `einsum` which replicate the functionality of standard NumPy functions: trace, matrix multiplication, inner and outer products. We've also included some test functions which you should run.\n",
    "\n",
    "Note - this version of einsum will require that you include `->`, even if you're summing to a scalar (i.e. the right hand side of your string expression is empty).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def einsum_trace(mat: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.trace`.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def einsum_mv(mat: np.ndarray, vec: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.matmul`, when `mat` is a 2D array and `vec` is 1D.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def einsum_mm(mat1: np.ndarray, mat2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.matmul`, when `mat1` and `mat2` are both 2D arrays.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def einsum_inner(vec1: np.ndarray, vec2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.inner`.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def einsum_outer(vec1: np.ndarray, vec2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.outer`.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "tests.test_einsum_trace(einsum_trace)\n",
    "tests.test_einsum_mv(einsum_mv)\n",
    "tests.test_einsum_mm(einsum_mm)\n",
    "tests.test_einsum_inner(einsum_inner)\n",
    "tests.test_einsum_outer(einsum_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "def einsum_trace(mat: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.trace`.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    return einops.einsum(mat, \"i i ->\")\n",
    "\n",
    "def einsum_mv(mat: np.ndarray, vec: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.matmul`, when `mat` is a 2D array and `vec` is 1D.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    return einops.einsum(mat, vec, \"i j, j -> i\")\n",
    "\n",
    "def einsum_mm(mat1: np.ndarray, mat2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.matmul`, when `mat1` and `mat2` are both 2D arrays.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    return einops.einsum(mat1, mat2, \"i j, j k -> i k\")\n",
    "\n",
    "def einsum_inner(vec1: np.ndarray, vec2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.inner`.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    return einops.einsum(vec1, vec2, \"i, i ->\")\n",
    "\n",
    "def einsum_outer(vec1: np.ndarray, vec2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns the same as `np.outer`.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    return einops.einsum(vec1, vec2, \"i, j -> i j\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "# Congratulations! ðŸŽ‰ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
