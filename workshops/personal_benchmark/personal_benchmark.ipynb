{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/personal_benchmark/personal_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "# Personal Benchmark with Google Sheets\n",
                "\n",
                "This notebook allows you to evaluate an LLM against custom benchmark data from a Google Sheet. \n",
                "\n",
                "The goal of this workshop is not so much on the technical implementation of evals, but on figuring out good things to evaluate!\n",
                "\n",
                "\n",
                "## In this notebook, we will\n",
                "\n",
                "1. **Fetch data from a public Google Sheet**\n",
                "    - Duplicate https://docs.google.com/spreadsheets/d/1qcqFI_5D6awdD6gSj7E33PWwfOs9hIC6JlwYKggx-_c/edit?usp=sharing \n",
                "    - Then add your own prompts, expected answers, and judging tips, when useful\n",
                "2. Evaluate a specified model using LiteLLM\n",
                "    - LiteLLM is a nice library that can call LLMs from any provider with the same interface. I.e. it takes care of the slight differences in the API of different providers (OpenAI, Gemini, Anthropic, etc.)\n",
                "    - The only thing that you need to use it, is an API key for the provider you want to use, and the name of the model.\n",
                "3. Then we run all your prompts through the model, and save the responses.\n",
                "4. Use a judge model to rate the responses (0-3) compared to expected answers\n",
                "5. Finally we print the results in a csv so that you can copy and paste them in the google sheet easily and analyse them there.\n",
                "\n",
                "\n",
                "I encorage you to look quickly at the code, especially if there are some parts that you are curious about! The code is pretty good.\n",
                "You can modify it, especially the judge model and its prompt, but there are no exercise like the other workshops.\n",
                "\n",
                "## Some inspiration for things you might want to evaluate:\n",
                "- Questions that you have asked LLMs in the past and they failed to answer correctly\n",
                "- Your final exams\n",
                "- Capabilities that you think are missing from current models (e.g. refer to the \"How to build AGI lecture\")\n",
                "- Revealed preferences on some topics (e.g. philosophy, politics, etc.)\n",
                "- Jailbreaks\n",
                "\n",
                "In general, try to aim it at things that are useful for YOU. So that when a new model comes, you have a better understanding of how it performs on things that matter to you (e.g. for you productivity or your understanding of the risks, ...).\n",
                "\n",
                "\n",
                "## To go further & Inspiration\n",
                "\n",
                "- Gread read on the topic: [Why you should maintain a personal LLM coding benchmark](https://blog.ezyang.com/2025/04/why-you-should-maintain-a-personal-llm-coding-benchmark/)\n",
                "- More powerful implementation of personal benchmark: https://github.com/carlini/yet-another-applied-llm-benchmark/\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages if not already installed\n",
                "!pip install -q litellm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "import csv\n",
                "import sys\n",
                "import os\n",
                "\n",
                "import pandas as pd\n",
                "import litellm\n",
                "from pydantic import BaseModel\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables (for API keys in development)\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Fetch Data from Google Sheets\n",
                "\n",
                "We'll extract data from a publicly accessible Google Sheet. The sheet must have columns for prompts, expected answers, and judging tips."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fetch_data_from_google_sheet(url: str) -> pd.DataFrame:\n",
                "    \"\"\"Fetch data from a publicly accessible Google Sheet.\n",
                "\n",
                "    URL format: https://docs.google.com/spreadsheets/d/SHEET_ID/\n",
                "    \"\"\"\n",
                "    if match := re.search(r\"/d/([^/]+)/\", url):\n",
                "        sheet_id = match.group(1)\n",
                "    else:\n",
                "        raise ValueError(\n",
                "            \"Invalid Google Sheets URL. Expected format: https://docs.google.com/spreadsheets/d/SHEET_ID/\"\n",
                "        )\n",
                "\n",
                "    url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv\"\n",
                "    df = pd.read_csv(url)\n",
                "    return df\n",
                "\n",
                "\n",
                "def validate_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Check if the dataframe has the required column.\"\"\"\n",
                "\n",
                "    required_columns = [\"prompt\", \"expected answer\", \"tips for judge\"]\n",
                "\n",
                "    # Check if all required columns are present\n",
                "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
                "\n",
                "    if missing_columns:\n",
                "        raise ValueError(\n",
                "            f\"The Google Sheet is missing required columns: {', '.join(missing_columns)}. \"\n",
                "            f\"Required columns are: {', '.join(required_columns)}.\"\n",
                "        )\n",
                "\n",
                "    # Remove rows with empty prompts\n",
                "    df = df.dropna(subset=[\"prompt\"])\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Update the url below with the url of your google sheet!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let user input the Google Sheets URL\n",
                "sheets_url = \"https://docs.google.com/spreadsheets/d/1qcqFI_5D6awdD6gSj7E33PWwfOs9hIC6JlwYKggx-_c/edit?gid=0#gid=0\"\n",
                "\n",
                "# Fetch data from the Google Sheet\n",
                "raw_data = fetch_data_from_google_sheet(sheets_url)\n",
                "print(f\"Successfully fetched data. Found {len(raw_data)} rows.\")\n",
                "\n",
                "# Validate and process the data\n",
                "benchmark_data = validate_columns(raw_data)\n",
                "print(f\"Data validated. Final dataset contains {len(benchmark_data)} prompts.\\n\")\n",
                "\n",
                "# Display the first few rows\n",
                "benchmark_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Evaluation\n",
                "\n",
                "Now we'll evaluate a model against our benchmark data using LiteLLM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class JudgeOutput(BaseModel):\n",
                "    reasoning: str\n",
                "    rating: int\n",
                "\n",
                "\n",
                "def generate_responses(model: str, df: pd.DataFrame) -> str:\n",
                "    \"\"\"Generate responses from the specified model using LiteLLM and add them to the dataframe in a column called 'response {model}'.\"\"\"\n",
                "\n",
                "    responses = litellm.batch_completion(\n",
                "        model=model,\n",
                "        messages=[[dict(role=\"user\", content=row[\"prompt\"])] for _, row in df.iterrows()],\n",
                "    )\n",
                "\n",
                "    # Add a column to the dataframe with the responses\n",
                "    column_name = f\"response {model}\"\n",
                "    df[column_name] = [response.choices[0].message.content for response in responses]\n",
                "\n",
                "    return column_name\n",
                "\n",
                "\n",
                "def evaluate_with_judge(df: pd.DataFrame, response_column: str, judge_model: str, model: str):\n",
                "    \"\"\"Use a judge model to rate the response on a scale of 0-5.\n",
                "\n",
                "    Adds two columns to the dataframe: 'judgment {model}' and 'rating {model}'.\n",
                "    \"\"\"\n",
                "\n",
                "    judge_prompt = \"\"\"\n",
                "You are evaluating the quality of a response to a specific question or task.\n",
                "\n",
                "Question:\n",
                "<question>\n",
                "{prompt}\n",
                "</question>\n",
                "\n",
                "Response to evaluate:\n",
                "<response>\n",
                "{response}\n",
                "</response>\n",
                "\n",
                "Correct answer:\n",
                "<correct_answer>\n",
                "{expected}\n",
                "</correct_answer>\n",
                "\n",
                "Rate the response on a scale from 0 to 3 where:\n",
                "0: Completely incorrect or irrelevant\n",
                "1: Some relevant information, but not complete or incorrect\n",
                "2: Mostly correct or useful with minor errors or omissions\n",
                "3: The response is correct, complete, and useful\n",
                "\n",
                "Additional judging tips:\n",
                "{judge_tips}\n",
                "\n",
                "Think step by step and provide your reasoning and rating in the following JSON format:\n",
                "{{\n",
                "    \"reasoning\": \"...\",\n",
                "    \"rating\": 0\n",
                "}}\n",
                "\"\"\"\n",
                "\n",
                "    messages = [\n",
                "        [\n",
                "            dict(\n",
                "                role=\"user\",\n",
                "                content=judge_prompt.format(\n",
                "                    prompt=row[\"prompt\"],\n",
                "                    expected=row[\"expected answer\"],\n",
                "                    response=row[response_column],\n",
                "                    judge_tips=row[\"tips for judge\"] or \"No additional tips\",\n",
                "                ),\n",
                "            )\n",
                "        ]\n",
                "        for _, row in df.iterrows()\n",
                "    ]\n",
                "\n",
                "    judge_response = litellm.batch_completion(\n",
                "        model=judge_model,\n",
                "        messages=messages,\n",
                "        response_format=JudgeOutput,\n",
                "    )\n",
                "\n",
                "    judgments = []\n",
                "    for response in judge_response:\n",
                "        if isinstance(response, Exception):\n",
                "            raise response\n",
                "        else:\n",
                "            content = response.choices[0].message.content\n",
                "            try:\n",
                "                judgments.append(JudgeOutput.model_validate_json(content))\n",
                "            except Exception as e:\n",
                "                print(f\"Error validating JSON: {e}\")\n",
                "                print(f\"Response: {response}\")\n",
                "                judgments.append(JudgeOutput(reasoning=content, rating=0))\n",
                "\n",
                "    df[f\"judgment {model}\"] = [judgment.rating for judgment in judgments]\n",
                "    df[f\"rating {model}\"] = [judgment.reasoning for judgment in judgments]\n",
                "\n",
                "\n",
                "def evaluate_model(df: pd.DataFrame, model_name: str, judge_model: str) -> pd.DataFrame:\n",
                "    \"\"\"Evaluate the model on all prompts in the benchmark dataset.\"\"\"\n",
                "\n",
                "    response_column = generate_responses(model_name, df)\n",
                "    print(f\"Generated responses for {model_name}. Moving to judge...\")\n",
                "    evaluate_with_judge(df, response_column, judge_model, model_name)\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Add your API keys for the models you want to evaluate!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
                "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n",
                "# os.environ[\"GEMINI_API_KEY\"] = \"AIza-...\"\n",
                "# os.environ[\"GROQ_API_KEY\"] = \"gsk_...\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let user specify the model to evaluate and the judge model\n",
                "model_to_evaluate = \"openai/gpt-4o-mini\"\n",
                "judge_model = \"openai/gpt-4o-mini\"\n",
                "\n",
                "# Run the evaluation\n",
                "evaluate_model(benchmark_data, model_to_evaluate, judge_model)\n",
                "print(\"\\nEvaluation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now print the results in a csv so that you can copy and paste them in the google sheet easily.\n",
                "This is the simplest way to read them kind of nicely! \n",
                "\n",
                "Note: If you have trouble pasting it in a table form, the steps might be \"Paste\", click the clipboard icon, and then \"Split into columns\" and choose the delimiter as \";\" (or any other delimiter you used, if you changed it).\n",
                "\n",
                "After this you can:\n",
                "- Analyse the data, compute the average score, make plots etc. in google sheets (or in this notebook, up to you!)\n",
                "- Keep track of how models do, and compare multiple models and their responses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "columns = [\n",
                "    f\"response {model_to_evaluate}\",\n",
                "    f\"judgment {model_to_evaluate}\",\n",
                "    f\"rating {model_to_evaluate}\",\n",
                "]\n",
                "\n",
                "printer = csv.DictWriter(sys.stdout, columns, delimiter=\";\")\n",
                "printer.writeheader()\n",
                "for _, row in benchmark_data.iterrows():\n",
                "    printer.writerow(dict(zip(columns, [row[col] for col in columns])))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If everything worked, congratulations! You have now a benchmark for any model for which you have API keys.\n",
                "\n",
                "Some directions for you are now:\n",
                "- Do something else, there's lots of other things to do in life\n",
                "- Find better questions for your benchmark\n",
                "- Evaluate an other model\n",
                "- Think of the minimal changes you could do to be able to test code that models generate. You would need to run it."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
