{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQB8lmYv4UPO"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/workshops/rlhf/rlhf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "This workshop was made by Callum McDougal for ARENA, and serves two purposes:\n",
        "- Play around RLHF\n",
        "- Familiarize with new libraries that might be useful for some of your future projects (HuggingFace, Weights&Biases, TRLX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqKzbxyEyLHT"
      },
      "source": [
        "# Reinforcement Learning from Human Feedback\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "\n",
        "### Context - Pretraining is not enough\n",
        "\n",
        "You've seen earlier in the course that we can train very large and performant models like GPT2 using next-token prediction. Such models, prior to any fine-tuning, must be steered carefully with prompts in order to generate useful output. Most language models used in services of any kind today are not only pre-trained models. Rather, we use many training techniques to make them more useful.\n",
        "\n",
        "RLHF is one of many techniques which can convert a pre-trained model, into a more useful model for practical application.\n",
        "\n",
        "### Context - RLHF as a naive alignment strategy\n",
        "\n",
        "The field AI alignment is concerned with aligning AI systems with our desired outcomes. There are many reasons to think that intelligent systems do not, by default, share human values or that whilst training against any objective will lead to reliable, expected outcomes being produced by AI systems. Nevertheless, training AI systems to produce outcomes that humans prefer over outcomes which they don't seems to be a concrete step towards AI alignment, which we can build on later.\n",
        "\n",
        "Thus we get the core idea of RLHF as an alignment strategy. We care about outcomes, so we provide the AI feedback based on what we think likely outcomes of it's actions are and update it to produce good outcomes according to our preferences.\n",
        "\n",
        "For more detail on RLHF, see Paul Christiano's blog post [here](https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research#The_case_for_a_positive_impact).\n",
        "\n",
        "\n",
        "### What is RLHF?\n",
        "\n",
        "Reinforcement Learning with Human Feedback (RLHF) is a RL technique where the rewards issued by a reward model, which is itself trained from labelled data from a human operator. Often, it can be hard to specify the reward function $R : S \\times A \\to \\mathbb{R}$ that the environment uses to issue reward to the agent, so we ask a human instead to reward/punish the agent based on the action it took. [OpenAI](https://openai.com/research/learning-from-human-preferences) uses RLHF to adjust the behaviour of models to desirable behaviour, but this can also incentivise the agent to hack the reward signal (by taking actions that look good to the human, or influencing the human to always give good rewards.)\n",
        "\n",
        "One should note that in the framework of RLHF, the environment only has one state, and the model that we are trying to fine-tune with RLHF no longer needs to \"plan ahead\", so in this sense it is closer to a bandit problem than the MDPs we saw in previous days.\n",
        "\n",
        "### Why does it matter?\n",
        "\n",
        "RLHF (at the moment) is a successful method of nudging large language models towards desired behaviour when that behaviour is difficult to write as an algorithm.\n",
        "\n",
        "For chess, it's easy to evaluate whether an agent won/lost the game, so we can reward that directly. For text generation, it can be hard to formally specify\n",
        "what we mean by harmful or abusive text. One could have simple proxies like a filter to encourage/discourge use of particular words, and use that\n",
        "to train against, but it's very easy to construct harmful text such that no particular word in the sentence would be classed as offensive:\n",
        "\"I would love to eat your pet puppy\" contains no offensive words, even though the semantic meaning of the entire sentence is quite offensive.\n",
        "A simple proxy for offensiveness might even rate this as a positive statement, as it contains \"nice\" words like *love* and *puppy*.\n",
        "\n",
        "However, samples from humans are expensive and slow. Even running a single batch of examples through the model could take a long time\n",
        "if we need a human to give a scalar reward for each action chosen by the model. So, the solution is to collect a lot of data from a human\n",
        "(a set of (observation, action, reward) tuples), train a reward model on this data, and then use the reward model as the reward function.\n",
        "\n",
        "\n",
        "### How does RLHF work in practice?\n",
        "\n",
        "RLHF involves 3 stages:\n",
        "\n",
        "1. We pretrain a language model (LM) using existing supervised learning techniques.\n",
        "2. We gather labelled data from humans, and train a reward model that will act as a proxy for the human's rewards.\n",
        "3. We fine-tuning the LM with reinforcement learning.\n",
        "\n",
        "#### 1. Pretraining\n",
        "\n",
        "Since reinforcement learning is very sample inefficient, it is unreasonable to expect to be able to train a language model from scratch using online learning. Rather, we must start with an existing pre-trained model and then fine-tune it.\n",
        "\n",
        "We will be using GPT-2-small as our base model to finetune.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jbloomAus/ARENA_2.0-RLHF/main/media/pretraining.png\" width=\"500\">\n",
        "\n",
        "#### 2. The Reward Model\n",
        "\n",
        "The reward model is used to assign a reward to any given output of the model during training.\n",
        "Rather than have reward be a simple function of the state of the world (as for RL environments like CartPole),\n",
        "the reward model assigns a reward to a given piece of text.\n",
        "The reward model acts like a text classifier, rewarding \"good\" pieces of text, and punishing \"bad\" text.\n",
        "\n",
        "The reward model is trained on a set of prompts, hand labelled by humans into \"good\" and \"bad\".\n",
        "This is then used to train the reward model, to act as a stand-in for the human during the fine-tuning stage.\n",
        "\n",
        "The model acts as a mapping between arbitrary text and human preferences.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jbloomAus/ARENA_2.0-RLHF/main/media/reward-model.png\" width=\"700\">\n",
        "\n",
        "#### 3. Fine-Tuning with Reinforcement Learning\n",
        "\n",
        "Finally, given some reward model and some pre-trained model, we can use an algorithm such as PPO to reward the model for producing prompt completions when the reward model predicts the completion to be preferable.\n",
        "\n",
        "In the standard RL framework, the agent recieves a reward on every timestep during interaction.\n",
        "Here, the \"observation\" that the agent receives is a textual prompt, and the \"action\" the agent takes is the choice of words\n",
        "to complete the prompt. The reward model then assigns a reward based on the prompt together with the completion from the agent,\n",
        "which is then used to compute the loss, and update the weights of the model.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jbloomAus/ARENA_2.0-RLHF/main/media/rlhf.png\" width=\"800\">\n",
        "\n",
        "### How does RLHF differ from PPO?\n",
        "\n",
        "- No \"environment\". RLHF operates on text completions made by the pre-trained generative model.\n",
        "- Reward Model. Reward itself is generated by the reward model which itself must be trained.\n",
        "- Adding a Value Head. We add a value head to the policy/LM architecture so that we have both an actor and a critic for PPO.\n",
        "- KL Divergence penalty. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, to ensure we maintain coherent outputs, and the fine-tuned model avoids generating text that overfits to what the reward model is looking for.\n",
        "\n",
        "#### Aside - value heads\n",
        "\n",
        "The \"actor\" in our PPO setup is the GPT model. We get the \"critic\" by adding a **value head** to the GPT architecture - i.e. you stick a classifier to GPT2 and train that as our value function.\n",
        "\n",
        "For an example, see the source code for AutoModelForCausalLMWithValueHead in the [TRLX github](https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py). This gives us an autoregressive transformer which has 2 outputs: one corresponding to the standard next token prediction objective, and one which sticks a classifier on the end to get a value function. It does this by adding `self.v_head`, a function which reads from the final value of the residual stream in GPT (which stores some compressed embedding of the prompt), and extracts a value function from this representation. You can think of this as a kind of feature extraction, analogous to the feature extraction that we implemented with our ResNet models in the first week.\n",
        "\n",
        "The TRLX library we'll be working with today handles all of this under the hood. However, you should definitely have a poke around this library to get a feel for how it works.\n",
        "\n",
        "#### Aside - KL divergence term\n",
        "\n",
        "**Note** - the KL div penalty is not the same as the version of PPO which uses a KL div penalty term in the surrogate objective function. The first one is a feature of the RLHF setup; it makes sure we don't get too far from the original model (i.e. it's static throughout training, used to constrain how much we change from the original model by the end). The second one is a feature of PPO setup; it makes sure we don't make huge updates from where we were before the last training step (i.e. it's a moving target, used to constrain how much we change each step).\n",
        "\n",
        "The KL div term we use in RL heavily penalises our new model when it outputs something which **would have low probability in the original model.** This is related to the reason RLHF'ed models are sometimes described as [\"lobotomized\"](https://twitter.com/repligate/status/1640488734192726018) - they converge to a subset of the kinds of outputs that our original model might have had, meaning they lose some of the variance and creativity of the original model.\n",
        "\n",
        "## Optionnal readings for RLHF\n",
        "\n",
        "* [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593) (paper)\n",
        "* [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325) (paper)\n",
        "* [AI safety via debate](https://openai.com/research/debate) (OpenAI blog post)\n",
        "* [Thoughts on the impact of RLHF research](https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research), by Paul Christiano\n",
        "\n",
        "## Content & Learning Objectives\n",
        "\n",
        "\n",
        "#### 1ï¸âƒ£ Prompt Dataset & Reward Model\n",
        "\n",
        "In the first section, we'll get set up with the prompt dataset and reward model we'll be using for the rest of the exercises.\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> * Learn about the BERT transformer model and how it can be used for sentiment analysis\n",
        "> * Load datasets from Huggingface and break them up into prompts\n",
        "> * Generate text from Huggingface models\n",
        "> * Output positive sentiments from models in vanilla PyTorch and Huggingface pipelines\n",
        "\n",
        "#### 2ï¸âƒ£ Using RLHF for Finetuning\n",
        "\n",
        "In the second section, we'll finetune a model pre-trained on the IMDB dataset using RLHF to generate positive reviews.\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> - Learn about TRLX and how it can be used\n",
        "> - Using RLHF to improve sentiment of GPT2 produced Movie Reviews\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozxGcEJb36-s"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4sVBo7ayLHX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import math\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "from transformers.models.bert.modeling_bert import BertForMaskedLM\n",
        "import logging\n",
        "from typing import cast, Any, List, Optional, Union, Tuple\n",
        "\n",
        "import trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giwG9EMKyLHY"
      },
      "source": [
        "# 1ï¸âƒ£ Prompt Dataset & Reward Model\n",
        "\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> * Learn about the BERT transformer model and how it can be used for sentiment analysis\n",
        "> * Load datasets from Huggingface and break them up into prompts\n",
        "> * Generate text from Huggingface models\n",
        "> * Output positive sentiments from models in vanilla PyTorch and Huggingface pipelines\n",
        "\n",
        "## Background - BERT\n",
        "\n",
        "In the transformers chapter, we only worked with autoregressive transformers like GPT2. Here, we'll work with BERT, a well-known **bidirectional transformer**.\n",
        "\n",
        "BERT predates GPT2 slightly (it was released in 2018, one year after the seminal \"Attention is all you need\" paper). It was the next in a proud tradition of naming transformers after muppets (no, [that's](https://arxiv.org/pdf/1910.13034.pdf) [not](https://arxiv.org/pdf/1904.09223.pdf) [a](https://arxiv.org/pdf/1905.12616.pdf) [joke](https://arxiv.org/pdf/1906.01604.pdf)). It has bidirectional attention, meaning we don't apply masking to the attention patterns - information can flow backwards and forwards in the model. BERT is usually used for classification tasks, such as sentiment analysis.\n",
        "\n",
        "### How is BERT trained?\n",
        "\n",
        "The architecture is similar to GPT, although the \"core BERT\" model doesn't have an unembedding (i.e. the output has shape `(batch, seq_len, d_model)`).\n",
        "\n",
        "BERT is trained on two kinds of tasks: **next sentence prediction** (NSP) and **masked language modelling** (MLM).\n",
        "\n",
        "* In MLM, we take a sequence and replace some of its tokens with a special `[MASK]` token, then train the model to predict the original token.\n",
        "* In NSP, we take two sentences, and train the model to predict whether the second sentence follows the first (we do this by adding a small classifier at the end of BERT, which just reads from the final value of the residual stream at the zeroth sequence position, which is a special classification token `[CLS]`).\n",
        "\n",
        "Importantly, **both of these two tasks require the model to learn some kind of compressed representation of the input sequence** in its residual stream.\n",
        "\n",
        "### How do we turn BERT into a classifier?\n",
        "\n",
        "We usually stick a classification head onto the end of the \"core BERT architecture\" at the `[CLS]` token, then take the pretrained model and fine-tune it on a classification task. If pretraining has been successful, the model will have learned some kind of compressed representation of the input sequence in its residual stream, and the classifier will be doing something like feature extraction.\n",
        "\n",
        "In the RLHF exercises you'll be taking advantage of BERT's ability to be used as a classifier, but for now we'll have a look at how BERT does at masked language modelling.\n",
        "\n",
        "### Exercise - load BERT, and play around with it\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ âšªâšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "We're going to use a HuggingFace tokenizer for now to encode text into a sequence of tokens that our model can use. The tokenizer has to match the model - our model was trained with the `bert-base-cased` tokenizer which is case-sensitive. If you tried to use the `bert-base-uncased` tokenizer which is case-insensitive, it wouldn't work at all.\n",
        "\n",
        "Check out `tokenizer.vocab` to get an idea of what sorts of strings are assigned to tokens. In WordPiece, tokens represent a whole word unless they start with `##`, which denotes this token is part of a word.\n",
        "\n",
        "You can also check out `tokenizer.special_tokens_map`. The strings here are mapped to tokens which have special meanings - for example `tokenizer.mask_token`, which is the literal string '[MASK]', is converted to `tokenizer.mask_token_id`, equal to 103.\n",
        "\n",
        "**Play around with this model**, until you get a sense of how it works. What kind of interesting completions can you find? Can BERT solve the IOI task? Can it do basic arithmetic?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZBZOHzQA6i6"
      },
      "outputs": [],
      "source": [
        "bert = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KuzjlfByLHa"
      },
      "outputs": [],
      "source": [
        "def predict(model: BertForMaskedLM, tokenizer: AutoTokenizer, text: str, k=15) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Return a list of k strings for each [MASK] in the input.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure we're in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenizer returns a bunch of special BERT-specific things, we just want input ids\n",
        "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "    # Get top predictions at all the places we masked\n",
        "    out = model(input_ids).logits\n",
        "    preds = out[input_ids == tokenizer.mask_token_id]\n",
        "    tops = preds.topk(k, dim=-1).indices\n",
        "\n",
        "    return [[tokenizer.decode(t) for t in mask] for mask in tops]\n",
        "\n",
        "\n",
        "your_text = \"The Answer to the Ultimate Question of Life, The Universe, and Everything is [MASK].\"\n",
        "predictions = predict(bert, bert_tokenizer, your_text)\n",
        "print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3p5RYHkyLHb"
      },
      "source": [
        "## IMDB dataset\n",
        "\n",
        "\n",
        "First, load in the IMDB user reviews dataset. Documentation about the IMDB dataset can be found here: https://huggingface.co/datasets/imdb. We want to use both the train and test splits to collect prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7kWU2zWyLHb"
      },
      "outputs": [],
      "source": [
        "imdb = load_dataset(\"imdb\", split=\"train+test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFsJ7cnRyLHc"
      },
      "source": [
        "### Exercise - Figure out the positive-negative review split in the dataset\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ âšªâšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ âšªâšªâšª\n",
        "\n",
        "You should spend up to 5 minutes on this exercise.\n",
        "```\n",
        "\n",
        "The positive-negative review split will tell us the distribution of sentiments our model will output out of the box. Write a function to print out the number of samples for each label.\n",
        "\n",
        "You should review the [documentation page](https://huggingface.co/datasets/imdb) to know more what the dataset looks like.\n",
        "\n",
        "<details>\n",
        "<summary><b>Hint</b> (click to expand)</summary>\n",
        "- You can access specific columns of the dataset using the `dataset['column_name']` syntax.\n",
        "- You can use the `.count(value)` method on a column to count the number of occurences of a value in that column.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSTbZMgryLHc"
      },
      "outputs": [],
      "source": [
        "def label_split(dataset) -> Tuple[int, int]:\n",
        "    ...\n",
        "\n",
        "nb_positive, nb_negative = label_split(imdb)\n",
        "\n",
        "print(f\"Number of positive reviews: {nb_positive}\")\n",
        "print(f\"Number of negative reviews: {nb_negative}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxPhX2ZLyLHd"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "def label_split(dataset) -> Tuple[int, int]:\n",
        "    positive_samples = dataset['label'].count(1)\n",
        "    negative_samples = dataset['label'].count(0)\n",
        "\n",
        "    return postive_samples, negative_samples\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "### Exercise - Create a set of prompts\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to ~10 minutes on this exercise.\n",
        "```\n",
        "\n",
        "A prompt to the model can look like \"Today was not fun \", \"In the event of \" or \"Mary gave John a \". These prompts will serve as the starting point for model generations during the RLHF process.\n",
        "\n",
        "In the context of the exercise to push GPT2 towards outputting reviews with more positive sentiment, we want to try and have a set of prompts that can produce varying kinds of sentiments rather than just one kind of sentiment. This set of prompts essentially forms our \"observation space\" and all completions are \"actions\", if our observation space contains primarily positive sentiment the model will not update heavily and will potentially still output negative sentiment when a prompt heavily favors it. Ideally we want our set of prompts to have a mix of sentiments.\n",
        "\n",
        "We want to collect the first few (3-5, the choice is yours) words from each review to serve as prompts for our finetuned model. The generated text from these prompts will be later used to evaluate the performance of our finetuned model.\n",
        "\n",
        "Emphasis - **we want to capture these prompts straight from the imdb dataset rather than write them ourselves.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPxwWUthyLHe"
      },
      "outputs": [],
      "source": [
        "def generate_prompts(dataset) -> List[str]:\n",
        "    \"\"\"Generate & return prompts from dataset.\"\"\"\n",
        "    ...\n",
        "\n",
        "\n",
        "prompts = generate_prompts(imdb)\n",
        "prompts[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b52BQmqDyLHf"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "def generate_prompts(dataset):\n",
        "    \"\"\"Generate & return prompts from dataset.\"\"\"\n",
        "    prompts = [\" \".join(review.split()[:4]) for review in dataset[\"text\"]]\n",
        "    return prompts\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "## GPT2-IMDB\n",
        "\n",
        "The model that we will perform RLHF on is a GPT-2 model fine-tuned on the IMDB dataset, which can be found here: https://huggingface.co/lvwerra/gpt2-imdb. Since this model is finetuned on the IMDB dataset, the distribution of sentiments of its generations will be close to the distribution of sentiments of the original dataset. This means that after fine-tuning, the responses that are categorized as \"nice\" tend to lean towards being positive movie reviews rather than just generically positive continuations.\n",
        "\n",
        "\n",
        "### Exercise - Load the GPT-2 model and generate reviews from prompts\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to 10-25 minutes on this exercise.\n",
        "```\n",
        "\n",
        "You will need to use the `AutoTokenizer` and `AutoModelForCausalLM` from the transformers package. You might want to use the `generate` method of the GPT-2 model that you load, if you do you should set the `max_new_tokens` argument to something that's large enough.\n",
        "\n",
        "Play around with generating completions from this prompt and verify whether the completions approximately fit your initial expectations of the sentiments that the model would output.\n",
        "\n",
        "**Note** - when you run `tokenizer(prompt)`, this will return a dictionary containing things like `token_ids` as well as a couple of other things that need to be passed into the model in a forward pass (e.g. a tensor indicating where you should mask `[PAD]` tokens). The best way to deal with this is to take `inputs = tokenizer(prompt)` and run `model.generate(**inputs)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_1hQ6mHyLHf"
      },
      "outputs": [],
      "source": [
        "def generate_completion(prompt: str, model, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Generates completions for the given prompt (in the form of a string).\n",
        "\n",
        "    Remember to set the `do_sample=True` flag when you call `model.generate`.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    out = model.generate(**inputs, do_sample=True)\n",
        "    return tokenizer.decode(out.squeeze())\n",
        "\n",
        "\n",
        "# Load the tokenizer and model.\n",
        "# You can find the name of the model and tokenizer at the documentation page: https://huggingface.co/lvwerra/gpt2-imdb.\n",
        "gpt2_tokenizer = ...\n",
        "gpt2 = ...\n",
        "\n",
        "generate_completion(prompts[0], gpt2, gpt2_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYzH6eXdyLHg"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def generate_completion(prompt: str, model, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Generates completions for the given prompt (in the form of a string).\n",
        "\n",
        "    Remember to set the `do_sample=True` flag when you call `model.generate`.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    out_tokens = model.generate(**inputs, do_sample=True, max_new_tokens=64)\n",
        "    return tokenizer.decode(out.squeeze())\n",
        "\n",
        "\n",
        "# Load the tokenizer and model.\n",
        "# You can find the name of the model and tokenizer at the documentation page: https://huggingface.co/lvwerra/gpt2-imdb.\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "### The reward function\n",
        "\n",
        "Judging by the name of this chapter you might think that you would be providing the reward function yourself but sadly we will not be doing this. Instead, we will be using a language model trained to perform sentiment analysis to generate the sentiment score (higher is positive). The language model we will be using to generate sentiment scores can be found here: https://huggingface.co/lvwerra/distilbert-imdb.\n",
        "\n",
        "\n",
        "#### Exercise - Get sentiment scores for a review\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "\n",
        "You should spend up to 15-30 minutes on this exercise.\n",
        "```\n",
        "\n",
        "We can use the model mentioned above in eval mode to generate sentiment scores and then transform the sentiments into rewards to be fed into the RLHF training loop.\n",
        "\n",
        "Note: Here you should use `AutoModelForSequenceClassification` instead of `AutoModelForCausalLM` since we are doing classification (what's the sentiment?) rather than generation. Do not hesitate to print the objects, shapes and types of the variable you're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlNnqQvxyLHg"
      },
      "outputs": [],
      "source": [
        "bert_imdb = ...\n",
        "bert_imdb_tokenizer = ...\n",
        "\n",
        "@torch.inference_mode()  # Tell PyTorch to not build a computation graph and a few other things, for speed\n",
        "def reward_model(samples: List[str], model=bert_imdb, tokenizer=bert_imdb_tokenizer, **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    Returns the rewards for the given samples.\n",
        "\n",
        "    kwargs are passed to your model during a forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_cBO5zdyLHg"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "bert_imdb = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
        "bert_imdb_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
        "\n",
        "@torch.inference_mode()  # Tell PyTorch to not build a computation graph and a few other things, for speed\n",
        "def reward_model(samples: List[str], model=bert_imdb, tokenizer=bert_imdb_tokenizer, **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    Returns the rewards for the given samples.\n",
        "\n",
        "    kwargs are passed to your model during a forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(samples, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model(**inputs, **kwargs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "    # 1 is the index of the positive class\n",
        "    return probabilities[:, 1].tolist()\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "Test your reward model on some example prompts. Do the numbers make sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vby7YV7eyLHg"
      },
      "outputs": [],
      "source": [
        "example_prompts = [\"Example string\", \"I'm having a good day\", \"You are an ugly person\"]\n",
        "rewards = reward_model(example_prompts)\n",
        "\n",
        "for prompt, reward in zip(example_prompts, rewards):\n",
        "    print(f\"{prompt}: {reward}\")\n",
        "\n",
        "# Should be around 0.54, 0.97 and 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYO_hcmEyLHh"
      },
      "source": [
        "### Exercise - Output sentiment scores using Huggingface pipelines\n",
        "\n",
        "This is an alternate way to get a reward model working directly using Huggingface pipelines. This will enable you to use a diverse range of models quite easily by changing a couple of arguments and provide you with more functionality than the vanilla PyTorch loop you implemented above. Reading the relevant documentation is the key to success here.\n",
        "\n",
        "**Part A: Create a huggingface pipeline to output sentiment scores for a generated review**\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to 10-25 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Pipelines are a high-level way to use huggingface models for inference. Since the model that acts as our reward function will be used strictly for inference, it makes sense to wrap it in a pipeline.\n",
        "\n",
        "The huggingface Pipeline documentation can be found here: [https://huggingface.co/docs/transformers/main_classes/pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
        "\n",
        "You will need to set the `top_k` argument to the number of labels we expect the pipeline to return, in our case this would be 2 (Positive and Negative).\n",
        "\n",
        "We would ideally also want to use the truncation flag and the batch_size argument to enable faster generation. For this exercise, these two things are not essential but could be experimented with as we will need these for later exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMRKmoMeyLHh"
      },
      "outputs": [],
      "source": [
        "def create_pipeline(model_path):\n",
        "    # Ensure we use a GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        device = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "    else:\n",
        "        device = -1\n",
        "\n",
        "    return pipeline(\n",
        "        \"text-classification\",\n",
        "        model_path,\n",
        "        top_k=2,\n",
        "        truncation=True,\n",
        "        batch_size=256,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "\n",
        "sentiment_fn = create_pipeline(\"lvwerra/distilbert-imdb\")\n",
        "\n",
        "sentiment_fn(\"What does the pipeline returns?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj-VenQ4yLHh"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "def create_pipeline(model_path):\n",
        "    # Ensure we use a GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        device = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "    else:\n",
        "        device = -1\n",
        "\n",
        "    return pipeline(\n",
        "        \"text-classification\",\n",
        "        model_path,\n",
        "        top_k=2,\n",
        "        truncation=True,\n",
        "        batch_size=256,\n",
        "        device=device,\n",
        "    )\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "**Part B: Map the sentiment pipeline to a reward function**\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to 10-20 minutes on this exercise.\n",
        "```\n",
        "\n",
        "We want the reward function to return a single number corresponding to the value of the positive label (the label we care about initially) for that generation rather than a dictionary containing the labels and their respective values (this is what the pipeline outputs, print it!).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FczChu1vyLHh"
      },
      "outputs": [],
      "source": [
        "def reward_model(samples: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    Returns a list of reward values corresponding to the samples in `samples`.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "\n",
        "example_prompts = [\"Example string\", \"I'm having a good day\", \"You are an ugly person\"]\n",
        "rewards = reward_model(example_prompts)\n",
        "\n",
        "for prompt, reward in zip(example_prompts, rewards):\n",
        "    print(f\"{prompt}: {reward}\")\n",
        "\n",
        "# Should still be around 0.54, 0.97 and 0.046"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860azpo5yLHh"
      },
      "source": [
        "\n",
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "def reward_model(samples: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    Returns a list of reward values corresponding to the samples in `samples`.\n",
        "    \"\"\"\n",
        "\n",
        "    # This is one way of doing it, but there are many others.\n",
        "    return [\n",
        "        result['score']\n",
        "        for results in sentiment_fn(samples)\n",
        "        for result in results\n",
        "        if result['label'] == \"POSITIVE\"\n",
        "    ]\n",
        "```\n",
        "</details>\n",
        "\n",
        "\n",
        "### Exercise - Sentiment playground\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ âšªâšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "The reward model is now ready and you should take some time to feed in sentences of varying sentiments to check whether the rewards are as you expect. Remember the reward model is also a trained model so it exhibits all the quirks of one such as weird failure modes and potential to be broken with adversarial examples.\n",
        "\n",
        "What are the most counterintuitive results you can find? **It's vitally important for the overall experience of the exercises today that you post your findings in the Signal chat.**\n",
        "\n",
        "We will also be using this opportunity to test whether your reward model is set up correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3D4xdtJyLHi"
      },
      "outputs": [],
      "source": [
        "## Code below has an interesting set of examples:\n",
        "\n",
        "prompts = [\n",
        "    \"I want to eat\",\n",
        "    \"I want your puppy\",\n",
        "    \"I want to eat your puppy\",\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(prompt, reward_model(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNCRQDyFaC-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2ï¸âƒ£ Using RLHF for Finetuning\n",
        "\n",
        "> ##### Learning objectives\n",
        ">\n",
        "> - Learn about TRL and how it can be used\n",
        "> - Use RLHF to improve sentiment of GPT2-produced movie reviews\n",
        "\n",
        "\n",
        "## TRL\n",
        "\n",
        "\n",
        "### What is TRL?\n",
        "\n",
        "The trl library is a library built on top of [transformers](https://github.com/huggingface/transformers), used to fine-tune and align transformer language models using RLHF (through PPO) and [DPO](https://arxiv.org/abs/2305.18290). The trl library can also perform both supervised fine-tuning (SFT) and reward modeling.\n",
        "\n",
        "\n",
        "### Using TRL\n",
        "\n",
        "Using trl for RLHF, we need to choose:\n",
        "\n",
        "- A model to be trained\n",
        "- A RLHF trainer\n",
        "- A prompt dataset.\n",
        "- A reward function (which makes use of the reward model).\n",
        "- Evaluation Prompts"
      ],
      "metadata": {
        "id": "wT_oAqf7aDth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we make a function for taking minibatches of strings\n",
        "def make_minibatch(prompts, batch_size):\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        yield prompts[i:i + batch_size]\n",
        "\n",
        "# We use 64 prompts (generated by ChatGPT) to use as inputs in our RLHF training\n",
        "# The prompts are a mix of positive, neutral, and negative sounding prompts.\n",
        "\n",
        "prompts = [\"This movie was\", \"I was simply\", \"I want to eat\", \"The plot was\",\n",
        "           \"Characters felt very\", \"I couldn't stop\", \"Would definitely recommend\", \"The ending left\",\n",
        "           \"Acting was just\", \"Cinematography seemed\", \"Soundtrack is a\", \"Scriptwriting felt\",\n",
        "           \"Directing could have\", \"The pacing was\", \"Visual effects were\", \"Emotionally, it was\",\n",
        "           \"Storyline seemed\", \"Loved the way\", \"Hated how the\", \"The movie overall\",\n",
        "           \"One of the\", \"Could not believe\", \"Was pleasantly surprised\", \"Expected more from\",\n",
        "           \"Definitely not worth\", \"Will watch again\", \"Could have been\", \"It was quite\",\n",
        "           \"Felt like a\", \"Such a disappointment\", \"This film is\", \"Was really bored\",\n",
        "           \"Truly captivating and\", \"Thought it would\", \"An absolute masterpiece\", \"Completely fell flat\",\n",
        "           \"Kept me on\", \"The humor was\", \"Loved every minute\", \"Left me feeling\",\n",
        "           \"The action scenes\", \"A total waste\", \"A true work\", \"Could watch it\",\n",
        "           \"Not my favorite\", \"An enjoyable and\", \"Wasn't expecting it\", \"What a great\",\n",
        "           \"It failed to\", \"The romance subplot\", \"Impressed by the\", \"Could not relate\",\n",
        "           \"A must-see for\", \"Lost interest after\", \"The film's message\", \"A visual delight\",\n",
        "           \"Unconvincing character arcs\", \"Absolutely thrilling\", \"Underwhelming performance\", \"Beautifully crafted\",\n",
        "           \"I was amazed\", \"Really dragged on\", \"A solid piece\", \"Not engaging enough\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "O9RflsKvasE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are using again GPT-2-IMDB library, except that now wrapped in TRL\n",
        "\n",
        "batch_size = 4\n",
        "num_epochs = 100\n",
        "\n",
        "trl_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"lvwerra/gpt2-imdb\",\n",
        ")  # The model to be RLHFed\n",
        "trl_ref_model = trl.create_reference_model(trl_model)  # The model used as a base model in KL divergence\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('lvwerra/gpt2-imdb')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "ppo_config = trl.PPOConfig(batch_size=batch_size,\n",
        "                           mini_batch_size=batch_size)\n",
        "ppo_trainer = trl.PPOTrainer(ppo_config,\n",
        "                             trl_model,\n",
        "                             trl_ref_model,\n",
        "                             tokenizer)\n"
      ],
      "metadata": {
        "id": "TkHUjZtYbh5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Putting it all together - Reinforcing *positive* sentiment\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to 10-20 minutes on this exercise.\n",
        "```\n",
        "\n",
        "We will now be creating the training loop using TRL. The only thing you should do is to carefully understand the training loop, and what each of the moving pieces are doing. For that, we are not commenting the training loop, and you should make those comments."
      ],
      "metadata": {
        "id": "bgcX-hbVcAqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    for batch_prompts in make_minibatch(prompts, 4):\n",
        "        prompts_tensor = [tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\") for prompt in batch_prompts]\n",
        "        responses_tensor  = [trl.core.respond_to_batch(trl_model, prompt_tensor) for prompt_tensor in prompts_tensor]\n",
        "        prompts_tensor = [p[0] for p in prompts_tensor]\n",
        "        responses_tensor = [r[0] for r in responses_tensor]\n",
        "        responses_strings = [tokenizer.decode(r) for r in responses_tensor]\n",
        "        full_sentences = [b + r for b, r in zip(batch_prompts, responses_strings)]\n",
        "\n",
        "        rewards = reward_model(full_sentences)\n",
        "        rewards = [torch.tensor(r) for r in rewards]\n",
        "\n",
        "        train_stats = ppo_trainer.step(prompts_tensor, responses_tensor, rewards)\n",
        "    print(f'Epoch {epoch + 1} completed')"
      ],
      "metadata": {
        "id": "RVDPFzKobhtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Sentiment playground - Post RLHF\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ âšªâšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to ~10 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Try out your RLHF'd model!"
      ],
      "metadata": {
        "id": "S1ER7pGocjVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model = ppo_trainer.model.cpu()\n",
        "generate_completion(\"This movie\", eval_model, tokenizer)"
      ],
      "metadata": {
        "id": "qLz1eEF6caDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Exercise (Hard): Now write your own PPOTrainer\n",
        "\n",
        "```c\n",
        "Difficulty: Hero Mode\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to ~??? minutes on this exercise.\n",
        "```\n",
        "\n",
        "You've learned what RLHF returns, but now we are giving you a much harder challenge: implement the RLHF part. Namely, you are going to implement the class PPOTrainer just as in trl.PPOTrainer. Have fun!\n",
        "\n",
        "Tips:\n",
        "- Look at the [documentation for trl.PPOConfig](https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_config.py), to see which configurations you can take from it (such as the KL Divergence constant value, and so on). If you find it best, you can implement your own PPOConfig class.\n",
        "- Also look at the documentation for [trl.PPOTrainer](https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py), to get some inspiration. It is a really useful skill to read documentation and source code in general (and this is not a trivial task).\n",
        "- Feel free to import any additional libraries you would like, look at the transformers library to understand its internals, and what not. This is an advanced coding exercise, and do not feel shy to use any tools at your hand (except for trl.PPOTrainer itself, of course).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P-i5L3bUclmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOConfig(object):\n",
        "    def __init__(self,\n",
        "                 batch_size,\n",
        "                 mini_batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "\n",
        "\n",
        "class PPOTrainer(object):\n",
        "    def __init__(self,\n",
        "                 ppo_config,\n",
        "                 model,\n",
        "                 ref_model,\n",
        "                 tokenizer):\n",
        "        self.ppo_config = ppo_config\n",
        "        self.model = model\n",
        "        self.ref_model = ref_model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def step(self,\n",
        "             prompts_tensor,\n",
        "             responses_tensor,\n",
        "             rewards):\n",
        "        pass"
      ],
      "metadata": {
        "id": "HXVbqQP_cZ3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your training (train it with a single epoch first):\n",
        "\n",
        "batch_size = 4\n",
        "num_epochs = 100\n",
        "\n",
        "trl_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"lvwerra/gpt2-imdb\",\n",
        ")  # The model to be RLHFed\n",
        "trl_ref_model = trl.create_reference_model(trl_model)  # The model used as a base model in KL divergence\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('lvwerra/gpt2-imdb')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# If you implemented PPOConfig by yourself, use your version\n",
        "ppo_config = trl.PPOConfig(batch_size=batch_size,\n",
        "                           mini_batch_size=batch_size)\n",
        "\n",
        "# Notice that we are not relying anymore on trl.PPOTrainer, but your PPOTrainer\n",
        "ppo_trainer = PPOTrainer(ppo_config,\n",
        "                         trl_model,\n",
        "                         trl_ref_model,\n",
        "                         tokenizer)\n"
      ],
      "metadata": {
        "id": "H2NA785Edxzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    for batch_prompts in make_minibatch(prompts, 4):\n",
        "        prompts_tensor = [tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\") for prompt in batch_prompts]\n",
        "        responses_tensor  = [trl.core.respond_to_batch(trl_model, prompt_tensor) for prompt_tensor in prompts_tensor]\n",
        "        prompts_tensor = [p[0] for p in prompts_tensor]\n",
        "        responses_tensor = [r[0] for r in responses_tensor]\n",
        "        responses_strings = [tokenizer.decode(r) for r in responses_tensor]\n",
        "        full_sentences = [b + r for b, r in zip(batch_prompts, responses_strings)]\n",
        "\n",
        "        rewards = reward_model(full_sentences)\n",
        "        rewards = [torch.tensor(r) for r in rewards]\n",
        "\n",
        "        train_stats = ppo_trainer.step(prompts_tensor, responses_tensor, rewards)\n",
        "    print(f'Epoch {epoch + 1} completed')"
      ],
      "metadata": {
        "id": "lAtTc5Nwdxoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DZXuzaZeT5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omh-rUlxyLHk"
      },
      "source": [
        "## Exercise: Sentiment playground - Post RLHF\n",
        "\n",
        "```c\n",
        "Difficulty: ðŸŸ âšªâšªâšªâšª\n",
        "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
        "\n",
        "You should spend up to ~10 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Try out your RLHF'd model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8PTFZ8sAvRS"
      },
      "outputs": [],
      "source": [
        "eval_model = ppo_trainer.model.cpu()\n",
        "generate_completion(\"This movie\", eval_model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}