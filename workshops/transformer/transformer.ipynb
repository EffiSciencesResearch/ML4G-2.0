{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformers from scratch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "Thanks to Callum McDougall for this Notebook.\n",
                "\n",
                "We also advise you not to look too much at the bonus or collapsed nerd-sniping stuff."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/TransformerLens-intro/main/images/page_images/transformer-building.png\" width=\"350\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is a clean, first principles implementation of GPT-2 in PyTorch. The architectural choices closely follow those used by the TransformerLens library (which you'll be using a lot more in later exercises).\n",
                "\n",
                "Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly). Please do skip exercises / look at solutions if you don't feel like they're important enough to be worth doing, and you'd rather get to the good stuff!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Content & Learning Objectives\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 1ï¸âƒ£ Understanding Inputs & Outputs of a Transformer\n",
                "\n",
                "In this section, we'll take a first look at transformers - what their function is, how information moves inside a transformer, and what inputs & outputs they take.\n",
                "\n",
                "> ##### Learning objectives\n",
                ">\n",
                "> - Understand what a transformer is used for\n",
                "> - Understand causal attention, and what a transformer's output represents\n",
                "> - Learn what tokenization is, and how models do it\n",
                "> - Understand what logits are, and how to use them to derive a probability distribution over the vocabulary\n",
                "\n",
                "#### 2ï¸âƒ£ Clean Transformer Implementation\n",
                "\n",
                "Here, we'll implement a transformer from scratch, using only PyTorch's tensor operations. This will give us a good understanding of how transformers work, and how to use them. We do this by going module-by-module, in an experience which should feel somewhat similar to last week's ResNet exercises. Much like with ResNets, you'll conclude by loading in pretrained weights and verifying that your model works as expected.\n",
                "\n",
                "> ##### Learning objectives\n",
                ">\n",
                "> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream\n",
                "> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)\n",
                "> * Learn about & implement the following transformer modules:\n",
                ">     * (Bonus) LayerNorm (transforming the input to have zero mean and unit variance)\n",
                ">     * Positional embedding (a lookup table from position indices to residual stream vectors)\n",
                ">     * Attention (the method of computing attention patterns for residual stream vectors)\n",
                ">     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
                ">     * Embedding (a lookup table from tokens to residual stream vectors)\n",
                ">     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup (don't read, just run!)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install numpy==2.0 transformer_lens einops jaxtyping circuitsvis -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
                "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
                "import einops\n",
                "from dataclasses import dataclass\n",
                "from transformer_lens import HookedTransformer\n",
                "from transformer_lens.utils import gelu_new\n",
                "import torch\n",
                "from torch import Tensor\n",
                "import torch.nn as nn\n",
                "import math\n",
                "from tqdm.notebook import tqdm\n",
                "from jaxtyping import Float, Int\n",
                "\n",
                "device = torch.device(\"cpu\")\n",
                "\n",
                "reference_gpt2 = HookedTransformer.from_pretrained(\n",
                "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
                ").to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1ï¸âƒ£ Understanding Inputs & Outputs of a Transformer\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ### Learning Objectives\n",
                ">\n",
                "> * Understand what a transformer is used for\n",
                "> * Understand causal attention, and what a transformer's output represents\n",
                "> * Learn what tokenization is, and how models do it\n",
                "> * Understand what logits are, and how to use them to derive a probability distribution over the vocabulary\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What is the point of a transformer?\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Transformers exist to model text!**\n",
                "\n",
                "We're going to focus GPT-2 style transformers. Key feature: They generate text! You feed in language, and the model generates a probability distribution over tokens. And you can repeatedly sample from this to generate text!\n",
                "\n",
                "(To explain this in more detail - you feed in a sequence of length $N$, then sample from the probability distribution over the $N+1$-th word, use this to construct a new sequence of length $N+1$, then feed this new sequence into the model to get a probability distribution over the $N+2$-th word, and so on.)\n",
                "\n",
                "### How is the model trained?\n",
                "\n",
                "You give it a bunch of text, and train it to predict the next token.\n",
                "\n",
                "Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, i.e. it produces 100 logit vectors (= probability distributions) over the set of all words in our vocabulary, with the `i`-th logit vector representing the probability distribution over the token *following* the `i`-th token in the sequence.\n",
                "\n",
                "<details>\n",
                "<summary>Aside - logits</summary>\n",
                "\n",
                "If you haven't encountered the term \"logits\" before, here's a quick refresher.\n",
                "\n",
                "Given an arbitrary vector $x$, we can turn it into a probability distribution via the **softmax** function: $x_i \\to \\frac{e^{x_i}}{\\sum e^{x_j}}$. The exponential makes everything positive; the normalization makes it add to one.\n",
                "\n",
                "The model's output is the vector $x$ (one for each prediction it makes). We call this vector a logit because it represents a probability distribution, and it is related to the actual probabilities via the softmax function.\n",
                "</details>\n",
                "\n",
                "How do we stop the transformer by \"cheating\" by just looking at the tokens it's trying to predict? Answer - we make the transformer have *causal attention* (as opposed to *bidirectional attention*). Causal attention only allows information to move forwards in the sequence, never backwards. The prediction of what comes after token 50 is only a function of the first 50 tokens, *not* of token 51. We say the transformer is **autoregressive**, because it only predicts future words based on past data.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-overview-new.png\" width=\"900\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokens - Transformer Inputs\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Our transformer's input is natural language (i.e., a string of characters). But ML models generally take vectors as input, not language. How do we convert language to vectors?\n",
                "\n",
                "We can factor this into 2 questions:\n",
                "\n",
                "1. How do we split up language into small sub-units?\n",
                "2. How do we convert these sub-units into vectors?\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Splitting language into sub-units (tokenizing)\n",
                "\n",
                "We need to find a way of splitting up language into substrings, where each substring is a member of our **vocabulary** set.\n",
                "\n",
                "Could we use a dictionary, and have our vocabulary be the set of all words in the dictionary? No, because this couldn't handle arbitrary text (e.g. URLs, punctuation, other languages, etc.)\n",
                "\n",
                "Could we just use the 256 ASCII characters? This partly addresses the previous problem, but it is an inefficient solution and still wouldn't support other alphabets.\n",
                "\n",
                "Intead, we'll take advantage of the fact that some sequences of characters appear more frequently together and use those to get an efficient yet flexible solution. We will create a dictionary of word-parts called **tokens**. We'll start with the 256 ASCII characters as our first tokens, and then use a technique called *Byte-pair encoding* to find the most common pair of tokens. We will repeat this process until our vocabulary capacity is filled.\n",
                "\n",
                "Because the space character (`' '`) is one of our first 256 tokens many merges contain it. Here are the five first merges for the tokenizer used by GPT-2 (you'll be able to verify this below).\n",
                "\n",
                "```\n",
                "\" t\"\n",
                "\" a\"\n",
                "\"he\"\n",
                "\"in\"\n",
                "\"re\"\n",
                "```\n",
                "\n",
                "Note - you might see the character `Ä ` in front of some tokens. This is a special character that indicates that the token begins with a space. Tokens with a leading space are unique from those without.\n",
                "\n",
                "You can run the code below to see some more of GPT-2's tokenizer's vocabulary:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])\n",
                "print(sorted_vocab[:20])\n",
                "print(sorted_vocab[970:980])\n",
                "print(sorted_vocab[-10:])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see from the last print statement, near the end of the vocabulary (which has 50,257 elements in the case of GPT-2) we get some pretty esoteric tokens. This is because all the short, frequently-occurring pairs have already been represented."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Transformers in the `transformer_lens` library have a `to_tokens` method that converts text to numbers. It also prepends them with a special token called BOS (beginning of sequence) to indicate the start of a sequence. You can disable this with the `prepend_bos=False` argument.\n",
                "\n",
                "### Some tokenization annoyances\n",
                "\n",
                "There are a few funky and frustrating things about tokenization which cause it to behave differently than you might expect, such as how it handles capitalization and leading spaces. \n",
                "\n",
                "You can spend a minute playing around with tokenization in [tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2). Enter the word \"Ralph\", then see how tokenization changes with an initial capital letter vs. not, and with a leading space vs. not. Then compare that with how a common word like \"water\" gets tokenized in different forms."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ### Key Takeaways\n",
                ">\n",
                "> * We learn a dictionary of vocab of tokens (sub-words).\n",
                "> * We losslessly convert language to integers via tokenizing.\n",
                "> * We convert integers to vectors via a lookup table (embedding).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Converting sub-units to vectors (embedding)\n",
                "\n",
                "Now that we have a list of tokens (integers) representing our input text, we just have to convert them into vectors before we can feed them into the transformer. To do this, we build a lookup table which we call an **embedding**. Each row of this table will store a unique vector for each token in our vocabulary. This vector is a **one-hot encoding**, meaning one element of the vector is set to 1, while all the rest are 0.\n",
                "\n",
                "<details>\n",
                "<summary>Aside - One-hot encodings</summary>\n",
                "\n",
                "The single 1 in a one-hot encoded vector corresponds to the word's index in the vocabulary. This means that indexing into the embedding (lookup table) is equivalent to multiplying the **embedding matrix** by the one-hot encoding (where the embedding matrix is the matrix we get by stacking all the embedding vectors on top of each other).\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "W_E &= \\begin{bmatrix}\n",
                "\\leftarrow v_0 \\rightarrow \\\\\n",
                "\\leftarrow v_1 \\rightarrow \\\\\n",
                "\\vdots \\\\\n",
                "\\leftarrow v_{d_{vocab}-1} \\rightarrow \\\\\n",
                "\\end{bmatrix} \\quad \\text{is the embedding matrix (size }d_{vocab} \\times d_{embed}\\text{),} \\\\\n",
                "\\\\\n",
                "t_i &= (0, \\dots, 0, 1, 0, \\dots, 0) \\quad \\text{is the one-hot encoding for the }i\\text{th word (length }d_{vocab}\\text{)} \\\\\n",
                "\\\\\n",
                "v_i &= t_i W_E \\quad \\text{is the embedding vector for the }i\\text{th word (length }d_{embed}\\text{).} \\\\\n",
                "\\end{aligned}\n",
                "$$\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Text generation\n",
                "\n",
                "Now that we understand the basic ideas, let's go through the entire process of text generation, from our original string to a new token which we can append to our string and feed back into the model.\n",
                "\n",
                "#### **Step 1:** Convert text to tokens\n",
                "\n",
                "The sequence gets tokenized, so it has shape `[batch, seq_len]`. Here, the batch dimension is just one because we only have one sequence. Think of `batch` as how many documents we process, and `seq_len` as document length in tokens.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
                "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
                "print(tokens)\n",
                "print(tokens.shape)\n",
                "print(reference_gpt2.to_str_tokens(tokens))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Step 2:** Map tokens to logits\n",
                "\n",
                "\n",
                "After running a feed-forward pass on our model with an input with shape `[batch, seq_len]`, we get an output of shape `[batch, seq_len, vocab_size]`. The `[i, j, :]`-th element of our output is a vector of logits representing our prediction for the `j+1`-th token in the `i`-th sequence.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
                "print(logits.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(`run_with_cache` tells the model to cache all intermediate activations. This isn't important right now; we'll look at it in more detail later.)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Step 3:** Convert the logits to a distribution with a softmax\n",
                "\n",
                "This doesn't change the shape, it is still `[batch, seq_len, vocab_size]`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "probs = logits.softmax(dim=-1)\n",
                "print(probs.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Bonus step:** What is the most likely next token at each position?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "most_likely_next_tokens = reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])\n",
                "\n",
                "\n",
                "print(\"          token\", \"-->\", \"next_token\")\n",
                "for token, next_token in zip(reference_gpt2.to_str_tokens(tokens), most_likely_next_tokens):\n",
                "    print(f\"{token!r:>15} --> {next_token!r}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see that, in a few cases (particularly near the end of the sequence), the model accurately predicts the next token in the sequence. We might guess that `\"take over the world\"` is a common phrase that the model has seen in training, which is why the model can predict it.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Step 4:** Map distribution to a token\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "next_token = logits[0, -1].argmax(dim=-1)\n",
                "next_char = reference_gpt2.to_string(next_token)\n",
                "print(repr(next_char))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that we're indexing `logits[0, -1]`. This is because logits have shape `[1, sequence_length, vocab_size]`, so this indexing returns the vector of length `vocab_size` representing the model's prediction for what token follows the **last** token in the input sequence.\n",
                "\n",
                "In this case, we can see that the model predicts the token `' I'`.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### **Step 5:** Add this to the end of the input, re-run\n",
                "\n",
                "There are more efficient ways to do this (e.g. where we cache some of the values each time we run our input, so we don't have to do as much calculation each time we generate a new value), but this doesn't matter conceptually right now.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Sequence so far: {reference_gpt2.to_string(tokens)[0]!r}\")\n",
                "\n",
                "for i in range(10):\n",
                "    print(f\"{tokens.shape[-1]+1}th char = {next_char!r}\")\n",
                "    # Define new input sequence, by appending the previously generated token\n",
                "    tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n",
                "    # Pass our new sequence through the model, to get new output\n",
                "    logits = reference_gpt2(tokens)\n",
                "    # Get the predicted token at the end of our sequence\n",
                "    next_token = logits[0, -1].argmax(dim=-1)\n",
                "    # Decode and print the result\n",
                "    next_char = reference_gpt2.to_string(next_token)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key takeaways\n",
                "\n",
                "* The transformer takes in language and predicts the next token, one at a time\n",
                "* We convert language to a sequence of integers with a tokenizer\n",
                "* We convert integers to vectors with an embedding\n",
                "* The transformer's output is a vector of logits (one for each input token) which we convert to a probability distribution using a softmax\n",
                "* The distrubution is sampled to select a new token\n",
                "* We append this new token to the input and then run again to continue generation (Jargon: *autoregressive*)\n",
                "* (Meta-level point: Transformers are sequence operation models; they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2ï¸âƒ£ Clean Transformer Implementation\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> ##### Learning objectives\n",
                ">\n",
                "> * Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream\n",
                "> * Understand that the attention heads in a single layer operate independently, and that they have the role of calculating attention patterns (which determine where information is moved to & from in the residual stream)\n",
                "> * Learn about & implement the following transformer modules:\n",
                ">     * (Bonus) LayerNorm (transforming the input to have zero mean and unit variance)\n",
                ">     * Positional embedding (a lookup table from position indices to residual stream vectors)\n",
                ">     * Attention (the method of computing attention patterns for residual stream vectors)\n",
                ">     * MLP (the collection of linear and nonlinear transformations which operate on each residual stream vector in the same way)\n",
                ">     * Embedding (a lookup table from tokens to residual stream vectors)\n",
                ">     * Unembedding (a matrix for converting residual stream vectors into a distribution over tokens)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## High-Level architecture\n",
                "\n",
                "Go watch Neel's [Transformer Circuits walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) if you want more intuitions!\n",
                "\n",
                "(Diagram is read bottom to top)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-new.png\" width=\"850\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tokenization & Embedding\n",
                "\n",
                "The input tokens $t$ are integers. We get them from taking a sequence, and tokenizing it (like we saw in the previous section).\n",
                "\n",
                "The token embedding is a lookup table mapping tokens to vectors, which is implemented as a matrix $W_E$. The matrix consists of a stack of token embedding vectors (one for each token).\n",
                "\n",
                "We also do a **positional embedding** here which gives the model information about each token's location in the sequence. The result gets summed with the result of the token embedding.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Residual stream\n",
                "\n",
                "The residual stream is the sum of all previous outputs of layers of the model, is the input to each new layer. It has shape `[batch, seq_len, d_model]` (where `d_model` is the length of a single embedding vector).\n",
                "\n",
                "The initial value of the residual stream is denoted $x_0$ in the diagram, and $x_i$ are later values of the residual stream (after more attention and MLP layers have been applied to the residual stream).\n",
                "\n",
                "The residual stream is *really* fundamental. It's the central object of the transformer. It's how model remembers things, moves information between layers for composition, and it's the medium used to store the information that attention moves between positions.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformer blocks\n",
                "\n",
                "Then we have a series of `n_layers` **transformer blocks** (also sometimes called **residual blocks**).\n",
                "\n",
                "Note - a block contains an attention layer *and* an MLP layer, but we say a transformer has $k$ layers if it has $k$ blocks (i.e. $2k$ total layers).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Attention\n",
                "\n",
                "Attention heads decide which tokens matter most for predicting the next one, copying info from those spots.\n",
                "\n",
                "*Every* token is evaluated in parallel using the same parameters. The only difference between tokens is that we look backwards only (to avoid \"cheating\" during training), which means that later tokens have more context than earlier tokens.\n",
                "\n",
                "Attention layers are the only bit of a transformer that moves information between positions (i.e. between vectors at different sequence positions in the residual stream).\n",
                "\n",
                "Attention layers are made up of `n_heads` attention heads - each with their own parameters, own attention pattern, and own information how to copy things from source to destination. The heads act independently and additively, we just sum their outputs together back into the residual stream.\n",
                "\n",
                "Each head does the following:\n",
                "* Produces an **attention pattern** for each destination token, a probability distribution of prior source tokens (including the current one) weighting how much information to copy.\n",
                "* Moves information (via a linear map) in the same way from each source token to each destination token.\n",
                "\n",
                "A few key points:\n",
                "\n",
                "* What information we copy depends on the source token's *residual stream*, but this doesn't mean it only depends on the value of that token, because the residual stream can store more information than just the token identity (the purpose of the attention heads is to move information between tokens).\n",
                "* We can think of each attention head as consisting of two different **circuits**:\n",
                "    * One circuit determines **where to move information to and from** (this is a function of the residual stream for the source/key and destination/query tokens)\n",
                "    * The other circuit determines **what information to move** (this is a function of only the source token's residual stream)\n",
                "    * For reasons which will become clear later, we refer to the first circuit as the **QK circuit**, and the second circuit as the **OV circuit**\n",
                "\n",
                "\n",
                "Below is a schematic diagram of the attention layers. Don't worry if you don't follow this right now, we'll go into more detail during implementation.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-new.png\" width=\"1100\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MLP\n",
                "\n",
                "The MLP layers are just a standard neural network, with a singular hidden layer and a nonlinear activation function. The exact activation function used isn't conceptually important, though [GeLU](https://paperswithcode.com/method/gelu) seems to perform best.\n",
                "\n",
                "Our hidden dimension is normally `d_mlp = 4 * d_model`. Exactly why the ratios are what they are isn't super important (people basically cargo-cult what GPT did back in the day!).\n",
                "\n",
                "Importantly, **the MLP operates on positions in the residual stream independently, and in exactly the same way**. It doesn't move information between positions.\n",
                "\n",
                "Intuition - once attention has moved relevant information to a single position in the residual stream, MLPs can actually do computation, reasoning, lookup information, etc. *What the hell is going on inside MLPs* is a pretty big open problem in transformer mechanistic interpretability - see the [Toy Model of Superposition Paper](https://transformer-circuits.pub/2022/toy_model/index.html) for more on why this is hard.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-mlp-new-2.png\" width=\"650\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Unembedding\n",
                "\n",
                "Finally, we unembed!\n",
                "\n",
                "This just consists of applying a linear map $W_U$, going from final residual stream to a vector of logits - this is the output.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Bonus things - less conceptually important but key technical details</summary>\n",
                "\n",
                "#### LayerNorm\n",
                "* Simple normalization function applied at the start of each layer (i.e. before each MLP, attention layer, and before the unembedding)\n",
                "* Converts each input vector (independently in parallel for each batch x position residual stream vector) to have mean `0.0` and variance `1.0`\n",
                "* Then applies an element-wise scaling and translation\n",
                "* Cool maths tangent: The scale & translate is just a linear map. LayerNorm is only applied immediately before another linear map. Linear compose linear = linear, so we can just fold this into a single effective linear layer and ignore it.\n",
                "    * `fold_ln=True` flag in `from_pretrained` does this for you.\n",
                "* LayerNorm is annoying for interpertability - the scale part is not linear, so you can't think about different bits of the input independently. But it's *almost* linear - if you're changing a small part of the input it's linear, but if you're changing enough to alter the norm substantially it's not linear.\n",
                "\n",
                "\n",
                "#### Positional embeddings\n",
                "\n",
                "* **Problem:** Attention operates over all pairs of positions. This means it's symmetric with regards to position - the attention calculation from token 5 to token 1 and token 5 to token 2 are the same by default\n",
                "    * This is inefficient because nearby tokens are often more relevant.\n",
                "* One solution is **learned, absolute positional embeddings**. This involves a learned lookup table mapping each token's position to a residual stream vector, and adding this to the embed.\n",
                "    * Note that we *add* rather than concatenate. This is because the residual stream is shared memory, and likely under significant superposition (the model compresses more features in there than the model has dimensions)\n",
                "    * We basically never concatenate inside a transformer, unless doing weird shit like generating text efficiently.\n",
                "* This connects to **attention as generalized convolution**\n",
                " * Since language does have locality it is helpful for transformers to have access to positional information so they \"know\" whether two tokens are next to each other, and hence likely relevant to each other.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Actual Code!\n",
                "\n",
                "### Parameters and Activations\n",
                "\n",
                "It's important to distinguish between parameters and activations in the model.\n",
                "\n",
                "* **Parameters** are the weights and biases that are learned during training.\n",
                "    * These don't change when the model input changes.\n",
                "* **Activations** are temporary numbers calculated during a forward pass, that are functions of the input.\n",
                "    * We can think of these values as only existing for the duration of a single forward pass, and disappearing afterwards.\n",
                "    * We can use hooks to access these values during a forward pass (more on hooks later), but it doesn't make sense to talk about a model's activations outside the context of some particular input.\n",
                "    * Attention scores and patterns are activations (this is slightly non-intuitve because they're used in a matrix multiplication with another activation).\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Print All Parameters Shapes of Reference Model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for name, param in reference_gpt2.named_parameters():\n",
                "    # Only print for first layer\n",
                "    if \".0.\" in name or \"blocks\" not in name:\n",
                "        print(f\"{name:18} {tuple(param.shape)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Config\n",
                "\n",
                "The config object contains all the hyperparameters of the model. We can print the config of the reference model to see what it contains:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures\n",
                "print(reference_gpt2.cfg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We define a stripped down config for our model:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class Config:\n",
                "    d_model: int = 768  # dimension of the residual_stream\n",
                "    debug: bool = True\n",
                "    d_vocab: int = 50257\n",
                "    n_ctx: int = 1024  # max nb of tokens that the model can handle\n",
                "    d_head: int = 64  # dimension of each key/query/value\n",
                "    d_mlp: int = 3072  # dimension of the hidden layer inside the MLPs\n",
                "    n_heads: int = 12  # Nb of heads\n",
                "    n_layers: int = 12  # Nb of (Attention+ MLP) in the GPT\n",
                "\n",
                "    layer_norm_eps: float = 1e-5  # (Bonus)\n",
                "    init_range: float = 0.02  # (bonus) standard deviation of 0.02 for weight initialization\n",
                "\n",
                "\n",
                "cfg = Config()\n",
                "print(cfg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tests\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tests are great, write lightweight ones to use as you go!\n",
                "\n",
                "**Naive test:** Generate random inputs of the right shape, input to your model, check whether there's an error and print the correct output.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rand_float_test(cls, shape):\n",
                "    cfg = Config(debug=True)\n",
                "    layer = cls(cfg).to(device)\n",
                "    random_input = torch.randn(shape).to(device)\n",
                "    print(\"Input shape:\", random_input.shape)\n",
                "    output = layer(random_input)\n",
                "    if isinstance(output, tuple):\n",
                "        output = output[0]\n",
                "    print(\"Output shape:\", output.shape, \"\\n\")\n",
                "\n",
                "\n",
                "def rand_int_test(cls, shape):\n",
                "    cfg = Config(debug=True)\n",
                "    layer = cls(cfg).to(device)\n",
                "    random_input = torch.randint(100, 1000, shape).to(device)\n",
                "    print(\"Input shape:\", random_input.shape)\n",
                "    output = layer(random_input)\n",
                "    if isinstance(output, tuple):\n",
                "        output = output[0]\n",
                "    print(\"Output shape:\", output.shape, \"\\n\")\n",
                "\n",
                "\n",
                "def load_gpt2_test(cls, gpt2_layer, input):\n",
                "    cfg = Config(debug=True)\n",
                "    layer = cls(cfg).to(device)\n",
                "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
                "    print(\"Input shape:\", input.shape)\n",
                "    output = layer(input)\n",
                "    if isinstance(output, tuple):\n",
                "        output = output[0]\n",
                "    print(\"Output shape:\", output.shape)\n",
                "    try:\n",
                "        reference_output = gpt2_layer(input)\n",
                "    except:\n",
                "        reference_output = gpt2_layer(input, input, input)\n",
                "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
                "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
                "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Embedding\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
                "\n",
                "You should spend up to 5-10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "This is basically a lookup table from tokens to residual stream vectors.\n",
                "\n",
                "(Hint - you can implement this in just one line, without any complicated functions.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Embed(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
                "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_model\"]:\n",
                "        \"\"\"Compute the embedding of the input tokens.\"\"\"\n",
                "        # Hide: all\n",
                "        return self.W_E[tokens]\n",
                "        # Hide: None\n",
                "\n",
                "\n",
                "rand_int_test(Embed, [2, 4])\n",
                "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Help - I keep getting <code>RuntimeError: CUDA error: device-side assert triggered</code>.</summary>\n",
                "\n",
                "This is a uniquely frustrating type of error message, because it (1) forces you to restart the kernel, and (2) often won't tell you where the error message actually originated from!\n",
                "\n",
                "You can fix the second problem by adding the line `os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"` to the very top of your file (after importing `os`). This won't fix your bug, but it makes sure the correct origin point is identified.\n",
                "\n",
                "As for actually fixing the bug, this error usually ends up being the result of bad indexing, e.g. you're trying to apply an embedding layer to tokens which are larger than your maximum embedding.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Positional Embedding\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
                "\n",
                "You should spend up to 10-15 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Positional embedding can also be thought of as a lookup table, but rather than the indices being our token IDs, the indices are just the numbers `0`, `1`, `2`, ..., `seq_len-1` (i.e. the position indices of the tokens in the sequence).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PosEmbed(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
                "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_model\"]:\n",
                "        # Hint: You should use the einops.repeat or torch.repeat function\n",
                "        # to repeat batch-wise the positional embedding.\n",
                "        # Hide: hard\n",
                "        # The value of tokens is not important here, only the size of the tensor!\n",
                "        # Hide: all\n",
                "        batch, seq_len = tokens.shape\n",
                "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
                "        # Or self.W_pos[:seq_len].repeat(batch, 1, 1)\n",
                "        # Hide: none\n",
                "\n",
                "\n",
                "rand_int_test(PosEmbed, [2, 4])\n",
                "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Attention\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ ðŸŸ \n",
                "\n",
                "You should spend up to 30-45 minutes on this exercise.\n",
                "```\n",
                "\n",
                "* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (including current token)\n",
                "    * Linear map from input -> query, key shape `[batch, seq_posn, head_index, d_head]`\n",
                "    * Dot product every *pair* of queries and keys to get attn_scores `[batch, head_index, query_pos, key_pos]` (query = dest, key = source)\n",
                "    * **Scale** and mask `attn_scores` to make it lower triangular, i.e. causal\n",
                "    * Softmax along the `key_pos` dimension, to get a probability distribution for each query (destination) token - this is our attention pattern!\n",
                "* **Step 2:** Move information from source tokens to destination token using attention pattern\n",
                "    * Linear map from input -> value `[batch, key_pos, head_index, d_head]`\n",
                "    * Mix along the `key_pos` with attn pattern to get `z`, which is a weighted average of the value vectors `[batch, query_pos, head_index, d_head]`\n",
                "    * Map to output, `[batch, position, d_model]` (position = query_pos, we've summed over all heads)\n",
                "\n",
                "Note - when we say **scale**, we mean dividing by `sqrt(d_head)`. The purpose of this is to avoid vanishing gradients (which is a big problem when we're dealing with a function like softmax - if one of the values is much larger than all the others, the probabilities will be close to 0 or 1, and the gradients will be close to 0).\n",
                "\n",
                "Below is a much larger, more detailed version of the attention head diagram from earlier. This should give you an idea of the actual tensor operations involved. A few clarifications on this diagram:\n",
                "\n",
                "* Whenever there is a third dimension shown in the pictures, this refers to the `head_index` dimension. We can see that all operations within the attention layer are done independently for each head.\n",
                "* The objects in the box are activations; they have a batch dimension (for simplicity, we assume the batch dimension is 1 in the diagram). The objects to the right of the box are our parameters (weights and biases); they have no batch dimension."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-21.png\" width=\"1400\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary><b>A few extra notes on attention (optional)</b></summary>\n",
                "\n",
                "Usually we have the relation `e = n * h` (i.e. `d_model = num_heads * d_head`). There are some computational justifications for this, but mostly this is just done out of convention (just like how we usually have `d_mlp = 4 * d_model`!).\n",
                "\n",
                "---\n",
                "\n",
                "The names **keys**, **queries** and **values** come from their analogy to retrieval systems. Broadly speaking:\n",
                "\n",
                "* The **queries** represent some information that a token is **\"looking for\"**\n",
                "* The **keys** represent the information that a token **\"contains\"**\n",
                "    * So the attention score being high basically means that the source (key) token contains the information which the destination (query) token **is looking for**\n",
                "* The **values** represent the information that is actually taken from the source token, to be moved to the destination token\n",
                "\n",
                "---\n",
                "\n",
                "This diagram can better help us understand the difference between the **QK** and **OV** circuit. We'll discuss this just briefly here, and will go into much more detail later on.\n",
                "\n",
                "The **QK** circuit consists of the operation of the $W_Q$ and $W_K$ matrices. In other words, it determines the attention pattern, i.e. where information is moved to and from in the residual stream. The functional form of the attention pattern $A$ is:\n",
                "\n",
                "$$\n",
                "A = \\text{softmax}\\left(\\frac{x^T W_Q W_K^T x}{\\sqrt{d_{head}}}\\right)\n",
                "$$\n",
                "\n",
                "where $x$ is the residual stream (shape `[seq_len, d_model]`), and $W_Q$, $W_K$ are the weight matrices for a single head (i.e. shape `[d_model, d_head]`).\n",
                "\n",
                "The **OV** circuit consists of the operation of the $W_V$ and $W_O$ matrices. Once attention patterns are fixed, these matrices operate on the residual stream at the source position, and their output is the thing which gets moved from source to destination position.\n",
                "\n",
                "The functional form of an entire attention head is:\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "\\text{output} &= \\text{softmax}\\left(\\frac{x W_Q W_K^T x^T}{\\sqrt{d_{head}}}\\right) (x W_V W_O) \\\\\n",
                "    &= Ax W_V W_O\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "where $W_V$ has shape `[d_model, d_head]`, and $W_O$ has shape `[d_head, d_model]`.\n",
                "\n",
                "Here, we can clearly see that the **QK circuit** and **OV circuit** are doing conceptually different things, and should be thought of as two distinct parts of the attention head.\n",
                "\n",
                "Again, don't worry if you don't follow all of this right now - we'll go into **much** more detail on all of this in subsequent exercises. The purpose of the discussion here is just to give you a flavour of what's to come!\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First, it's useful to visualize and play around with attention patterns - what exactly are we looking at here? (Click on a head to lock onto just showing that head's pattern, it'll make it easier to interpret)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import circuitsvis as cv\n",
                "from IPython.display import display\n",
                "\n",
                "html = cv.attention.attention_patterns(\n",
                "    tokens=reference_gpt2.to_str_tokens(reference_text),\n",
                "    attention=cache[\"pattern\", 0][0],\n",
                ")\n",
                "display(html)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can also use the `attention_heads` function, which has similar syntax but presents the information in a different (sometimes more helpful) way.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Help - my <code>attention_heads</code> plots are behaving weirdly.</summary>\n",
                "\n",
                "This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.\n",
                "\n",
                "Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:\n",
                "\n",
                "```python\n",
                "attn_heads = cv.attention.attention_heads(\n",
                "    tokens=reference_gpt2.to_str_tokens(reference_text),\n",
                "    attention=cache[\"pattern\", 0][0]\n",
                ")\n",
                "\n",
                "path = \"attn_heads.html\"\n",
                "\n",
                "with open(path, \"w\") as f:\n",
                "    f.write(str(attn_heads))\n",
                "\n",
                "webbrowser.open(path)\n",
                "```\n",
                "\n",
                "To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "Note - don't worry if you don't get 100% accuracy here; the tests are pretty stringent. Even things like having your `einsum` input arguments in a different order might result in the output being very slightly different. You should be getting at least 99% accuracy though, so if the value is lower then this it probably means you've made a mistake somewhere.\n",
                "\n",
                "Also, this implementation will probably be the most challenging exercise on this page, so don't worry if it takes you some time! You should look at parts of the solution or hints if you're stuck.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Hint: high level steps</summary>\n",
                "\n",
                "```python\n",
                "# Calculate query, key and value vectors\n",
                "# Calculate attention scores\n",
                "# Then scale and apply mask and apply softmax on the correct dimension to get probabilities\n",
                "# Take weighted sum of value vectors, according to attention probabilities\n",
                "# Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
                "```\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "<summary>Hint: detailed steps with only a few blanks to fill.</summary>\n",
                "\n",
                "```python\n",
                "# Calculate query, key and value vectors\n",
                "q = (\n",
                "    einops.einsum(\n",
                "        normalized_resid_pre,\n",
                "        self.W_Q,\n",
                "        \"batch token d_model, head d_model d_head -> ???\",\n",
                "    )\n",
                "    + self.b_Q\n",
                ")\n",
                "\n",
                "k = ...\n",
                "\n",
                "v = ...\n",
                "\n",
                "# Calculate attention scores\n",
                "attn_scores = einops.einsum(\n",
                "    q,\n",
                "    k,\n",
                "    \"???,??? -> batch head token_Q token_K\",\n",
                ")\n",
                "\n",
                "# then scale and apply mask and apply softmax on the correct dimension to get probabilities\n",
                "attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
                "attn_pattern = attn_scores_masked.softmax(dim=...)\n",
                "\n",
                "# Take weighted sum of value vectors, according to attention probabilities\n",
                "z = einops.einsum(\n",
                "    v,\n",
                "    attn_pattern,\n",
                "    \"???,??? -> batch token_Q head d_head\",\n",
                ")\n",
                "\n",
                "# Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
                "attn_out = (\n",
                "    einops.einsum(\n",
                "        z,\n",
                "        self.W_O,\n",
                "        \"batch token_Q head d_head, head d_head d_model -> batch token_Q d_model\",\n",
                "    )\n",
                "    + self.b_O\n",
                ")\n",
                "return attn_out\n",
                "```\n",
                "</details>\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Attention(nn.Module):\n",
                "    IGNORE: Float[Tensor, \"\"]\n",
                "\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
                "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
                "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
                "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
                "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
                "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=device))\n",
                "\n",
                "    # fmt: off\n",
                "    def forward(\n",
                "        self, normalized_resid_pre: Float[Tensor, \"batch token d_model\"]\n",
                "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
                "        # Use einops!\n",
                "        # And to help us understand your code quickly, try to use only the following names with einops:\n",
                "        # batch head token token_k token_q d_model d_head\n",
                "\n",
                "        # Hide: hard\n",
                "        # Calculate query, key and value vectors\n",
                "        # Hide: all\n",
                "        q = einops.einsum(normalized_resid_pre, self.W_Q,\n",
                "                \"batch token d_model, head d_model d_head -> batch token head d_head\",\n",
                "            ) + self.b_Q\n",
                "        k = einops.einsum(normalized_resid_pre, self.W_K,\n",
                "                \"batch token d_model, head d_model d_head -> batch token head d_head\",\n",
                "            ) + self.b_K\n",
                "        v = einops.einsum(normalized_resid_pre, self.W_V,\n",
                "                \"batch token d_model, head d_model d_head -> batch token head d_head\",\n",
                "            ) + self.b_V\n",
                "        # Hide: hard\n",
                "\n",
                "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
                "        # Hide: all\n",
                "        attn_scores = einops.einsum(q, k,\n",
                "            \"batch token_q head d_head, batch token_k head d_head -> batch head token_q token_k\")\n",
                "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
                "        attn_pattern = attn_scores_masked.softmax(-1)\n",
                "        # Hide: hard\n",
                "\n",
                "        # Take weighted sum of value vectors, according to attention probabilities\n",
                "        # Hide: all\n",
                "        z = einops.einsum(v, attn_pattern,\n",
                "            \"batch token_k head d_head, batch head token_q token_k -> batch token_q head d_head\")\n",
                "        # Hide: hard\n",
                "\n",
                "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
                "        # Hide: all\n",
                "        attn_out = einops.einsum(z, self.W_O,\n",
                "                \"batch token_q head d_head, head d_head d_model -> batch token_q d_model\",\n",
                "            ) + self.b_O\n",
                "\n",
                "        return attn_out\n",
                "        # Hide: none\n",
                "\n",
                "    # fmt: on\n",
                "    def apply_causal_mask(\n",
                "        self, attn_scores: Float[Tensor, \"batch head token_q token_k\"]\n",
                "    ) -> Float[Tensor, \"batch head token_q token_k\"]:\n",
                "        \"\"\"\n",
                "        Applies a causal mask to attention scores, and returns masked scores.\n",
                "        \"\"\"\n",
                "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
                "        all_ones = torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
                "        mask = torch.triu(all_ones, diagonal=1).bool()\n",
                "        # Apply the mask to attention scores, then return the masked scores\n",
                "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
                "        return attn_scores\n",
                "\n",
                "\n",
                "rand_float_test(Attention, [2, 4, 768])\n",
                "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: The `\"IGNORE\"` buffer is a very large negative number. This is the value you should mask your attention scores with (i.e. set them to this number wherever you want the probabilities to be zero).\n",
                "\n",
                "<details>\n",
                "<summary>Question - why do you think we mask the attention scores by setting them to a large negative number, rather than the attention probabilities by setting them to zero?</summary>\n",
                "\n",
                "If we masked the attention probabilities, then the probabilities would no longer sum to 1.\n",
                "\n",
                "We want to mask the scores and *then* take softmax, so that the probabilities are still valid probabilities (i.e. they sum to 1), and the values in the masked positions have no influence on the model's output.\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MLP\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ ðŸŸ âšª\n",
                "\n",
                "You should spend up to 10-15 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Next, you should implement the MLP layer, which consists of:\n",
                "\n",
                "* A linear layer, with weight `W_in`, bias `b_in`\n",
                "* A nonlinear functino (we usually use GELU; the function `gelu_new` has been imported for this purpose)\n",
                "* A linear layer, with weight `W_out`, bias `b_out`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MLP(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
                "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
                "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
                "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
                "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
                "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
                "\n",
                "    def forward(\n",
                "        self, normalized_resid_mid: Float[Tensor, \"batch token d_model\"]\n",
                "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
                "        # Hide: all\n",
                "        pre = (\n",
                "            einops.einsum(\n",
                "                normalized_resid_mid,\n",
                "                self.W_in,\n",
                "                \"batch token d_model, d_model d_mlp -> batch token d_mlp\",\n",
                "            )\n",
                "            + self.b_in\n",
                "        )\n",
                "        post = gelu_new(pre)\n",
                "        mlp_out = (\n",
                "            einops.einsum(\n",
                "                post,\n",
                "                self.W_out,\n",
                "                \"batch token d_mlp, d_mlp d_model -> batch token d_model\",\n",
                "            )\n",
                "            + self.b_out\n",
                "        )\n",
                "        return mlp_out\n",
                "        # Hide: none\n",
                "\n",
                "\n",
                "rand_float_test(MLP, [2, 4, 768])\n",
                "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## (Bonus) LayerNorm\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
                "\n",
                "You should spend up to 10-15 minutes on this exercise.\n",
                "```\n",
                "\n",
                "You should fill in the code below, and then run the tests to verify that your layer is working correctly.\n",
                "\n",
                "Your LayerNorm should do the following:\n",
                "\n",
                "* Make mean 0\n",
                "* Normalize to have variance 1\n",
                "* Scale with learned weights\n",
                "* Translate with learned bias\n",
                "\n",
                "You can use the PyTorch [LayerNorm documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) as a reference. A few more notes:\n",
                "\n",
                "* Your layernorm implementation always has `affine=True`, i.e. you do learn parameters `w` and `b` (which are represented as $\\gamma$ and $\\beta$ respectively in the PyTorch documentation).\n",
                "* Remember that, after the centering and normalization, each vector of length `d_model` in your input should have mean 0 and variance 1.\n",
                "* As the PyTorch documentation page says, your variance should be computed using `unbiased=False`.\n",
                "* The `layer_norm_eps` argument in your config object corresponds to the $\\epsilon$ term in the PyTorch documentation (it is included to avoid division-by-zero errors).\n",
                "* We've given you a `debug` argument in your config. If `debug=True`, then you can print output like the shape of objects in your `forward` function to help you debug (this is a very useful trick to improve your coding speed)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LayerNorm(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
                "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
                "\n",
                "    def forward(\n",
                "        self, residual: Float[Tensor, \"batch token d_model\"]\n",
                "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
                "        # Hide: hard\n",
                "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
                "        residual_std = (\n",
                "            residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps\n",
                "        ).sqrt()\n",
                "\n",
                "        residual = (residual - residual_mean) / residual_std\n",
                "        return residual * self.w + self.b\n",
                "        # Hide: none\n",
                "\n",
                "\n",
                "rand_float_test(LayerNorm, [2, 4, 768])\n",
                "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transformer Block\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
                "\n",
                "You should spend up to ~10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "Now, we can put together the attention, MLP and layernorms into a single transformer block. Remember to implement the residual connections correctly!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.ln1 = LayerNorm(cfg)\n",
                "        self.attn = Attention(cfg)\n",
                "        self.ln2 = LayerNorm(cfg)\n",
                "        self.mlp = MLP(cfg)\n",
                "\n",
                "    def forward(\n",
                "        self, resid_pre: Float[Tensor, \"batch token d_model\"]\n",
                "    ) -> Float[Tensor, \"batch token d_model\"]:\n",
                "        # Hide: hard\n",
                "        # First, we add in the attention, but the residual stream needs to be normalized beforehand\n",
                "        # Hide: all\n",
                "        resid_mid = resid_pre + self.attn(self.ln1(resid_pre))\n",
                "        # Hide: hard\n",
                "\n",
                "        # Then, we add in the MLP, again, the input of the MLP needs to be normalized beforehand\n",
                "        # Hide: all\n",
                "        resid_post = resid_mid + self.mlp(self.ln2(resid_mid))\n",
                "        # Hide: hard\n",
                "\n",
                "        return resid_post\n",
                "        # Hide: none\n",
                "\n",
                "\n",
                "rand_float_test(TransformerBlock, [2, 4, 768])\n",
                "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Unembedding\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
                "\n",
                "You should spend up to ~10 minutes on this exercise.\n",
                "```\n",
                "\n",
                "The unembedding is jus a linear layer (with weight `W_U` and bias `b_U`).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Unembed(nn.Module):\n",
                "    def __init__(self, cfg):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
                "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
                "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
                "\n",
                "    def forward(\n",
                "        self, normalized_resid_final: Float[Tensor, \"batch token d_model\"]\n",
                "    ) -> Float[Tensor, \"batch token d_vocab\"]:\n",
                "        # Hide: all\n",
                "        return (\n",
                "            einops.einsum(\n",
                "                normalized_resid_final,\n",
                "                self.W_U,\n",
                "                \"batch token d_model, d_model d_vocab -> batch token d_vocab\",\n",
                "            )\n",
                "            + self.b_U\n",
                "        )\n",
                "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
                "        # Hide: none\n",
                "\n",
                "\n",
                "rand_float_test(Unembed, [2, 4, 768])\n",
                "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Full Transformer\n",
                "\n",
                "```c\n",
                "Difficulty: ðŸŸ ðŸŸ âšªâšªâšª\n",
                "Importance: ðŸŸ ðŸŸ ðŸŸ âšªâšª\n",
                "\n",
                "You should spend up to ~10 minutes on this exercise.\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DemoTransformer(nn.Module):\n",
                "    def __init__(self, cfg: Config):\n",
                "        super().__init__()\n",
                "        self.cfg = cfg\n",
                "        self.embed = Embed(cfg)\n",
                "        self.pos_embed = PosEmbed(cfg)\n",
                "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
                "        self.ln_final = LayerNorm(cfg)\n",
                "        self.unembed = Unembed(cfg)\n",
                "\n",
                "    def forward(self, tokens: Int[Tensor, \"batch token\"]) -> Float[Tensor, \"batch token d_vocab\"]:\n",
                "        # Hint: modules are defined in the order they should be used\n",
                "        # Hide: all\n",
                "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
                "        for block in self.blocks:\n",
                "            residual = block(residual)\n",
                "        logits = self.unembed(self.ln_final(residual))\n",
                "        return logits\n",
                "        # Hide: none\n",
                "\n",
                "\n",
                "rand_int_test(DemoTransformer, [2, 4])\n",
                "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Try it out!**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
                "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
                "\n",
                "demo_logits = demo_gpt2(tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's take a test string, and calculate the loss!\n",
                "\n",
                "We're using the formula for **cross-entropy loss**. The cross entropy loss between a modelled distribution $Q$ and target distribution $P$ is:\n",
                "\n",
                "$$\n",
                "-\\sum_x P(x) \\log Q(x)\n",
                "$$\n",
                "\n",
                "In the case where $P$ is just the empirical distribution from target classes (i.e. $P(x^*) = 1$ for the correct class $x^*$) then this becomes:\n",
                "\n",
                "$$\n",
                "-\\log Q(x^*)\n",
                "$$\n",
                "\n",
                "in other words, the negative log prob of the true classification.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_log_probs(\n",
                "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
                ") -> Float[Tensor, \"batch posn-1\"]:\n",
                "    log_probs = logits.log_softmax(dim=-1)\n",
                "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
                "    # Hide: hard\n",
                "    log_probs_for_tokens = (\n",
                "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
                "    )\n",
                "    # Hide: none\n",
                "\n",
                "    return log_probs_for_tokens\n",
                "\n",
                "\n",
                "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
                "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
                "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
                "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also greedily generate text, by taking the most likely next token and continually appending it to our prompt before feeding it back into the model:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_string = (\n",
                "    \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
                ")\n",
                "for i in tqdm(range(100)):\n",
                "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
                "    demo_logits = demo_gpt2(test_tokens)\n",
                "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
                "\n",
                "print(test_string)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you've finished this, congrats! \n",
                "You should ask the TA what to do next. One option is \n",
                "to look at training and sampling from transformers in the rest of the arena notebook, which you can find it at https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer/transformer-arena.ipynb."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
