{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/rl/A2C-workbook-empty-hard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
    "## Value and target networks\n",
    "First we develop the value network class for A2C. We will use an MLP for the value network. Remeber that the value network in A2C is a state value network, and maps from states to a single value, $V(s;\\phi)$. The target network has the same architecture as the value network, but uses a lagged set of parameters $\\phi^-$.\n",
    "\n",
    "### Architecture recommendations\n",
    "* Use 3 hidden layers with 128 units each\n",
    "* Use ReLU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the (state) value network\n",
    "class VNet(nn.Module):\n",
    "    # Initialise the network\n",
    "    def __init__(self, env: gym.Env):\n",
    "        # Call the parent class\n",
    "        super().__init__()\n",
    "        # Set the input and output size\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        # Define the layers of the network\n",
    "        self.layers = nn.Sequential(_____________________)\n",
    "\n",
    "    def _______(self, x):\n",
    "        return ______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy network\n",
    "Next we develop the policy network class for A2C. We will again use an MLP for the policy network. Remeber that the policy network maps from states to a distribution over actions, $\\pi(a|s;\\theta)$\n",
    "\n",
    "### Architecture recommendations\n",
    "* Use 3 hidden layers with 128 units each\n",
    "* Use ReLU activations\n",
    "* In the last layer, use a softmax activation function to output a probability distribution over actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy network\n",
    "class PolicyNet(nn.Module):\n",
    "    # Initialise the network\n",
    "    def __init__(self, env: gym.Env):\n",
    "        # Call the parent class\n",
    "        super().__init__()\n",
    "        # Set the input and output size\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.n_actions = env.action_space.n\n",
    "        # Define the layers of the network\n",
    "        self.layers = nn.Sequential(\n",
    "            ________________\n",
    "            # After the last linear layer of the network apply a softmax function at dim=-1\n",
    "            ________(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def __________(self, x):\n",
    "        return _________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C agent\n",
    "Finally, we develop the A2C agent class. The agent a policy network, a value network, and a target value network. The policy network and value network both have optimisers. We will also implement a method to sample actions from the policy network and sync the target value network with the value network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent\n",
    "class A2CAgent:\n",
    "    # Initialise the agent\n",
    "    def __init__(\n",
    "        self, env: gym.Env, gamma: float = 0.98, value_lr: float = 0.001, policy_lr: float = 0.0005\n",
    "    ):\n",
    "        # Set the discount factor\n",
    "        self.gamma = gamma\n",
    "        # Create the value network\n",
    "        self.value_network = _______\n",
    "        # Create the target value network\n",
    "        self.target_network = _______\n",
    "        # Sync the target value network with the value network\n",
    "        _________\n",
    "        # Create the policy network\n",
    "        self.policy_network = _________\n",
    "        # Create the value optimizer\n",
    "        self.value_optimizer = ______________\n",
    "        # Create the policy optimizer. Remeber that we are maximising the policy objective\n",
    "        self.policy_optimizer = ________________\n",
    "\n",
    "    # Create the sync method\n",
    "    def sync(self):\n",
    "        # Get the state dict of the value network\n",
    "        value_state_dict = _____________\n",
    "        # Load the state dict into the target value network\n",
    "        ______________________\n",
    "\n",
    "    # Define the sample action function\n",
    "    def sample_action(self, state: np.ndarray) -> int:\n",
    "        # Convert the state to a tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        # Enter no gradient mode\n",
    "        with ____________:\n",
    "            # Get the action probabilities\n",
    "            action_probs = ___________(state)\n",
    "        # Sample the action\n",
    "        action = np.random.choice(len(action_probs), p=________.numpy())\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the environment\n",
    "\n",
    "This method interacts the agent with the environment for a number of steps. This is much the same as interaction in DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interact function\n",
    "def interact(agent: A2CAgent, env: gym.Env, steps: int) -> Tuple[List[dict], List[float]]:\n",
    "    # Initialise the data storage\n",
    "    batch = []\n",
    "    # Initialise the returns list\n",
    "    return_list = []\n",
    "    # Initialize the state\n",
    "    state, _ = env.reset()\n",
    "    # Initialize the episode return\n",
    "    episode_return = 0\n",
    "    for _ in range(steps):\n",
    "        # Create a dictionary to store the datapoint\n",
    "        data = {\"state\": state}\n",
    "        # Choose the action\n",
    "        action = agent.sample_action(state)\n",
    "        data[\"action\"] = action\n",
    "        # Perform the action in the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        data[\"reward\"] = reward\n",
    "        data[\"next_state\"] = next_state\n",
    "        data[\"terminated\"] = terminated\n",
    "        episode_return += reward\n",
    "        batch.append(data)\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "        # Check if the episode is over\n",
    "        if terminated or truncated:\n",
    "            # Store the episode reward\n",
    "            return_list.append(episode_return)\n",
    "            # Reset the episode reward\n",
    "            episode_return = 0\n",
    "            # Reset the environment\n",
    "            state, _ = env.reset()\n",
    "    return batch, return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the value network\n",
    "\n",
    "For each batch of data, we first train the value network. This involves looping over the following steps:\n",
    "1. Extract a mini-batch of data\n",
    "2. Compute the regression targets. Remember that these are given by $$y_i = r_i + \\gamma (1 - d_i) V(s'_i;\\phi^-)$$ where $\\phi^-$ are the parameters of the target value network \n",
    "3. Compute the mean square error loss between the predicted values and the regression targets\n",
    "4. Backpropagate the loss through the value network and update the value network parameters\n",
    "\n",
    "After following this procedure for a number of iterations, we sync the target value network with the value network, $$\\phi^- \\gets \\phi$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value_network(batch: List[dict], agent: A2CAgent, num_iterations: int, mini_batch_size:int = 32): \n",
    "    \n",
    "    # Perform num_iterations gradient updates \n",
    "    for ii in range(num_iterations):\n",
    "        # Sample a mini_batch of data from the batch\n",
    "        mini_batch = random.sample(batch, mini_batch_size)\n",
    "        # Extract the mini-batch of states as float32 tensors \n",
    "        states = torch.tensor([transition[\"state\"] for transition in mini_batch], dtype=torch.float32)\n",
    "        # Extract the mini-batch of rewards as float32 tensors\n",
    "        rewards = torch.tensor([transition[\"reward\"] for transition in mini_batch], dtype=torch.float32)\n",
    "        rewards = rewards.unsqueeze(-1)\n",
    "        # Extract the mini-batch of next states as float32 tensors\n",
    "        next_states = torch.tensor([transition[\"next_state\"] for transition in mini_batch], dtype=torch.float32)\n",
    "        # Extract the mini-batch of terminated flags as bool tensors\n",
    "        terminated = torch.tensor([transition[\"terminated\"] for transition in mini_batch], dtype=torch.bool)\n",
    "\n",
    "        # Enter no-gradient mode \n",
    "        ______________:\n",
    "            # Compute the next state values using the target network\n",
    "            next_state_value = _______________\n",
    "            # Zero out the next state values for the terminal states\n",
    "            next_state_value[_________] = ____\n",
    "            # Compute the regression targets\n",
    "            regression_targets = _____________\n",
    "            \n",
    "        # Compute the value predictions\n",
    "        value_predictions = _______________\n",
    "        # Compute the loss\n",
    "        loss = ________________\n",
    "        # Zero the gradients\n",
    "        _________________\n",
    "        # Compute the gradients\n",
    "        _________________\n",
    "        # Update the weights\n",
    "        _________________\n",
    "\n",
    "    # Finally, at the end of the training loop, sync the target network with the value network\n",
    "    ____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the policy network\n",
    "\n",
    "For each batch of data, we perform a single update of the policy network. \n",
    "1. Loop through the batch of data. For each data point, compute the advantage estimate, $A_i = r_i + \\gamma(1 - d_i)V(s'_i;\\phi) - V(s_i;\\phi)$\n",
    "2. Compute the policy objective, $$J(\\theta) = \\frac{1}{N} \\sum_i A_i \\log \\pi(a_i|s_i;\\theta)$$\n",
    "3. Perform a gradient ascent step on the policy objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_network(batch: List[Dict], agent: A2CAgent):\n",
    "    # Create the policy objective\n",
    "    J = 0\n",
    "    # Iterate through the batch and form the regression targets using the TD error\n",
    "    for point in batch:\n",
    "        state = torch.tensor(point[\"state\"], dtype=torch.float32)\n",
    "        action = point[\"action\"]\n",
    "        reward = torch.tensor(point[\"reward\"], dtype=torch.float32)\n",
    "        next_state = torch.tensor(point[\"next_state\"], dtype=torch.float32)\n",
    "        terminated = point[\"terminated\"]\n",
    "        # Enter no gradient mode:\n",
    "        ___________:\n",
    "            # Compute the next state values using the target network\n",
    "            next_state_value = _________________\n",
    "            # Zero out the next state values for the terminal states\n",
    "            next_state_value[terminated] = 0\n",
    "            # Compute the advantage\n",
    "            advantage = ____________________\n",
    "        \n",
    "        # Compute the log probability of the action\n",
    "        log_prob = torch.log(________)[_____]\n",
    "        # Add advantage times log probability to the policy objective\n",
    "        J += _______*______\n",
    "    \n",
    "    # Divide J by the number of datapoints in the batch\n",
    "    J = J/len(batch)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    ___________\n",
    "    # Compute the gradients\n",
    "    ___________\n",
    "    # Take a step with the optimiser\n",
    "    ___________        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the training loop\n",
    "\n",
    "Finally, we implement the training loop. \n",
    "1. Interact with the environment for a number of steps, getting a batch of data\n",
    "2. Train the value network\n",
    "3. Train the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    agent: A2CAgent, env: gym.Env, num_epochs: int, steps_per_epoch: int = 1000\n",
    ") -> List[float]:\n",
    "    # Create a list to store all the rewards\n",
    "    all_returns = []\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Interact with the environment\n",
    "        batch, return_list = _____________\n",
    "        # Train the value network\n",
    "        ______________(batch, agent, num_iterations=100)\n",
    "        # Train the policy network\n",
    "        ______________(batch, agent)\n",
    "        # Store the rewards\n",
    "        all_returns.extend(return_list)\n",
    "    return all_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "We define some helper functions to:\n",
    "1. Evaluate the agent's performance\n",
    "2. Visualise the agent's performance\n",
    "3. Plot the (smoothed) returns from training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the returns\n",
    "def plot_returns(return_list):\n",
    "    plt.xlabel(\"Episode number\")\n",
    "    plt.plot(return_list)\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.xlim(0, len(return_list))\n",
    "    plt.ylim(0.9 * min(return_list), 1.1 * max(return_list))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate function\n",
    "def evaluate(agent: A2CAgent, env: gym.Env, n_episodes: int) -> float:\n",
    "    # Initialise the list of rewards\n",
    "    returns = []\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        # Get the initial state\n",
    "        state, _ = env.reset()\n",
    "        # Initialise the episode reward\n",
    "        episode_return = 0\n",
    "\n",
    "        # Loop over the steps\n",
    "        while True:\n",
    "            # Choose the action according to the policy\n",
    "            action = agent.sample_action(state)\n",
    "            # Take the action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            # Update the state and reward\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "            # Break if the episode has terminated\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Append the episode reward to the list of rewards\n",
    "        returns.append(episode_return)\n",
    "    # Return the mean of the rewards\n",
    "    return np.mean(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the visualise function\n",
    "# This displays the agent's behaviour in the environment for n_steps.\n",
    "def visualise(agent: A2CAgent, env: gym.Env, n_steps: int):\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Initialise the list of frames\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Render the environment and store the frame\n",
    "        frames.append(env.render())\n",
    "\n",
    "        # Take an action\n",
    "        action = agent.sample_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "        # Check if the episode is over\n",
    "        if terminated or truncated:\n",
    "            state, _ = env.reset()\n",
    "\n",
    "    # Display the movie\n",
    "    for frame in frames:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        sleep(0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's gooooooooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "# Create the agent\n",
    "agent = A2CAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance before training\n",
    "print(\"Performance before training:\", evaluate(agent, env, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour\n",
    "visualise(agent, env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_list = train_loop(agent, env, num_epochs=150, steps_per_epoch=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_returns(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance before training\n",
    "print(\"Performance before training:\", evaluate(agent, env, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour\n",
    "visualise(agent, env, 500)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
