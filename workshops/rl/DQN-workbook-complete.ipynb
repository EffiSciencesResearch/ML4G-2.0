{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cartpole environment\n",
    "\n",
    "Cartpole is a classic RL control task. A cart lies on a track and has a pole attached to it which can freely rotate. At each time step, the cart may choose to move left or right. The goal is to keep the pole upright for as long as possible. \n",
    "\n",
    "### Termination and truncations conditions\n",
    "The environment terminates if the pole angle is more than 12 degrees from vertical, or if the cart position is more than 2.4 units from the center.\n",
    "The environment is truncated after 500 time steps, if the pole is still upright.\n",
    "\n",
    "### Rewards\n",
    "The agent recieves a reward of +1 at each time step, as long as the pole is upright.\n",
    "\n",
    "### Observations\n",
    "At each time point, the RL agent recieves the following observations:\n",
    "1. Cart position\n",
    "2. Cart velocity \n",
    "3. Pole angle from vertical\n",
    "4. Pole anglular velocity \n",
    "\n",
    "### Action space\n",
    "The agent can take one of two actions at each time step:\n",
    "1. Move the cart to the left\n",
    "2. Move the cart to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN algorithm components\n",
    "\n",
    "The DQN algorithm requires the following components, with the following roles:\n",
    "1. **Value network**: A neural network that takes in states and outputs predictions for the value of each action in that state\n",
    "2. **Target network**: A copy of the value network with lagged parameters that is used to compute the target values for the regression targets\n",
    "3. **Replay buffer**: A buffer that stores transitions (state, action, reward, next_state, terminated) that the agent has experienced. This is used to sample mini-batches of transitions to train the value network\n",
    "4. **Policies**: These use the value network to select actions in the environment. We will consider two policies:\n",
    "    - **Greedy policy**: This policy selects the action with the highest value, and is used to evaluate the value network\n",
    "    - **$\\epsilon$-greedy policy**: This policy selects a random action with probability $\\epsilon$, and the action with the highest value with probability 1-$\\epsilon$. This is used to explore the environment using the training phase. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The value and target networks\n",
    "\n",
    "We start by defining the value network architecture. This is a simple feedforward neural network (a **multi-layer perceptron**). The input to the network is the state of the environment, and the output is the value prediction *of each action*. The target network is a copy of the value network, with lagged parameters. \n",
    "\n",
    "### Architecture recommendations\n",
    "* Use two hidden layers with 128 units each\n",
    "* Use ReLU activation functions\n",
    "* Use a softmax activation function in the output layer - for this environment, all values are positive, so we can use a softmax activation function to ensure that the values are positive. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the QNet class\n",
    "class QNet(nn.Module):\n",
    "    # Initialise the network using the size of the observation space and the number of actions\n",
    "    def __init__(self, env: gym.Env):\n",
    "        # Use the nn.Module's __init__ method to ensure that the parameters can be updated during training\n",
    "        super().__init__()\n",
    "        # Store the size of the observation space and the number of actions\n",
    "        self.obs_size = env.observation_space.shape[0]\n",
    "        self.n_actions = env.action_space.n\n",
    "\n",
    "        # Define the layers of the network\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.obs_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.n_actions),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    # Define the forward method\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The replay buffer\n",
    "\n",
    "To implement the replay buffer, we first define a named tuple data type to store transitions. We then define the replay buffer class, which stores transitions and can sample mini-batches of transitions. The replay buffer has the following methods:\n",
    "1. **Push**: This method stores a new transition in the buffer, and removes the oldest transition if the buffer is full\n",
    "2. **Sample**: This method samples a mini-batch of transitions from the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transition named tuple\n",
    "# This will be used to store the transitions (state, action, reward, next_state, terminated) in the replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
    "\n",
    "# Define the ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    # Initialise the buffer with a capacity. \n",
    "    def __init__(self, capacity:int):\n",
    "        # Use a deque object to implement the buffer\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    # Define the push method to add a transition to the buffer\n",
    "    def push(self, state:np.ndarray, action:int, reward:float, next_state:np.ndarray, terminated:bool):\n",
    "        new_transition = Transition(state, action, reward, next_state, terminated)\n",
    "        self.buffer.append(new_transition)\n",
    "\n",
    "    # Sample a mini_batch of transitions for training\n",
    "    def sample(self, mini_batch_size: int):\n",
    "        return random.sample(self.buffer, mini_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DQN agent\n",
    "We now define the DQN agent class. This class will have:\n",
    "1. **A value network**, implemented using the QNet class\n",
    "2. **A target network**, implemented using the QNet class\n",
    "3. **A replay buffer**, implemented using the ReplayBuffer class\n",
    "4. **An optimiser**, which is used to train the value network\n",
    "\n",
    "The DQN agent will have the following methods:\n",
    "1. **Greedy policy**: This method selects the action with the highest value\n",
    "2. **$\\epsilon$-greedy policy**: This method selects a random action with probability $\\epsilon$, and the action with the highest value with probability 1-$\\epsilon$\n",
    "3. **Sync**: This method synchronises the target network with the value network by loading the parameters of the value network into the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQNAgent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, env: gym.Env, buffer_capacity:int = 200000, gamma:float = 0.95, epsilon:float =0.05, lr:float = 0.003, mini_batch_size:int=32):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        \n",
    "        # Save the number of actions and the observation size\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.obs_size = env.observation_space.shape[0]\n",
    "\n",
    "        # Define the value network for the agent. \n",
    "        self.value_network = QNet(env)\n",
    "        # Define the target network for the agent.\n",
    "        self.target_network = QNet(env)\n",
    "        # Sync the networks\n",
    "        self.sync()\n",
    "        # Define the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        # Define the optimizer\n",
    "        self.optimiser = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # Define the sync method to sync the target network with the value network\n",
    "    def sync(self):\n",
    "        # First, retrieve the state dictionary of the value network\n",
    "        value_network_state_dict = self.value_network.state_dict()\n",
    "        # Load the state dictionary into the target network\n",
    "        self.target_network.load_state_dict(value_network_state_dict)\n",
    "\n",
    "\n",
    "    # Define the greedy policy\n",
    "    def greedy_policy(self, state) -> int:\n",
    "        # Enter no-gradient mode (since we are not training the network when we sample actions)\n",
    "        with torch.no_grad():\n",
    "            # Convert the state to a tensor\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            # Compute the Q values for the state using the value network\n",
    "            q_values = self.value_network(state)\n",
    "            # Find the action with the highest Q value, converting it to a integer\n",
    "            max_action = q_values.argmax().item()\n",
    "            # Return the action\n",
    "            return max_action \n",
    "           \n",
    "\n",
    "    # Define the epsilon greedy policy \n",
    "    def epsilon_greedy_policy(self, state) -> int:\n",
    "        # Sample a random number uniformly betwen 0 and 1  \n",
    "        rand_num = np.random.random()\n",
    "\n",
    "        # If the random number is less than the exploration rate, choose a random action\n",
    "        if rand_num < self.epsilon:\n",
    "            # Choose a random action\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "            # Return the action\n",
    "            return action\n",
    "        \n",
    "        # Otherwise, choose the action with the highest action-value\n",
    "        else:\n",
    "            # Use the greedy policy to choose the action\n",
    "            action = self.greedy_policy(state)\n",
    "            # Return the action\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the environment\n",
    "During training, the DQN agent must interact with the environment in order to collect transitions to be added to the replay buffer. We define an **interact** function that takes in the environment, the DQN agent, and the number of time steps to interact for. This function will use the $\\epsilon$-greedy policy to select actions, and will store the transitions in the replay buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interact method\n",
    "# This method takes in an agent, an environment, and the number of steps to interact for.\n",
    "# It returns the final state after the interaction.\n",
    "def interact(agent, env: gym.Env, current_state: np.ndarray, n_steps:int) -> np.ndarray:\n",
    "    # Loop over the steps\n",
    "    for _ in range(n_steps):\n",
    "        # Choose an action using the epsilon greedy policy\n",
    "        action = agent.epsilon_greedy_policy(current_state)\n",
    "        # Take a step in the environment using the action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Push the transition to the replay buffer\n",
    "        agent.replay_buffer.push(current_state, action, reward, next_state, terminated)\n",
    "        # Reset the environment if the episode is terminated or truncated\n",
    "        if terminated or truncated:\n",
    "            current_state, _= env.reset()\n",
    "        else: \n",
    "            # Update the current state\n",
    "            current_state = next_state\n",
    "    return current_state            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DQN agent\n",
    "\n",
    "We define a **update weights** function that updates the weights of the value network for a DQN agent. This function will:\n",
    "1. Sample a mini-batch of transitions from the replay buffer\n",
    "2. Extracts the states, actions, rewards, next_states, and terminated flags from the transitions\n",
    "3. Uses the target network, rewards, and terminated flags to compute the target values for the value network \n",
    "4. Computes the loss between the value network predictions and the target values\n",
    "5. Runs backpropagation and updates the weights of the value network using that loss\n",
    "\n",
    "As a reminder, the regression targets for $Q(s,a)$ are given by:\n",
    "$$ y = r + \\gamma (1-d) \\max_{a'} Q(s',a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method updates the weights of the value-network of the agent using a mini-batch of transitions\n",
    "def update_weights(agent: DQNAgent):\n",
    "    # Sample a mini-batch of transitions from the replay buffer\n",
    "    mini_batch = agent.replay_buffer.sample(agent.mini_batch_size)\n",
    "    \n",
    "    # Extract the mini-batch of states as float32 tensors \n",
    "    states = torch.tensor([transition.state for transition in mini_batch], dtype=torch.float32)\n",
    "    # Extract the mini-batch of actions as int64 tensors\n",
    "    actions = torch.tensor([transition.action for transition in mini_batch], dtype=torch.int64)\n",
    "    # Extract the mini-batch of rewards as float32 tensors\n",
    "    rewards = torch.tensor([transition.reward for transition in mini_batch], dtype=torch.float32)\n",
    "    # Extract the mini-batch of next states as float32 tensors\n",
    "    next_states = torch.tensor([transition.next_state for transition in mini_batch], dtype=torch.float32)\n",
    "    # Extract the mini-batch of terminated flags as bool tensors\n",
    "    terminated = torch.tensor([transition.terminated for transition in mini_batch], dtype=torch.bool)\n",
    "\n",
    "    # Enter no-gradient mode \n",
    "    with torch.no_grad():\n",
    "        # Compute the next state values using the target network\n",
    "        next_state_values = agent.target_network(next_states)\n",
    "        # Take the max over the actions (dim=1) \n",
    "        max_next_values = next_state_values.max(dim=1)[0] \n",
    "        # Zero out the max-next-values for the terminal states\n",
    "        max_next_values[terminated] = 0\n",
    "        # Compute the regression targets\n",
    "        regression_targets = rewards + agent.gamma * max_next_values\n",
    "\n",
    "    # Compute the Q values for all actions\n",
    "    q_values = agent.value_network(states)\n",
    "    # Compute the Q values for the actions that were taken\n",
    "    q_SA = torch.zeros(agent.mini_batch_size)\n",
    "    for i in range(agent.mini_batch_size):\n",
    "        q_SA[i] = q_values[i, actions[i]]\n",
    "    # Compute the loss using the mean squared error\n",
    "    loss = F.mse_loss(q_SA, regression_targets)\n",
    "    \n",
    "    # Zero the gradients using the optimiser\n",
    "    agent.optimiser.zero_grad()\n",
    "    # Compute the gradients of the loss through backpropagation\n",
    "    loss.backward()\n",
    "    # Take a step with the optimiser\n",
    "    agent.optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop for the agent\n",
    "We now put everything together into a training loop for the DQN agent. This loop will:\n",
    "1. Interact with the environment for a number of time steps\n",
    "2. Update the weights of the value network\n",
    "3. Synchronize the target network with the value network at regular intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_loop method\n",
    "def train_loop(agent: DQNAgent, env: gym.Env, interactions_per_update:int, update_steps:int, sync_delay:int):\n",
    "    # Initialise the environment\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Gather initial interactions for the experience replay buffer\n",
    "    state = interact(agent, env, state, 10000)\n",
    "    \n",
    "    # Loop over the update_steps\n",
    "    for step in tqdm(range(update_steps)):\n",
    "        # Interact with the environment\n",
    "        state = interact(agent, env, state, interactions_per_update)\n",
    "        \n",
    "        # Update the weights of the value network\n",
    "        update_weights(agent)\n",
    "\n",
    "        # Sync the networks every sync_delay steps\n",
    "        if step % sync_delay == 0:\n",
    "            agent.sync()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "We define some helper functions to:\n",
    "1. Evaluate the agent's performance\n",
    "2. Visualise the agent's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate function\n",
    "def evaluate(agent: DQNAgent, env: gym.Env, n_episodes: int) -> float:\n",
    "    # Initialise the list of rewards\n",
    "    returns = []\n",
    "    \n",
    "    # Loop over the episodes\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        # Get the initial state\n",
    "        state, _ = env.reset()\n",
    "        # Initialise the episode reward\n",
    "        episode_return = 0\n",
    "        \n",
    "        # Loop over the steps\n",
    "        while True:\n",
    "            # Choose the action with the highest Q value\n",
    "            action = agent.greedy_policy(state)\n",
    "            # Take the action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            # Update the state and reward\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "            # Break if the episode has terminated\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Append the episode reward to the list of rewards\n",
    "        returns.append(episode_return)\n",
    "    # Return the mean of the rewards\n",
    "    return np.mean(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the visualise function\n",
    "# This displays the agent's behaviour in the environment.  \n",
    "def visualise(agent: DQNAgent, env: gym.Env, n_steps: int):\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Initialise the list of frames   \n",
    "    frames = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Render the environment and store the frame\n",
    "        frames.append(env.render())\n",
    "\n",
    "        # Take an action using the greedy policy\n",
    "        action = agent.greedy_policy(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Display the movie\n",
    "    for frame in frames:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        sleep(0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's gooooo\n",
    "\n",
    "We will now train our network using the DQN algorithm and visualise the agent's performance. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "# Create the agent\n",
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance before training\n",
    "print('Performance before training:', evaluate(agent, env, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour before training\n",
    "visualise(agent, env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "train_loop(agent, env, interactions_per_update=8, update_steps=40000, sync_delay=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's performance after training\n",
    "print('Performance after training:', evaluate(agent, env, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the agent's behaviour after training\n",
    "visualise(agent, env, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
