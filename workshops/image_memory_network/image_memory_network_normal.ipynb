{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Only execute on Colab\n",
                "try:\n",
                "    import google.colab\n",
                "\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    # Install packages\n",
                "    %pip install einops\n",
                "\n",
                "    # Code to make sure output widgets display\n",
                "    from google.colab import output\n",
                "\n",
                "    output.enable_custom_widget_manager()\n",
                "\n",
                "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
                "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/image_memory_network/*'\n",
                "    !mv --no-clobber ML4G-2.0-master/workshops/image_memory_network/* .\n",
                "    !rm -r ML4G-2.0-master\n",
                "\n",
                "    print(\"Imports & installations complete!\")\n",
                "\n",
                "else:\n",
                "    from IPython import get_ipython\n",
                "\n",
                "    ipython = get_ipython()\n",
                "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
                "    ipython.run_line_magic(\"autoreload\", \"2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/image_memory_network/image_memory_network_normal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "# Learning To Reproduce a Picture\n",
                "\n",
                "In this exercise you will train a neural network to memorize a picture of your choice! Your network will implement a function from the $(x, y)$ coordinates of a pixel to three numbers $(R, G, B)$ representing the color of that pixel. Implement the `ImageMemorizer` network with three Linear layers and two ReLUs (generally, you don't want a ReLU after the last Linear layer). Test that your model matches the reference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "from torch.functional import F\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "from torchvision import transforms\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "import einops\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "import image_memoriser_tests as tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImageMemorizer(nn.Module):\n",
                "    \"\"\"A simple MLP that takes the coordinates (x, y) of a pixel and outputs the pixel's RGB color.\"\"\"\n",
                "\n",
                "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
                "        # TODO: define the network's architecture.\n",
                "        # Feel free to go back to the Pytorch basics' https://pytorch.org/tutorials/beginner/basics/intro.html\n",
                "        ...  # TODO: ~22 words\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        # TODO: implement the forward pass of the network\n",
                "        ...  # TODO: ~4 words\n",
                "\n",
                "\n",
                "tests.test_mlp(ImageMemorizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "class ImageMemorizer(nn.Module):\n",
                "    \"\"\"A simple MLP that takes the coordinates (x, y) of a pixel and outputs the pixel's RGB color.\"\"\"\n",
                "\n",
                "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
                "        # TODO: define the network's architecture.\n",
                "        # Feel free to go back to the Pytorch basics' https://pytorch.org/tutorials/beginner/basics/intro.html\n",
                "        super().__init__()\n",
                "        self.layers = nn.Sequential(\n",
                "            nn.Linear(in_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(hidden_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(hidden_dim, out_dim),\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        # TODO: implement the forward pass of the network\n",
                "        return self.layers(x)\n",
                "\n",
                "\n",
                "tests.test_mlp(ImageMemorizer)",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Choose a picture and save it on the filesystem, or use the provided image. If your chosen image is much larger than 1 million pixels, crop it with `img.crop((left, top, right, bottom))` and/or resize it with `img.resize((width, height))`.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "image_path = Path(\".\") / \"w1d4_vangogh.jpg\"\n",
                "img = Image.open(image_path)\n",
                "print(f\"Image size in pixels: {img.size[0]} x {img.size[1]} = {img.size[0] * img.size[1]}\")\n",
                "plt.imshow(img);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Data Preprocessing\n",
                "\n",
                "Most of the work in training a neural network is getting the data in top condition first. The relevant saying is \"garbage in, garbage out\".\n",
                "\n",
                "The `preprocess_image` function does the following:\n",
                "\n",
                "- Use `transforms.ToTensor()(img)` to obtain a tensor of shape `(channels, height, width)`.\n",
                "- Remove the fourth (alpha) channel if present and just use the first three channels which are R, G, B values.\n",
                "- Build a tensor of all combinations of `(x, y)` from `(0, 0)` up to `(height, width)`. Then, scale these coordinates down to the range `[-1, 1]`. These will be the inputs to your model. Without scaling them down, the training would either be very slow or not work at all.\n",
                "- Build a tensor of the corresponding RGB values and scale each color to the range `[-1, 1]`. These will be the labels.\n",
                "- Return the inputs and labels wrapped in a `TensorDataset`. \n",
                "\n",
                "### TensorDataset\n",
                "The class `torch.utils.data.dataset.TensorDataset` is a convenient wrapper for passing around multiple tensors that have the same size in the first dimension. The most common example of this is in supervised learning, where you have one tensor of inputs and a second tensor with corresponding labels. Often these tensors will have different `dtype`s, so it doesn't make sense to `torch.stack` them into one big tensor, and it be cumbersome to pass them around as separate variables or as a tuple.\n",
                "\n",
                "`TensorDataset` accepts and stores any number of tensors in the constructor along with implementing `__getitem__` so that `my_dataset[n]` returns a tuple containing element `n` from each stored `Tensor`. Similarly, `my_dataset[:5]` returns a tuple containing the first five elements from each stored `Tensor`.\n",
                "\n",
                "There's a bonus exercise at the end of this notebook to re-implement TensorDataset from scratch.\n",
                "\n",
                "Execute the following cell, and make sure you understand the input and output of those functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def all_coordinates_scaled(height: int, width: int) -> torch.Tensor:\n",
                "    \"\"\"Return a tensor of shape (height*width, 2) where each row is a (x, y) coordinate.\n",
                "\n",
                "    The range of x and y should be from [-1, 1] in both height and width dimensions.\n",
                "    \"\"\"\n",
                "    xs = einops.repeat(torch.arange(width, dtype=torch.float32), \"w -> (h w)\", h=height) / width\n",
                "    ys = einops.repeat(torch.arange(height, dtype=torch.float32), \"h -> (h w)\", w=width) / height\n",
                "    return torch.stack((xs, ys), dim=1) * 2.0 - 1.0\n",
                "\n",
                "\n",
                "def preprocess_image(img: Image.Image) -> TensorDataset:\n",
                "    \"\"\"Convert an image into a supervised learning problem predicting (R, G, B) given (x, y).\n",
                "\n",
                "    Return: TensorDataset wrapping input and label tensors.\n",
                "    input: shape (num_pixels, 2)\n",
                "    label: shape (num_pixels, 3)\n",
                "    \"\"\"\n",
                "    img_t = transforms.ToTensor()(img)[:3, :, :]\n",
                "    _, height, width = img_t.shape\n",
                "    X = all_coordinates_scaled(height, width)\n",
                "    labels = einops.rearrange(img_t, \"c h w -> (h w) c\") * 2.0 - 1.0\n",
                "    return TensorDataset(X, labels)\n",
                "\n",
                "\n",
                "all_data = preprocess_image(img)\n",
                "\n",
                "print(\"All coordinates:\", all_coordinates_scaled(3, 4))\n",
                "print(\"TensorDataset:\", all_data, all_data.tensors)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Train-Test Split\n",
                "\n",
                "Next, we will randomly split the data into \n",
                "1. a training set that the model will use for computing gradients,\n",
                "2. a validation set that will be used later for choosing hyperparameters, and \n",
                "3. a held-out test set that will tell us how well the model is generalizing. For validation and test statistics to be a reliable measure of generalization, it is necessary for the training set to not overlap with the validation or test sets.\n",
                "\n",
                "This was relatively straightforward in the era of small datasets that could be thoroughly inspected by humans, but is increasingly an issue as models are trained on massive piles of haphazardly cleaned Internet data. When reading ML papers, it's important to evaluate the potential for \"leakage\" between sets.\n",
                "\n",
                "You'll see rules of thumb online about how much of your data to use for training/validation/test sets, such as a \"80%/10%/10% split\". In deep learning, these are generally wrong. The size of the validation and test sets only need to be big enough that sampling error doesn't introduce too much noise into the resulting estimate.\n",
                "\n",
                "For example, ImageNet has around 1.3 million training images and only 50K validation images. The percentage (under 4%) is irrelevant and what matters is that 50K is large enough in absolute terms to achieve some standard error of the mean. Implement `train_test_split` below to split the dataset as described.\n",
                "\n",
                "Hint: use [`torch.randperm`](https://pytorch.org/docs/stable/generated/torch.randperm.html).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_test_split(\n",
                "    all_data: TensorDataset, train_frac=0.8, val_frac=0.01, test_frac=0.01\n",
                ") -> list[TensorDataset]:\n",
                "    \"\"\"Return [train, val, test] datasets containing the specified fraction of examples.\n",
                "\n",
                "    If the fractions add up to less than 1, some of the data is not used.\n",
                "    \"\"\"\n",
                "\n",
                "    ...  # TODO: ~34 words\n",
                "\n",
                "\n",
                "all_data = preprocess_image(img)\n",
                "train_data, val_data, test_data = train_test_split(all_data)\n",
                "# If you used the default image, this should print\n",
                "# 106396, 1329 and 1329 (±1 depending on how you rounded the fractions)\n",
                "print(f\"Dataset sizes: train {len(train_data)}, val {len(val_data)} test {len(test_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "def train_test_split(\n",
                "    all_data: TensorDataset, train_frac=0.8, val_frac=0.01, test_frac=0.01\n",
                ") -> list[TensorDataset]:\n",
                "    \"\"\"Return [train, val, test] datasets containing the specified fraction of examples.\n",
                "\n",
                "    If the fractions add up to less than 1, some of the data is not used.\n",
                "    \"\"\"\n",
                "\n",
                "    n = len(all_data)\n",
                "    perm = torch.randperm(n)\n",
                "    start = 0\n",
                "    out = []\n",
                "    for frac in [train_frac, val_frac, test_frac]:\n",
                "        split_size = int(n * frac)\n",
                "        idx = perm[start : start + split_size]\n",
                "        out.append(TensorDataset(*all_data[idx]))\n",
                "        start += split_size\n",
                "    return out\n",
                "\n",
                "\n",
                "all_data = preprocess_image(img)\n",
                "train_data, val_data, test_data = train_test_split(all_data)\n",
                "# If you used the default image, this should print\n",
                "# 106396, 1329 and 1329 (±1 depending on how you rounded the fractions)\n",
                "print(f\"Dataset sizes: train {len(train_data)}, val {len(val_data)} test {len(test_data)}\")",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### Visualizing the Training Data\n",
                "\n",
                "Many times, I've made errors in the preprocessing step and not noticed because my model still trains and learns anyway, just at a lower accuracy than was possible. One way to reduce the chance of this happening is to inspect the preprocessed data carefully to see if it still makes sense.\n",
                "\n",
                "We make a zero tensor of shape `(height, width, 3)` representing the grid of pixels, that we display with `plt.imshow`.\n",
                "\n",
                "Just execute the following cell:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def to_grid(X: torch.Tensor, Y: torch.Tensor, width: int, height: int) -> torch.Tensor:\n",
                "    \"\"\"Convert preprocessed data from the format used in the Dataset back to an image tensor.\n",
                "\n",
                "    X: shape (n_pixels, dim=2)\n",
                "    Y: shape (n_pixels, channel=3)\n",
                "\n",
                "    Return: shape (height, width, channels=3)\n",
                "    \"\"\"\n",
                "    X = ((X + 1.0) / 2.0 * torch.tensor([width, height]) + 0.5).long()\n",
                "    x_coords = X[:, 0]\n",
                "    y_coords = X[:, 1]\n",
                "    Y = (Y + 1.0) / 2.0\n",
                "    grid = torch.zeros((height, width, 3))\n",
                "    grid[y_coords, x_coords] = Y\n",
                "    return grid\n",
                "\n",
                "\n",
                "width, height = img.size\n",
                "X, Y = train_data.tensors\n",
                "plt.figure()\n",
                "plt.imshow(to_grid(X, Y, width, height));"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary> What are the black pixels in the image? </summary>\n",
                "\n",
                "The black pixels are the pixels that are not in the train dataset. They are the pixels that will be used for validation and test... or won't be used at all because our fractions did not add up to 1.0.\n",
                "\n",
                "You can also visualize the validation and test pixels.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## DataLoaders\n",
                "\n",
                "Today, our `Dataset` is small enough to fit in memory, so we could just use `torch.randperm` on our training set to fetch random batches from it.\n",
                "\n",
                "In general, we only want to load parts of our dataset as they're needed because our dataset may be too large to fit in memory, it may take too long to preprocess the entire dataset, or we may just want the GPU to be active as much as possible instead of waiting for data to be ready.\n",
                "\n",
                "This is where `torch.DataLoader` comes in. A `DataLoader` instance is responsible for spawning multiple worker processes which load data in parallel and communicate back to the `DataLoader`. Ideally, the `DataLoader` can prepare the next batch while the GPU is processing the current one, eliminating GPU downtime.\n",
                "\n",
                "We'll implement our own version of this another day when we're dealing with parallelism, and just use the PyTorch implementation today. We've provided DataLoaders with `shuffle=True` for the train loader. What would happen if you didn't shuffle the training data?\n",
                "\n",
                "<details>\n",
                "<summary>Answer - Shuffling Training Data</summary>\n",
                "\n",
                "If our training data was sorted and we didn't shuffle it at least once, then the learning process could oscillate instead of converging. Suppose that the top half of the image was mostly blue sky and the bottom half was mostly green grass. The model would get gradients that first suggest \"everything is mostly blue\" and later \"everything is mostly green\" successively. In this case, we already used `randperm` above so our training data has been shuffled regardless.\n",
                "\n",
                "In practice, SGD is relatively insensitive to whether you shuffle on every epoch, just once, or even sample each minibatch with replacement from the full dataset. For some theory behind this, see [this paper](https://arxiv.org/pdf/2106.06880.pdf).\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
                "val_loader = DataLoader(val_data, batch_size=256)\n",
                "test_loader = DataLoader(test_data, batch_size=256)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Implement the `train_one_epoch` function below.\n",
                "\n",
                "- Use the `to()` method of a `Tensor` to send the data to the device indicated by the global variable `device`.\n",
                "- You can convert a one-element tensor to a regular Python number using the `item` method.\n",
                "\n",
                "<details>\n",
                "\n",
                "<summary>It's not working and I'm confused!</summary>\n",
                "\n",
                "- Did you remember to call `optimizer.zero_grad()` before each forward pass?\n",
                "- Does `model.parameters()` return what you expect?\n",
                "- Are you calling `backward()` on the mean loss over the batch items? Note that if you don't use the mean, the magnitude of the gradients scales up linearly with the batch size, which is not what you want.\n",
                "\n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model: ImageMemorizer, dataloader: DataLoader) -> float:\n",
                "    \"\"\"Show each example in the dataloader to the model once.\n",
                "\n",
                "    Use `torch.optim.Adam` for the optimizer.\n",
                "    Use `F.l1_loss(prediction, actual)` for the loss function. This just puts less weight on very bright or dark pixels, which seems to produce nicer images.\n",
                "\n",
                "    Return: the average loss per example seen, i.e. sum of losses of each batch weighted by the size of the batch, divided by the total number of examples seen\n",
                "    \"\"\"\n",
                "\n",
                "    ...  # TODO: ~56 words\n",
                "\n",
                "\n",
                "tests.test_train(train_one_epoch)\n",
                "\n",
                "\n",
                "def evaluate(model: ImageMemorizer, dataloader: DataLoader) -> float:\n",
                "    \"\"\"Return the total L1 loss over the provided data divided by the number of examples.\"\"\"\n",
                "    ...  # TODO: ~42 words\n",
                "\n",
                "\n",
                "tests.test_evaluate(evaluate)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "def train_one_epoch(model: ImageMemorizer, dataloader: DataLoader) -> float:\n",
                "    \"\"\"Show each example in the dataloader to the model once.\n",
                "\n",
                "    Use `torch.optim.Adam` for the optimizer.\n",
                "    Use `F.l1_loss(prediction, actual)` for the loss function. This just puts less weight on very bright or dark pixels, which seems to produce nicer images.\n",
                "\n",
                "    Return: the average loss per example seen, i.e. sum of losses of each batch weighted by the size of the batch, divided by the total number of examples seen\n",
                "    \"\"\"\n",
                "\n",
                "    model.train()  # Does nothing on this particular model, but good practice to have it\n",
                "    optim = torch.optim.Adam(model.parameters())\n",
                "    loss_sum = 0.0\n",
                "    datapoint_seen = 0\n",
                "    for X, y in tqdm(dataloader):\n",
                "        optim.zero_grad()\n",
                "        pred = model(X)\n",
                "        loss = F.l1_loss(pred, y)\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "        datapoint_seen += len(X)\n",
                "        loss_sum += loss.item() * len(X)\n",
                "    return loss_sum / datapoint_seen\n",
                "\n",
                "\n",
                "tests.test_train(train_one_epoch)\n",
                "\n",
                "\n",
                "def evaluate(model: ImageMemorizer, dataloader: DataLoader) -> float:\n",
                "    \"\"\"Return the total L1 loss over the provided data divided by the number of examples.\"\"\"\n",
                "    with torch.inference_mode():\n",
                "        model.eval()  # Does nothing on this particular model, but good practice to have it\n",
                "        loss_sum = 0.0\n",
                "        datapoint_seen = 0\n",
                "        for X, y in dataloader:\n",
                "            loss_sum += F.l1_loss(model(X), y).item() * len(X)\n",
                "            datapoint_seen += len(X)\n",
                "        return loss_sum / datapoint_seen\n",
                "\n",
                "\n",
                "tests.test_evaluate(evaluate)",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "The following cell creates a model with 400 neurons in each hidden layer and trains it for an epoch.\n",
                "\n",
                "If no errors appeared, do a few more epochs and plot the training loss and validation loss over time as a function of number of epochs. Compute the validation loss using your `evaluate` function. I was able to reach a validation loss below 0.2 after 40 epochs. Your image might be easier or harder to learn.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = ImageMemorizer(2, 400, 3)\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "\n",
                "num_epochs = 1  # Increase this a bit if/once it works\n",
                "bar = tqdm(range(num_epochs))\n",
                "for epoch in bar:\n",
                "    train_losses.append(train_one_epoch(model, train_loader))\n",
                "    val_loss = evaluate(model, val_loader)\n",
                "    bar.set_description(f\"val loss: {val_loss:.3f}\")\n",
                "    bar.refresh()\n",
                "    val_losses.append(val_loss)\n",
                "\n",
                "fig, ax = plt.subplots()\n",
                "ax.plot(train_losses, label=\"Training loss\")\n",
                "ax.plot(val_losses, label=\"Validation loss\")\n",
                "ax.set(xlabel=\"Epochs\", ylabel=\"L1 Loss\")\n",
                "fig.legend();"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "Finally, execute this cell to display the image your network has memorized:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = all_coordinates_scaled(height, width)\n",
                "with torch.inference_mode():\n",
                "    Y = model(X).cpu()\n",
                "grid = to_grid(X, Y, width, height)\n",
                "grid.clip_(0, 1)\n",
                "fig, ax = plt.subplots(figsize=(12, 12))\n",
                "ax.imshow(grid)\n",
                "ax.get_xaxis().set_visible(False)\n",
                "ax.get_yaxis().set_visible(False)\n",
                "\n",
                "# ax.set_position([0, 0, 1, 1])\n",
                "# fig.savefig(\"w1d4_vangogh_solution.jpg\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Share your image with your friends if you like it! Here's the one my network learned:\n",
                "\n",
                "![Alt text](https://github.com/EffiSciencesResearch/ML4G/blob/main/mlab/w1d4_vangogh_solution.jpg?raw=true \"a title\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Bonus: Re-implement more tools from the pytorch library\n",
                "\n",
                "## Build Your Own TensorDataset\n",
                "\n",
                "The class `torch.utils.data.dataset.TensorDataset` is a convenient wrapper for passing around multiple tensors that have the same size in the first dimension. The most common example of this is in supervised learning, where you have one tensor of inputs and a second tensor with corresponding labels. Often these tensors will have different `dtype`s, so it doesn't make sense to `torch.stack` them into one big tensor, and it be cumbersome to pass them around as separate variables or as a tuple.\n",
                "\n",
                "`TensorDataset` accepts and stores any number of tensors in the constructor along with implementing `__getitem__` so that `my_dataset[n]` returns a tuple containing element `n` from each stored `Tensor`. Similarly, `my_dataset[:5]` returns a tuple containing the first five elements from each stored `Tensor`.\n",
                "\n",
                "### Slice Objects in Python\n",
                "\n",
                "`slice` is a built-in type containing `start`, `stop`, and `step` fields which can be integers or `None`. Given `x=[1,2,3,4,5,6,7]`, writing `x[1:5:2]` is syntactic sugar for `x[slice(1, 5, 2)]`.\n",
                "\n",
                "### Dunder (Magic) Methods in Python\n",
                "\n",
                "`__getitem__` is an example of a \"dunder\" or \"magic\" method in Python. These are methods that are called implicitly by Python in certain situations. For example, `x + y` is syntactic sugar for `x.__add__(y)`. `__getitem__` is called when you write `x[y]` and `__len__` is called when you write `len(x)`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TensorDataset:\n",
                "    def __init__(self, *tensors: torch.Tensor):\n",
                "        \"\"\"Validate the sizes and store the tensors in a field named `tensors`.\"\"\"\n",
                "        ...  # TODO: ~24 words\n",
                "\n",
                "    def __getitem__(self, index: int | slice) -> tuple[torch.Tensor, ...]:\n",
                "        \"\"\"Return a tuple of length len(self.tensors) with the index applied to each.\"\"\"\n",
                "        ...  # TODO: ~9 words\n",
                "\n",
                "    def __len__(self):\n",
                "        \"\"\"Return the size in the first dimension, common to all the tensors.\"\"\"\n",
                "        ...  # TODO: ~6 words\n",
                "\n",
                "\n",
                "# if MAIN:\n",
                "# w1d4_part1_test.test_tensor_dataset(TensorDataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Show solution</summary>\n",
                "\n",
                "```python\n",
                "class TensorDataset:\n",
                "    def __init__(self, *tensors: torch.Tensor):\n",
                "        \"\"\"Validate the sizes and store the tensors in a field named `tensors`.\"\"\"\n",
                "        if tensors:\n",
                "            size = tensors[0].shape[0]\n",
                "            assert all(\n",
                "                tensor.shape[0] == size for tensor in tensors\n",
                "            ), \"Size mismatch between tensors\"\n",
                "        self.tensors = tensors\n",
                "\n",
                "    def __getitem__(self, index: int | slice) -> tuple[torch.Tensor, ...]:\n",
                "        \"\"\"Return a tuple of length len(self.tensors) with the index applied to each.\"\"\"\n",
                "        return tuple(tensor[index] for tensor in self.tensors)\n",
                "\n",
                "    def __len__(self):\n",
                "        \"\"\"Return the size in the first dimension, common to all the tensors.\"\"\"\n",
                "        return self.tensors[0].shape[0]\n",
                "\n",
                "\n",
                "# if MAIN:\n",
                "# w1d4_part1_test.test_tensor_dataset(TensorDataset)",
                "\n",
                "```\n",
                "\n",
                "</details>\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
