{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Only execute on Colab\n",
                "try:\n",
                "    import google.colab\n",
                "\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    # Install packages\n",
                "    %pip install einops jaxtyping typeguard\n",
                "\n",
                "    # Code to make sure output widgets display\n",
                "    from google.colab import output\n",
                "\n",
                "    output.enable_custom_widget_manager()\n",
                "\n",
                "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
                "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/optimizers/*'\n",
                "    !mv --no-clobber ML4G-2.0-master/workshops/optimizers/* .\n",
                "    !rm -r ML4G-2.0-master\n",
                "\n",
                "    print(\"Imports & installations complete!\")\n",
                "\n",
                "else:\n",
                "\n",
                "    from IPython import get_ipython\n",
                "\n",
                "    ipython = get_ipython()\n",
                "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
                "    ipython.run_line_magic(\"autoreload\", \"2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/optimizers/optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "# Optimization\n",
                "\n",
                "Today's material focuses on understanding the standard methods of optimisation in machine learning. You're going to learn about the training loop and different optimizers.\n",
                "\n",
                "## Gradient Descent\n",
                "\n",
                "In the preparation work, you have seen how backpropagation works. Today, we're going to use the gradients produced by backpropagation for optimizing a loss function using gradient descent.\n",
                "\n",
                "A loss function can be any differentiable function such that we prefer a lower value. To apply gradient descent, we start by initializing the parameters to random values (the details of this are subtle), and then repeatedly compute the gradient of the loss with respect to the model parameters. It [can be proven](https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx) that for an infinitesimal step, moving in the direction of the gradient would increase the loss by the largest amount out of all possible directions.\n",
                "\n",
                "We actually want to decrease the loss, so we subtract the gradient to go in the opposite direction. Taking infinitesimal steps is no good, so we pick some learning rate $\\alpha$ (also called the step size) and scale our step by that amount to obtain the update rule for gradient descent:\n",
                "\n",
                "$$\\theta_{t+1} \\leftarrow \\theta_t - \\alpha \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
                "\n",
                "\n",
                "## Hyperparameters\n",
                "\n",
                "The learning rate is an example of a **hyperparameter**, which will be described below. As a reminder, a regular parameter is an adjustable value with the special and extremely convenient property that we can differentiate the loss with respect to the parameter, allowing us to efficiently learn good values for the parameter using gradient descent. In other words, the process of training is a function that takes a dataset, a model architecture, and a random seed and outputs model parameters.\n",
                "\n",
                "The learning rate, in contrast, cannot be determined by this scheme. As a hyperparameter, we need to introduce an outer loop that wraps the training loop to search for good learning rate values. This outer loop is called a hyperparameter search, and each iteration consists of testing different combinations of hyperparameters using a dataset of pairs of $(\\text{hyperparameters}, \\text{validation performance})$. Obtaining results for each iteration (a single pair) requires running the inner training loop.\n",
                "\n",
                "Due to a fixed budget of ML researcher time and available compute, we are interested in a trade-off between the ML researcher time, the cost of running the search, and the cost of training the final model. Due to the vast search space and cost of obtaining data, we don't hope to find any sort of optimum but merely to improve upon our initial guesses enough to justify the cost.\n",
                "\n",
                "In addition, a hyperparameter isn't necessarily a single continuous value like the learning rate. Discrete unordered choices such as padding type as well as discrete ordered choices such as the number of layers in the network or the width of each convolution are all common. You will also need to choose between functions for optimizers, nonlinearities, or learning rate scheduling, of which there are an infinite number of possibilities, requiring us to select a small subset to test.\n",
                "\n",
                "More broadly, every design decision can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, the types of operations, etc. Papers such as [AmeobaNet](https://arxiv.org/pdf/1801.01548.pdf) demonstrated that it's possible to find architectures superior to human-designed ones.\n",
                "\n",
                "In the second part of today's material, you will learn about various strategies for searching over hyperparameters.\n",
                "\n",
                "## Stochastic Gradient Descent\n",
                "\n",
                "The terms gradient descent and SGD are used loosely in deep learning. To be technical, there are three variations:\n",
                "\n",
                "- Batch gradient descent - the loss function is the loss over the entire dataset. This requires too much computation unless the dataset is small, so it is rarely used in deep learning.\n",
                "- Stochastic gradient descent - the loss function is the loss on a randomly selected example. Any particular loss may be completely in the wrong direction of the loss on the entire dataset, but in expectation it's in the right direction. This has some nice properties but doesn't parallelize well, so it is rarely used in deep learning.\n",
                "- Mini-batch gradient descent - the loss function is the loss on a batch of examples of size `batch_size`. This is the standard in deep learning.\n",
                "\n",
                "The class `torch.SGD` can be used for any of these by varying the number of examples passed in. We will be using only mini-batch gradient descent in this course.\n",
                "\n",
                "## Batch Size\n",
                "\n",
                "In addition to choosing a learning rate or learning rate schedule, we need to choose the batch size or batch size schedule as well. Intuitively, using a larger batch means that the estimate of the gradient is closer to that of the true gradient over the entire dataset, but this requires more compute. Each element of the batch can be computed in parallel so with sufficient compute, one can increase the batch size without increasing wall-clock time. For small-scale experiments, a good heuristic is thus \"fill up all of your GPU memory\".\n",
                "\n",
                "At a larger scale, we would expect diminishing returns of increasing the batch size, but empirically it's worse than that - a batch size that is too large generalizes more poorly in many scenarios. The intuition that a closer approximation to the true gradient is always better is therefore incorrect. See [this paper](https://arxiv.org/pdf/1706.02677.pdf) for one discussion of this.\n",
                "\n",
                "For a batch size schedule, most commonly you'll see batch sizes increase over the course of training. The intuition is that a rough estimate of the proper direction is good enough early in training, but later in training it's important to preserve our progress and not \"bounce around\" too much.\n",
                "\n",
                "You will commonly see batch sizes that are a multiple of 32. One motivation for this is that when using CUDA, threads are grouped into \"warps\" of 32 threads which execute the same instructions in parallel. So a batch size of 64 would allow two warps to be fully utilized, whereas a size of 65 would require waiting for a third warp to finish. As batch sizes become larger, this wastage becomes less important.\n",
                "\n",
                "Powers of two are also common - the idea here is that work can be recursively divided up among different GPUs or within a GPU. For example, a matrix multiplication can be expressed by recursively dividing each matrix into four equal blocks and performing eight smaller matrix multiplications between the blocks.\n",
                "\n",
                "## Computing Gradients in PyTorch\n",
                "\n",
                "Recall that gradients are only saved for `Tensor`s for which `requires_grad=True`. For convenience, `nn.Parameter` automatically sets `requires_grad=True` on the wrapped `Tensor`. As you call `torch` functions, PyTorch tracks the relevant information needed in case you call `backward` later on, at which point it does the actual computation to compute the gradient and stores it in the `Tensor`'s `grad` field.\n",
                "\n",
                "Also recall that PyTorch accumulates gradients across multiple `backward` calls. So if your tensor's `grad` already contains a value, after calling `backward` again it will have the sum of the original value and the new gradient. This behavior comes in handy in many situations, such as computing gradients over multiple runs on a GPU as part of a single batch. Suppose you choose a batch size of 32, but only 8 inputs fit on your GPU. A typical loss function for a batch computes the sum of losses over each example, so you can compute the losses 8 at a time and sum their gradients, producing the same result as running all 32 inputs at once.\n",
                "\n",
                "### Stopping gradients with `torch.no_grad` or `torch.inference_mode`\n",
                "\n",
                "You may not want PyTorch to track gradients for some computations despite involving tensors with `requires_grad=True`. In this case, you can wrap the computation in the `with torch.inference_mode()` context to prevent this tracking.\n",
                "\n",
                "\n",
                "In this notebook, we will discuss the most popular optimizers used in training neural networks. You will implement optimizers from simple to compex:\n",
                "- Stochastic Gradient Descent (SGD)\n",
                "- SGD with Momentum\n",
                "- RMSprop\n",
                "- Adam\n",
                "\n",
                "> ## Learning Objectives\n",
                "> - Understand what optimizers are and why they are used\n",
                "> - Get an intuition for momentum and RMS\n",
                "> - Understand what the different hyperparameters in Adam are and how they affect the training process\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import Tensor\n",
                "from jaxtyping import Float, jaxtyped\n",
                "import typeguard\n",
                "from SGD_tests import test_SGD, test_momentum, test_RMSprop, test_Adam, plot_rosenbrock\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "typechecked = jaxtyped(typechecker=typeguard.typechecked)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The job of an optimizer is to find the minimum in a Loss funciton. The loss function we are going to consider here is Rosenbrocks Banana function. \n",
                "Of course, in practice, loss funcitons are very high dimensional and not so simple. But we have an easier time plotting 2D functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@typechecked\n",
                "def rosenbrocks_banana(x: Tensor, y: Tensor, a=1, b=100) -> Tensor:\n",
                "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
                "\n",
                "\n",
                "plot_rosenbrock();"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - Implementing the training loop and Stochastic Gradient Descent (SGD) optimizer\n",
                "(2-5 mins)\n",
                "\n",
                "The training loop consists of the following steps:\n",
                "- Compute the loss\n",
                "- Compute the gradients\n",
                "- Update the parameters\n",
                "- Zero the gradients\n",
                "\n",
                "The update rule for Stochastic Gradient Descent (SGD) is given by:\n",
                "\n",
                "$$\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
                "\n",
                "where:\n",
                "- $\\theta_t$ represents the model's **parameters** at iteration $t$.\n",
                "- $\\alpha$ is the **learning rate**, which determines the step size of the parameter updates.\n",
                "- $\\nabla_\\theta \\text{Loss}(\\theta_t)$ denotes the **gradient of the loss function** $\\text{Loss}$ with respect to the parameters $\\theta_t$.\n",
                "\n",
                "Note: If you are stuck for too long, call a TA, or look at (part of) the solution. You can expand the \"Solution\" toggle after each exercise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def optimize_function(\n",
                "    function: callable, parameters: Float[Tensor, \"xy=2\"], optimizer, n_steps: int\n",
                ") -> Float[Tensor, \"n_steps xy=2\"]:\n",
                "    \"\"\"\n",
                "    Optimize a function using the given optimizer for n_steps steps.\n",
                "\n",
                "    Args:\n",
                "    - function (callable): The function to optimize. For example, `rosenbrocks_banana`.\n",
                "\n",
                "    \"\"\"\n",
                "    trajectory = []\n",
                "    for _ in range(n_steps):\n",
                "        trajectory.append(parameters.detach().clone())\n",
                "        loss = function(*parameters)\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "    trajectory = torch.stack(trajectory).float()\n",
                "    return trajectory\n",
                "\n",
                "\n",
                "class StocasticGradientDescent:\n",
                "    def __init__(self, parameters: Tensor, learning_rate: float):\n",
                "        self.parameters = parameters  # theta\n",
                "        self.learning_rate = learning_rate  # alpha\n",
                "\n",
                "    def step(self):\n",
                "        with torch.no_grad():\n",
                "            # Hide: all\n",
                "            self.parameters -= self.learning_rate * self.parameters.grad\n",
                "            # Hide: none\n",
                "\n",
                "    def zero_grad(self):\n",
                "        self.parameters.grad = None\n",
                "\n",
                "\n",
                "test_SGD(StocasticGradientDescent)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's examine the trajectory your optimizer follows in the parameter space! Does it appear sensible? Is it effective?\n",
                "\n",
                "You can experiment for a few minutes with the learning rate to observe how it influences the trajectory.\n",
                "\n",
                "**Note:** If you observe a straight line in the plot, it indicates that the learning rate is too high and the optimization is diverging."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parameters = torch.tensor([-1.0, 2.0], requires_grad=True)\n",
                "\n",
                "N_steps = 100\n",
                "learning_rate = 0.001\n",
                "optimizer = StocasticGradientDescent(parameters, learning_rate)\n",
                "\n",
                "trajectories = {}\n",
                "trajectories[\"SGD\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
                "plot_rosenbrock(trajectories);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - Implement the training loop and Stochastic Gradient Descent with Momentum optimizer\n",
                "(2-5 mins)\n",
                "\n",
                "When we have momentum in our optimizer, we additionally keep track of the \"velocity\" with which we are currently moving $a_t$. The update rule for Stochastic Gradient Descent with Momentum is given by:\n",
                "\n",
                "$$a_{t+1} = \\mu \\cdot a_t + (1 - \\mu) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
                "$$\\theta_{t+1} = \\theta_t - \\alpha \\cdot a_{t+1}$$\n",
                "\n",
                "where:\n",
                "- $\\mu$ is the **momentum parameter**, which determines how much of the previous velocity we keep.\n",
                "- $a_t$ represents the **running average** of the velocity at iteration $t$.\n",
                "- $\\alpha$ is the **learning rate**, which determines the step size of the parameter updates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Momentum:\n",
                "    def __init__(self, parameters: Tensor, learning_rate: float, momentum: float):\n",
                "        self.parameters = parameters  # theta\n",
                "        self.learning_rate = learning_rate  # alpha\n",
                "        # Hide: all\n",
                "        self.momentum_parameter = momentum  # mu\n",
                "        # Hide: none\n",
                "        self.average_grad = torch.zeros_like(parameters)\n",
                "\n",
                "    def step(self):\n",
                "        with torch.no_grad():\n",
                "            # Hide: all\n",
                "            self.average_grad = (\n",
                "                self.momentum_parameter * self.average_grad\n",
                "                + (1 - self.momentum_parameter) * self.parameters.grad\n",
                "            )\n",
                "            self.parameters -= self.learning_rate * self.average_grad\n",
                "            # Hide: none\n",
                "\n",
                "    def zero_grad(self):\n",
                "        self.parameters.grad = None\n",
                "\n",
                "\n",
                "test_momentum(Momentum)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parameters = torch.tensor([-1.0, 2.0], requires_grad=True)\n",
                "N_steps = 100\n",
                "learning_rate = 0.01\n",
                "momentum = 0.9\n",
                "optimizer = Momentum(parameters, learning_rate, momentum)\n",
                "trajectories[\"Momentum\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
                "plot_rosenbrock(trajectories);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Note about Bonus Exercises\n",
                "\n",
                "The following two exercises are similar to the last two. They introduce one new idea in RMSprop, and the Adam algorithm that combines RMSprop and momentum. Adam is the most popular optimizer in deep learning.\n",
                "\n",
                "Skip those ercises, and come back to them only if you have time. Instead, run the cell below to use pytorch implementation rather than yours.\n",
                "Then, go to the last exercise to play with hyperparameters. It's fun and it will help you build intuition about how they affect the training process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.optim import RMSprop, Adam"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bonus Exercise - Implement RMSprop optimizer\n",
                "(2-5 mins)\n",
                "\n",
                "GPT4 explaining RMSprop: Imagine you're hiking up a mountain with a bumpy path. If you take big, uniform steps, you might trip or miss the best route. RMSprop helps by adjusting your step size based on the terrain: smaller steps on rough patches (steep gradients) and bigger steps on smoother areas (gentle gradients). This way, you can reach the top more efficiently and avoid stumbling.\n",
                "\n",
                "RMSprop is an optimizer that adapts the learning rate for each parameter. To do that, we keep track of the moving average of the squared gradients $r_t$ and update the parameters as follows:\n",
                "\n",
                "$$r_{t+1} = \\mu \\cdot r_t + (1 - \\mu) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)^2$$\n",
                "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{r_{t+1} + \\epsilon}} \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
                "\n",
                "where:\n",
                "- $\\mu$ is the **momentum parameter**, which determines how much of the previous squared gradients we keep.\n",
                "- $r_t$ represents the **moving average** of the squared gradients at iteration $t$.\n",
                "- $\\epsilon$ is a small value added to the denominator to avoid division by zero.\n",
                "\n",
                "When the variance of gradients is high, we reduce the learning rate as we want to be more conservative. And when the variance of gradients is low, we increase the learning rate, thus going faster towards the optima."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RMSprop:\n",
                "    def __init__(self, parameters: Tensor, learning_rate: float, momentum: float, epsilon: float):\n",
                "        self.parameters = parameters  # theta\n",
                "        self.learning_rate = learning_rate  # alpha\n",
                "        self.momentum_parameter = momentum  # mu\n",
                "        self.epsilon = epsilon\n",
                "        # Hide: all\n",
                "        self.average_squared_grad = torch.zeros_like(parameters)\n",
                "        # Hide: none\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def step(self):\n",
                "        # Hide: all\n",
                "        self.average_squared_grad = (\n",
                "            self.momentum_parameter * self.average_squared_grad\n",
                "            + (1 - self.momentum_parameter) * self.parameters.grad**2\n",
                "        )\n",
                "        self.parameters -= (\n",
                "            self.learning_rate\n",
                "            * self.parameters.grad\n",
                "            / (torch.sqrt(self.average_squared_grad + self.epsilon))\n",
                "        )\n",
                "        # Hide: none\n",
                "\n",
                "    def zero_grad(self):\n",
                "        self.parameters.grad = None\n",
                "\n",
                "\n",
                "test_RMSprop(RMSprop)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parameters = torch.tensor([-1.0, 2.0], requires_grad=True)\n",
                "N_steps = 100\n",
                "learning_rate = 0.2\n",
                "momentum = 0.9\n",
                "epsilon = 1e-8\n",
                "if RMSprop is torch.optim.RMSprop:\n",
                "    # If you skiped the exercise, the built-in optimizer needs a list of parameters, not a single tensor\n",
                "    optimizer = RMSprop([parameters], learning_rate, momentum, epsilon)\n",
                "else:\n",
                "    optimizer = RMSprop(parameters, learning_rate, momentum, epsilon)\n",
                "trajectories[\"RMSProp\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
                "\n",
                "plot_rosenbrock(trajectories);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bonus Exercise - Implement Adam optimizer\n",
                "(2-5 mins)\n",
                "\n",
                "For the Adam optimizer, we now bring both momentum and RMSprop together. We keep track of the moving average of the gradients $a_t$ and the moving average of the squared gradients $r_t$ and update the parameters as follows:\n",
                "\n",
                "$$a_{t+1} = \\mu_1 \\cdot a_t + (1 - \\mu_1) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)$$\n",
                "$$r_{t+1} = \\mu_2 \\cdot r_t + (1 - \\mu_2) \\cdot \\nabla_\\theta \\text{Loss}(\\theta_t)^2$$\n",
                "$$\\hat{a}_{t+1} = \\frac{a_{t+1}}{1 - \\mu_1^{t+1}}$$\n",
                "$$\\hat{r}_{t+1} = \\frac{r_{t+1}}{1 - \\mu_2^{t+1}}$$\n",
                "\n",
                "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{r}_{t+1} + \\epsilon}} \\cdot \\hat{a}_{t+1}$$\n",
                "\n",
                "where:\n",
                "- $\\mu_1$ and $\\mu_2$ are the **momentum parameters** for the gradients and squared gradients, respectively.\n",
                "- $a_t$ and $r_t$ represent the **moving averages** of the gradients and squared gradients at iteration $t$.\n",
                "- $\\hat{a}_{t+1}$ and $\\hat{r}_{t+1}$ are the **bias-corrected** moving averages.\n",
                "- $\\epsilon$ is a small value added to the denominator to avoid division by zero.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Adam:\n",
                "    def __init__(\n",
                "        self,\n",
                "        parameters: Tensor,\n",
                "        learning_rate: float,\n",
                "        momentum_grad: float,\n",
                "        momentum_grad_squared: float,\n",
                "        epsilon: float,\n",
                "    ):\n",
                "        self.parameters = parameters  # theta\n",
                "        self.learning_rate = learning_rate  # alpha\n",
                "        self.epsilon = epsilon\n",
                "        # Hide: all\n",
                "        self.momentum_grad = momentum_grad  # mu_1\n",
                "        self.momentum_grad_squared = momentum_grad_squared  # mu_2\n",
                "        self.average_grad = torch.zeros_like(parameters)\n",
                "        self.average_squared_grad = torch.zeros_like(parameters)\n",
                "        self.t = 0\n",
                "        # Hide: none\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def step(self):\n",
                "        # Hide: all\n",
                "        self.t += 1\n",
                "        self.average_grad = (\n",
                "            self.momentum_grad * self.average_grad + (1 - self.momentum_grad) * self.parameters.grad\n",
                "        )\n",
                "        self.average_squared_grad = (\n",
                "            self.momentum_grad_squared * self.average_squared_grad\n",
                "            + (1 - self.momentum_grad_squared) * self.parameters.grad**2\n",
                "        )\n",
                "        average_grad_hat = self.average_grad / (1 - self.momentum_grad**self.t)\n",
                "        average_squared_grad_hat = self.average_squared_grad / (\n",
                "            1 - self.momentum_grad_squared**self.t\n",
                "        )\n",
                "        self.parameters -= (\n",
                "            self.learning_rate\n",
                "            * average_grad_hat\n",
                "            / (torch.sqrt(average_squared_grad_hat + self.epsilon))\n",
                "        )\n",
                "        # Hide: none\n",
                "\n",
                "    def zero_grad(self):\n",
                "        self.parameters.grad = None\n",
                "\n",
                "\n",
                "test_Adam(Adam)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parameters = torch.tensor([-1.0, 2.0], requires_grad=True)\n",
                "\n",
                "N_steps = 100\n",
                "learning_rate = 0.9\n",
                "mu1 = 0.9\n",
                "mu2 = 0.999\n",
                "epsilon = 1e-8\n",
                "if Adam is torch.optim.Adam:\n",
                "    # If you skiped the exercise, the built-in optimizer needs a list of parameters, not a single tensor\n",
                "    optimizer = Adam([parameters], learning_rate, (mu1, mu2), epsilon)\n",
                "else:\n",
                "    optimizer = Adam(parameters, learning_rate, mu1, mu2, epsilon)\n",
                "\n",
                "trajectories[\"Adam\"] = optimize_function(rosenbrocks_banana, parameters, optimizer, N_steps)\n",
                "plot_rosenbrock(trajectories);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise - Play around with the hyperparameters\n",
                "\n",
                "Play around with learning rates, the betas, and epsilon.\n",
                "\n",
                "Challenge until time is up: get within a 1e-2 range of the minimum of the banana function in as few steps as possible."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
