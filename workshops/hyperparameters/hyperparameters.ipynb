{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/hyperparameters/hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "# Hyperparameters\n",
                "\n",
                "\n",
                "## Goals\n",
                "\n",
                "The goal of this workshop is to get familiar with:\n",
                "- the concept of hyperparameters\n",
                "- what the usual hyperparameters are for a neural network and what are their effects\n",
                "- how to tune them and validate a choice of hyperparameters\n",
                "\n",
                "## Structure of the notebook\n",
                "- Generate toy data\n",
                "- Creating the neural network in pytorch\n",
                "- Creating the training loop\n",
                "- Creating evaluation functions\n",
                "- Hyper-parameter optimization\n",
                "  - The optimizer\n",
                "  - The architecture\n",
                "  - The loss function\n",
                "  - Prediction on a test set\n",
                "\n",
                "## Hyperparameters\n",
                "\n",
                "OPTIONAL: This block of text is optional reading and recaps what was explained during the lecture.\n",
                "\n",
                "In the prerequisites you trained a model to learn the sinus function. In the process, we implicitly made many decisions, such as to use a degree 3 polynomial, to use a specific learning rate, etc.\n",
                "\n",
                "The learning rate is an example of a **hyperparameter**. A regular parameter is a variable whose value is automatically determined during the training process, using the optimizer. (And because we are using gradient descent, we must be able to differentiate the loss with respect to the parameters.)\n",
                "\n",
                "The learning rate, in contrast, cannot be determined by this training process. As a hyperparameter, we need to introduce an outer loop that wraps the training loop to search for good learning rate values. This outer loop is called a hyperparameter search, and each iteration consists of testing different combinations of hyperparameters, creating a table of results consisting of pairs $(\\text{hyperparameters}, \\text{validation performance})$. Obtaining results for each row of the table requires running the full inner training loop.\n",
                "\n",
                "Due to a fixed budget of ML researcher time and available compute, we are interested in a trade-off between the ML researcher time, the cost of running the search, and the cost of training the final model. Due to the vast search space and cost of obtaining data, we don't hope to find any sort of optimum but merely to improve upon our initial guesses enough to justify the cost.\n",
                "\n",
                "In addition, hyperparameters are not always continuous, like the learning rate, but can be discrete too, e.g. the number of layers in the network, choice of loss function, choice of optimization algorithm, learning rate scheduler, etc.\n",
                "\n",
                "More broadly, every design decision can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, the types of operations, etc. Papers such as [AmeobaNet](https://arxiv.org/pdf/1801.01548.pdf) demonstrate that it's possible to find architectures superior to human-designed ones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "# Display figures on jupyter notebook\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate toy data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We define a function to generate our synthetic the dataset, in the form of two interlaced spirals\n",
                "# You don't need to understand this code, just run it\n",
                "\n",
                "\n",
                "def spiral(phi):\n",
                "    x = (phi + 1) * torch.cos(phi)\n",
                "    y = phi * torch.sin(phi)\n",
                "    return torch.cat((x, y), dim=1)\n",
                "\n",
                "\n",
                "def generate_data(num_data):\n",
                "    angles = torch.empty((num_data, 1)).uniform_(1, 15)\n",
                "    data = spiral(angles)\n",
                "    # add some noise to the data\n",
                "    data += torch.empty((num_data, 2)).normal_(0.0, 0.4)\n",
                "    labels = torch.zeros((num_data,), dtype=torch.int)\n",
                "    # flip half of the points to create two classes\n",
                "    data[num_data // 2 :, :] *= -1\n",
                "    labels[num_data // 2 :] = 1\n",
                "    return data, labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate the training set with 4000 examples\n",
                "x_train, y_train = generate_data(4000)\n",
                "\n",
                "print(\"X_train\", x_train.shape)\n",
                "print(\"y_train\", y_train.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You don't need to understand this code, just run it\n",
                "def plot_data(x, y):\n",
                "    \"\"\"Plot labeled data points X and y. Label 1 is a red +, label 0 is a blue +.\"\"\"\n",
                "    plt.figure(figsize=(5, 5))\n",
                "    plt.plot(x[y == 1, 0], x[y == 1, 1], \"r+\")\n",
                "    plt.plot(x[y == 0, 0], x[y == 0, 1], \"b+\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the data\n",
                "plot_data(x_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As seen in the pre-requisite materials, PyTorch has `Dataset` and `DataLoader` objects, which make it convenient to load the data in batches, shuffle the data, etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "\n",
                "training_set = TensorDataset(x_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##  Creating the neural network\n",
                "\n",
                "Here we create the neural network. This is the model you'll try to improve in the exercises.\n",
                "\n",
                "It is already created for you, but you should read through the code and understand what is done on each line."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from typing_extensions import Literal"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A tutorial for constructing models can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read this code line-by-line. It's code you will see again many times\n",
                "\n",
                "\n",
                "class Model(nn.Module):\n",
                "    \"\"\"\n",
                "    A fully connected neural network with any number of layers.\n",
                "    \"\"\"\n",
                "\n",
                "    NAME_TO_NONLINEARITY = {\n",
                "        \"relu\": nn.ReLU,\n",
                "        \"sigmoid\": nn.Sigmoid,\n",
                "        \"tanh\": nn.Tanh,\n",
                "    }\n",
                "\n",
                "    def __init__(\n",
                "        self, layers=[2, 10, 1], non_linearity: Literal[\"relu\", \"sigmoid\", \"tanh\"] = \"relu\"\n",
                "    ):\n",
                "        super(Model, self).__init__()\n",
                "\n",
                "        modules = []\n",
                "        for input_dim, output_dim in zip(layers[:-1], layers[1:]):\n",
                "            modules.append(nn.Linear(input_dim, output_dim))\n",
                "            # After each linear layer, we apply a non-linearity\n",
                "            modules.append(self.NAME_TO_NONLINEARITY[non_linearity]())\n",
                "\n",
                "        # Remove the last non-linearity, since the last layer is the output layer\n",
                "        self.layers = nn.Sequential(*modules[:-1])\n",
                "\n",
                "    def forward(self, inputs):\n",
                "        ouput = self.layers(inputs)\n",
                "\n",
                "        # We want the model to predict 0 for one class and 1 for the other class\n",
                "        # A Sigmoid activation function maps the output from [-inf, inf] to [0, 1]\n",
                "        prediction = torch.sigmoid(ouput)\n",
                "        return prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the model:\n",
                "model = Model()\n",
                "\n",
                "# Choose the hyperparameters for the training loop:\n",
                "num_epochs = 10\n",
                "batch_size = 10\n",
                "\n",
                "# Loss function. This one is a mean squared error (MSE) loss between the output\n",
                "# of the network and the target label\n",
                "criterion = nn.MSELoss()\n",
                "\n",
                "# Optimizer. We use SGD optimizer with a learning rate (lr) of 0.01\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training the model\n",
                "More information can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) if needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read this code line-by-line. It's code you want to understand as it is central to ML\n",
                "\n",
                "# tqdm is a library used to display progress bars. It's useful when training.\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "\n",
                "def train(\n",
                "    num_epochs: int, batch_size: int, criterion, optimizer, model, dataset, verbose: bool = False\n",
                "):\n",
                "    \"\"\"Train a model.\"\"\"\n",
                "    # Store the training errors\n",
                "    train_losses = []\n",
                "    # Create a DataLoader to iterate over the dataset in batches\n",
                "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
                "\n",
                "    for epoch in tqdm(range(num_epochs)):\n",
                "        epoch_average_loss = 0\n",
                "        # Each epoch, we iterate over the dataset once\n",
                "        for x_batch, y_true in train_loader:\n",
                "            # Compute the predictions.\n",
                "            # Output shape is (batch_size, 1), so we squeeze the last dimension\n",
                "            y_predicted = model(x_batch).squeeze(1)\n",
                "\n",
                "            # The loss is how far the predictions are from the true labels\n",
                "            loss = criterion(y_predicted, y_true.float())\n",
                "\n",
                "            # Do gradient descent to minimize the loss\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            # Record the average loss for this batch\n",
                "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
                "\n",
                "        train_losses.append(epoch_average_loss)\n",
                "\n",
                "        if verbose:\n",
                "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_average_loss:.4f}\")\n",
                "\n",
                "    return train_losses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# run the training loop\n",
                "train_losses = train(num_epochs, batch_size, criterion, optimizer, model, training_set, 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the training error wrt. the number of epochs\n",
                "plt.plot(range(1, num_epochs + 1), train_losses)\n",
                "plt.xlabel(\"num_epochs\")\n",
                "plt.ylabel(\"Train error\")\n",
                "plt.title(\"Visualization of convergence\")\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating the model on the validation set\n",
                "\n",
                "We first evaluate the accuracy on a validation set, to see how the model performs on data it did not see during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read this code line-by-line. It's code you want to understand as it is central to ML\n",
                "\n",
                "# Generate 1000 validation datapoints\n",
                "x_val, y_val = generate_data(1000)\n",
                "\n",
                "\n",
                "def get_accuracy(model, x=x_val, y=y_val):\n",
                "    \"\"\"Compute the accuracy of the model on a dataset.\"\"\"\n",
                "    # Compute the predictions, without keeping track of the gradients\n",
                "    with torch.no_grad():\n",
                "        y_predicted = model(x).squeeze(1)\n",
                "\n",
                "    # The predictions are in [0, 1] and the labels are either 0 or 1\n",
                "    # So we round the predictions to get the predicted labels\n",
                "    y_predicted = torch.round(y_predicted)\n",
                "\n",
                "    # Compute the accuracy by counting the number of correct predictions\n",
                "    accuracy = (y_predicted == y).sum().item() / len(y)\n",
                "\n",
                "    print(f\"Accuracy on {len(y)} examples: {accuracy:.2%}\")\n",
                "    return accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "get_accuracy(model);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then we visualize what the model has learned by plotting all the predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You don't need to understand this code, just run it\n",
                "\n",
                "\n",
                "def compare_predictions(model, x=x_val, y_real=y_val):\n",
                "    \"\"\"Compare the prediction with real labels.\"\"\"\n",
                "\n",
                "    with torch.no_grad():\n",
                "        y_predicted = model(x).squeeze(1)\n",
                "\n",
                "    plt.figure(figsize=(10, 5))\n",
                "\n",
                "    reds = y_real > 0.5\n",
                "    plt.subplot(121)\n",
                "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
                "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
                "    plt.title(\"real data\")\n",
                "\n",
                "    reds = y_predicted > 0.5\n",
                "    plt.subplot(122)\n",
                "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
                "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
                "    plt.title(\"predicted data\")\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_predictions(model)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hyper-parameter optimisation\n",
                "\n",
                "We will now try to find the best combination of hyper-parameters.\n",
                "\n",
                "- RECOMMENDATION: For this exercise to be maximally useful, make predictions about each experiment for running the code. Write down your predictions somewhere and check your predictions against what is observed.\n",
                "\n",
                "Bonus:\n",
                "- if you want, you can make your predictions on [FateBook](https://fatebook.io), a nice website to easily make predictions, resolve them and see your calibration.\n",
                "- organize the results of the experiments in a tidy summary table, so that this table would be produced if you reset the notebook and clicked 'Run All'\n",
                "- At the end of this notebook, we have a separate test dataset. Why do we need this in addition to the training and validation set?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 1: Impact of the optimizer\n",
                "\n",
                "Retrain the model by using different hyperparameters, you can change them in the previous sections definition, or put everything you need in the cell below for convenience.\n",
                "\n",
                "Try to see the impact of the following factors:\n",
                "* Use different batch size from 10 to 400\n",
                "* Use different values of the learning rate (between 0.001 and 10), and see how these impact the training process.\n",
                "* Change the duration of the training by increasing the number of epochs\n",
                "* Use other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Impact of the architecture of the model\n",
                "\n",
                "Try to see the impact of the following factors:\n",
                "\n",
                "* Try to add more layers (1, 2, 3, more ?)\n",
                "* Try different activation functions ([sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc.)\n",
                "* Try to change the number of neurons for each layer (5, 10, 20, more ?)\n",
                "* Do all network architectures react the same way to different learning rates?\n",
                "\n",
                "**Note:** These changes may interact with your previous choices of hyperparameters, and you may need to change them as well!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Impact of the loss function\n",
                "\n",
                "MSELoss is rarely used nowadays for classification and instead Cross Entropy is used.\n",
                "It consists in interpreting the output of the network as the probability $p(y | x)$ of the point $x$ belonging to the class $y$.\n",
                "Hence, the goal of the neural network is to maximize the probability for the *correct* class, that is, in maximizing $\\displaystyle \\prod_{(x,y) \\in Dataset} p(y|x)$.\n",
                "Applying $-\\log$, we obtain:\n",
                "\n",
                "$$ \\sum_{(x,y) \\in Dataset} - \\log p(y | x) $$\n",
                "\n",
                "This is the Cross Entropy loss and usually the number of outputs in the final layer equals the number of possible classes.\n",
                "Because we have a binary problem, it is easier to just have a single output and use [BCELoss](https://pytorch.org/docs/stable/nn.html?highlight=bce#torch.nn.BCELoss).\n",
                "\n",
                "Counterintuitively, for numerical stability reasons, it is better combine the sigmoid (done at the end of forward) and the BCELoss into a single function.\n",
                "This is done by [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html?highlight=bcewithlogit#torch.nn.BCEWithLogitsLoss).\n",
                "\n",
                "So explicitly, your task is:\n",
                "- Use `BCEWithLogitsLoss` instead of MSE and see how this changes the behavior in the network. This can also interact with the changes of the previous exercices.\n",
                "- Ensure you modify your network so there is no sigmoid done on the final output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 4: Prediction on test set\n",
                "\n",
                "Once you have chosen your hyper-parametrs and trained your final model, you should evaluate it on a test dataset (that was never seen during training or during validation).\n",
                "\n",
                "Question: why is this needed?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
