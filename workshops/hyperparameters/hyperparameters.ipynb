{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/hyperparameters/hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Hyperparameters for a Binary classification problem\n",
    "\n",
    "The goal of this workshop is to get familiar with:\n",
    "- the concept of hyperparameters\n",
    "- what the usual hyperparameters are for a neural network and what are their effects\n",
    "- how to tune them and validate a choice of hyperparameters\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "In the prerequisites you trained a very small model to learn the sinus function. In the process, you had to make many decisions, such as to use a degree 3 polynomial, to use a specific learning rate, etc.\n",
    "\n",
    "The learning rate is an example of a **hyperparameter**, which will be described below. As a reminder, a regular parameter is an adjustable value with the special and extremely convenient property that we can differentiate the loss with respect to the parameter, allowing us to efficiently learn good values for the parameter using gradient descent. In other words, the process of training is a function that takes a dataset, a model architecture, and a random seed and outputs model parameters.\n",
    "\n",
    "The learning rate, in contrast, cannot be determined by this scheme. As a hyperparameter, we need to introduce an outer loop that wraps the training loop to search for good learning rate values. This outer loop is called a hyperparameter search, and each iteration consists of testing different combinations of hyperparameters using a dataset of pairs of $(\\text{hyperparameters}, \\text{validation performance})$. Obtaining results for each iteration (a single pair) requires running the inner training loop.\n",
    "\n",
    "Due to a fixed budget of ML researcher time and available compute, we are interested in a trade-off between the ML researcher time, the cost of running the search, and the cost of training the final model. Due to the vast search space and cost of obtaining data, we don't hope to find any sort of optimum but merely to improve upon our initial guesses enough to justify the cost.\n",
    "\n",
    "In addition, a hyperparameter isn't necessarily a single continuous value like the learning rate. Discrete unordered choices such as padding type as well as discrete ordered choices such as the number of layers in the network or the width of each convolution are all common. You will also need to choose between functions for optimizers, nonlinearities, or learning rate scheduling, of which there are an infinite number of possibilities, requiring us to select a small subset to test.\n",
    "\n",
    "More broadly, every design decision can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, the types of operations, etc. Papers such as [AmeobaNet](https://arxiv.org/pdf/1801.01548.pdf) demonstrated that it's possible to find architectures superior to human-designed ones.\n",
    "\n",
    "In the second part of this workshop, you will be able to test various strategies for searching over hyperparameters.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "We study first a binary classification problem, performed by a neural network. Each input has two real features, that is, they are points in 2D and the output can be only 0 or 1.\n",
    "\n",
    "The training set contains 4000 examples, and the validation set, 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Display figures on jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to generate our synthetic the dataset, in the form of two interlaced spirals\n",
    "# You don't need to understand this code, just run it\n",
    "\n",
    "\n",
    "def spiral(phi):\n",
    "    x = (phi + 1) * torch.cos(phi)\n",
    "    y = phi * torch.sin(phi)\n",
    "    return torch.cat((x, y), dim=1)\n",
    "\n",
    "\n",
    "def generate_data(num_data):\n",
    "    angles = torch.empty((num_data, 1)).uniform_(1, 15)\n",
    "    data = spiral(angles)\n",
    "    # add some noise to the data\n",
    "    data += torch.empty((num_data, 2)).normal_(0.0, 0.4)\n",
    "    labels = torch.zeros((num_data,), dtype=torch.int)\n",
    "    # flip half of the points to create two classes\n",
    "    data[num_data // 2 :, :] *= -1\n",
    "    labels[num_data // 2 :] = 1\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training set with 4000 examples\n",
    "x_train, y_train = generate_data(4000)\n",
    "\n",
    "print(\"X_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y):\n",
    "    \"\"\"Plot labeled data points X and y. Label 1 is a red +, label 0 is a blue +.\"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(x[y == 1, 0], x[y == 1, 1], \"r+\")\n",
    "    plt.plot(x[y == 0, 0], x[y == 0, 1], \"b+\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now invoke the `plot_data` function on the dataset previously generated to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `TensorDataset` wrapper from pytorch, so that the framework can easily understand our tensors as a proper dataset.\n",
    "\n",
    "A `Dataset` in PyTorch holds together the inputs and the corresponding labels (if any), and provides a way to access them.\n",
    "It also interfaces neatly with the `DataLoader` class, which is used to load the data in batches, shuffle it, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "training_set = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A neural network to classify the data\n",
    "\n",
    "Here is a skeleton of a neural network with, by default, a single hidden layer. This is the model you'll try to improve during this exercise.\n",
    "\n",
    "Look at the code and run it to see the structure, then follow the questions below to iteratively improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step, we define a neural network with just two layers. A useful tutorial for constructing models can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't just run, but also read this code, it's code you would find again many times in the future\n",
    "\n",
    "from typing_extensions import Literal\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected neural network with any number of layers.\n",
    "    \"\"\"\n",
    "\n",
    "    NAME_TO_NONLINEARITY = {\n",
    "        \"relu\": nn.ReLU,\n",
    "        \"sigmoid\": nn.Sigmoid,\n",
    "        \"tanh\": nn.Tanh,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, layers=[2, 10, 1], non_linearity: Literal[\"relu\", \"sigmoid\", \"tanh\"] = \"relu\"\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        for input_dim, output_dim in zip(layers[:-1], layers[1:]):\n",
    "            modules.append(nn.Linear(input_dim, output_dim))\n",
    "            # After each linear layer, we apply a non-linearity\n",
    "            modules.append(self.NAME_TO_NONLINEARITY[non_linearity]())\n",
    "\n",
    "        # Remove the last non-linearity, since the last layer is the output layer\n",
    "        self.layers = nn.Sequential(*modules[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ouput = self.layers(inputs)\n",
    "\n",
    "        # We want the model to predict 0 for one class and 1 for the other class\n",
    "        # A Sigmoid activation function appropriate to map the output from [-inf, inf] to [0, 1]\n",
    "        prediction = torch.sigmoid(ouput)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "model = Model()\n",
    "\n",
    "# Choose the hyperparameters for training:\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Training criterion. This one is a mean squared error (MSE) loss between the output\n",
    "# of the network and the target label\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate (lr) of 0.01\n",
    "# It is initialized on our model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "More information can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a library used to display progress bars. It's extremely useful when training.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    num_epochs: int, batch_size: int, criterion, optimizer, model, dataset, verbose: bool = False\n",
    "):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    # Store the training errors\n",
    "    train_losses = []\n",
    "    # Create a DataLoader to iterate over the dataset in batches\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_average_loss = 0\n",
    "        # Each epoch, we iterate over the dataset once\n",
    "        for x_batch, y_true in train_loader:\n",
    "            # Compute the predictions.\n",
    "            # Output shape is (batch_size, 1), so we squeeze the last dimension\n",
    "            y_predicted = model(x_batch).squeeze(1)\n",
    "\n",
    "            # The loss is how far the predictions are from the true labels\n",
    "            loss = criterion(y_predicted, y_true.float())\n",
    "\n",
    "            # Do gradient descent to minimize the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the average loss for this batch\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "\n",
    "        train_losses.append(epoch_average_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_average_loss:.4f}\")\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train(num_epochs, batch_size, criterion, optimizer, model, training_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error wrt. the number of epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_losses)\n",
    "plt.xlabel(\"num_epochs\")\n",
    "plt.ylabel(\"Train error\")\n",
    "plt.title(\"Visualization of convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the validation set\n",
    "\n",
    "We first evaluate the accuracy on a validation set, to see how the model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 validation datapoints\n",
    "x_val, y_val = generate_data(1000)\n",
    "\n",
    "\n",
    "def get_accuracy(model, x=x_val, y=y_val):\n",
    "    \"\"\"Compute the accuracy of the model on a dataset.\"\"\"\n",
    "    # Compute the predictions, without keeping track of the gradients\n",
    "    with torch.no_grad():\n",
    "        y_predicted = model(x).squeeze(1)\n",
    "\n",
    "    # The predictions are in [0, 1] and the labels are either 0 or 1\n",
    "    # So we round the predictions to get the predicted labels\n",
    "    y_predicted = torch.round(y_predicted)\n",
    "\n",
    "    # Compute the accuracy by counting the number of correct predictions\n",
    "    accuracy = (y_predicted == y).sum().item() / len(y)\n",
    "\n",
    "    print(f\"Accuracy on {len(y)} examples: {accuracy:.2%}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize what the model has learned by plotting all the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(model, x=x_val, y_real=y_val):\n",
    "    \"\"\"Compare the prediction with real labels.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_predicted = model(x).squeeze(1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    reds = y_real > 0.5\n",
    "    plt.subplot(121)\n",
    "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
    "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
    "    plt.title(\"real data\")\n",
    "\n",
    "    reds = y_predicted > 0.5\n",
    "    plt.subplot(122)\n",
    "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
    "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
    "    plt.title(\"predicted data\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_predictions(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-optimisation\n",
    "\n",
    "We defined a lot a hyper-parameters (learning rate, layer sizes...) in the previous section. We will now try to find the best combination of hyper-parameters.\n",
    "\n",
    "> WARNING: For this exercise to be maximally useful, before you start answering the questions, try to make predictions about the impact of each meta-parameter.\n",
    "Afterwards, you can check that the predictions were correct.\n",
    "\n",
    "Bonus: if you want, you can make your predictions on [FateBook](https://fatebook.io).\n",
    "\n",
    "\n",
    "Questions:\n",
    "- Meta optimization: The goal of this tutorial is to get a summary table of the impact of each meta-parameter by clicking once on the \"Run All\" button.\n",
    "To do this, you need to think about the difference between the grid search strategy and the sensitivity analysis strategy? Which strategy is more suitable in case there are a lot of meta parameters? Try to implement this strategy in the following.\n",
    "- Why do you need the test dataset in addition to the validation dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Impact of the optimizer\n",
    "\n",
    "Retrain the model by using different hyperparameters, you can change them in the previous sections definition, or put everything you need in the cell below for convenience.\n",
    "\n",
    "Try to see the impact of the following factors:\n",
    "* Use different batch size from 10 to 400\n",
    "* Try different values of the learning rate (between 0.001 and 10), and see how these impact the training process.\n",
    "* Change the duration of the training by increasing the number of epochs\n",
    "* Try other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Impact of the architecture of the model\n",
    "\n",
    "The class `Model` is the definition of your model.\n",
    "Retrain the model by using different architectures, similarly as before.\n",
    "\n",
    "Try out different architectures and\n",
    "see the impact of the following factors:\n",
    "\n",
    "* Try to add more layers (1, 2, 3, more ?)\n",
    "* Try to different activation functions ([sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc.)\n",
    "* Try to change the number of neurons for each layer (5, 10, 20, more ?)\n",
    "* Do all network architectures react the same way to different learning rates?\n",
    "\n",
    "\n",
    "**Note:** These changes may interact with your previous choices of hyperparameters, and you may need to change them as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Impact of the loss function\n",
    "\n",
    "The current model uses a mean square error (MSE) loss. While this loss can be used in this case, it is now rarely used for classification, and instead a Binary Cross Entropy (BCE) is used. It consists in interpreting the output of the network as the probability $p(y | x)$ of the point $x$ to belong to the class $y$, and in maximizing the probability to be correct for all samples $x$, that is, in maximizing $\\displaystyle \\prod_{(x,y) \\in Dataset} p(y|x)$. Applying $-\\log$ to this quantity, we obtain the following criterion to minimize:\n",
    "\n",
    "$$ \\sum_{(x,y) \\in Dataset} - \\log p(y | x) $$\n",
    "\n",
    "This is implemented as such by the [BCELoss](https://pytorch.org/docs/stable/nn.html?highlight=bce#torch.nn.BCELoss) of pytorch. Note that this criterion requires its input to be a probability, i.e. in $[0,1]$, which requires the use of an appropriate activation function beforehand, e.g., a sigmoid.\n",
    "\n",
    "It turns out that, for numerical stability reasons, it is better to incorporate this sigmoid and the BCELoss into a single function; this is done by the [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html?highlight=bcewithlogit#torch.nn.BCEWithLogitsLoss). Try to replace the MSE by this one and see how this changes the behavior in the network. This can also interact with the changes of the two previous exercices.\n",
    "\n",
    "**Note:** As a consequence, when using the BCEWithLogitsLoss, the last layer of your network should not be followed by an activation function, as BCEWithLogitsLoss already adds a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Prediction on test set\n",
    "\n",
    "Once you have a model that seems satisfying on the validation dataset, you SHOULD evaluate it on a test dataset that has never been used before, to obtain a final accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
