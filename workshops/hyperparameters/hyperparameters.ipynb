{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/hyperparameters/hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "# Hyperparameters\n",
                "\n",
                "\n",
                "## Goals\n",
                "\n",
                "The goal of this workshop is to become more familiar with:\n",
                "- the concept of hyperparameters\n",
                "- the usual hyperparameters for a neural network\n",
                "- the basics of what it looks like to develop machine learning models\n",
                "- what can make the difference between successful and unsuccessful training\n",
                "\n",
                "## Structure of the notebook\n",
                "- Generating toy data\n",
                "- Creating the neural network\n",
                "- Writing the training loop\n",
                "- Evaluating the model\n",
                "- Searching for good hyperparameters\n",
                "  - Impact of the optimizer\n",
                "  - Impact of the architecture of the model\n",
                "  - Impact of the loss function\n",
                "- Evaluating the model on a test set\n",
                "\n",
                "## Hyperparameters\n",
                "\n",
                "*OPTIONAL: The text before the next code block is optional reading and recaps what was already explained in the session introduction.*\n",
                "\n",
                "In the prerequisites you trained a model to learn the sine function. In the process, we implicitly made many decisions, such as to use a degree 3 polynomial, to use a specific learning rate, etc.\n",
                "\n",
                "The learning rate is an example of a **hyperparameter**. A regular parameter is a variable whose value is optimized during the training process. (And because we are using gradient descent for optimization, we must be able to differentiate the loss with respect to the parameters.) The learning rate, in contrast, is not typically determined by this training process. (We cannot differentiate the loss with respect to the learning rate â€” in this case since the learning rate is something that shows up in the definition of gradient descent itself.)\n",
                "\n",
                "One way we might tune hyperparameters is to introduce an outer loop (that wraps around the training loop) to search for good hyperparameter values. This outer loop is called a hyperparameter search. Obtaining results for each combination of hyperparameters requires running the full inner training loop and then checking how well the model does on a held-out validation dataset (which should be different from the test dataset you use to see how good your final model is).\n",
                "\n",
                "Since every iteration of a hyperparameter search involves a training run (each of which might be very expensive), we often do not hope to find an optimum but rather to improve upon our initial guesses enough to justify the cost of the search.\n",
                "\n",
                "In addition, though some hyperparameters are continuous, like the learning rate, they can also be discrete, like the number of layers in the network, type of loss function, type of optimization algorithm, type of learning rate scheduler, and so on.\n",
                "\n",
                "More broadly, every design decision for the training process can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, and so on. What exactly people mean by the hyperparameters depends on the context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generating toy data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We define a function to generate our synthetic the dataset, in the form of two interlaced spirals.\n",
                "# You don't need to understand this code, just run it.\n",
                "\n",
                "\n",
                "def spiral(phi):\n",
                "    x = (phi + 1) * torch.cos(phi)\n",
                "    y = phi * torch.sin(phi)\n",
                "    return torch.cat((x, y), dim=1)\n",
                "\n",
                "\n",
                "def generate_data(num_data):\n",
                "    angles = torch.empty((num_data, 1)).uniform_(1, 15)\n",
                "    data = spiral(angles)\n",
                "    # Add some noise to the data.\n",
                "    data += torch.empty((num_data, 2)).normal_(0.0, 0.4)\n",
                "    labels = torch.zeros((num_data,), dtype=torch.int)\n",
                "    # Flip half of the points to create two classes.\n",
                "    data[num_data // 2 :, :] *= -1\n",
                "    labels[num_data // 2 :] = 1\n",
                "    return data, labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate a training set with 4000 examples.\n",
                "\n",
                "x_train, y_train = generate_data(4000)\n",
                "\n",
                "print(\"x_train shape:\", x_train.size())\n",
                "print(\"y_train shape:\", y_train.size())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You don't need to understand this code, just run it.\n",
                "def plot_data(x, y):\n",
                "    \"\"\"Plot data points x with labels y. Label 1 is a red +, label 0 is a blue +.\"\"\"\n",
                "    plt.figure(figsize=(5, 5))\n",
                "    plt.plot(x[y == 1, 0], x[y == 1, 1], \"r+\")\n",
                "    plt.plot(x[y == 0, 0], x[y == 0, 1], \"b+\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the data.\n",
                "plot_data(x_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As seen in the pre-requisite materials, PyTorch has `Dataset` and `DataLoader` objects, which make it convenient to load the data in batches, shuffle the data, etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "\n",
                "training_set = TensorDataset(x_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##  Creating the neural network\n",
                "\n",
                "Here we create the neural network. This is the model you'll try to improve in the exercises.\n",
                "\n",
                "It is already created for you, but you should read through the code and understand what is done on each line."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "from typing import Literal"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A tutorial for constructing models can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read this code line-by-line.\n",
                "\n",
                "\n",
                "class Model(nn.Module):\n",
                "    \"\"\"\n",
                "    A fully connected neural network with any number of layers.\n",
                "    \"\"\"\n",
                "\n",
                "    NAME_TO_NONLINEARITY = {\n",
                "        \"relu\": nn.ReLU,\n",
                "        \"sigmoid\": nn.Sigmoid,\n",
                "        \"tanh\": nn.Tanh,\n",
                "    }\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        layer_sizes: list[int] = [2, 10, 1],\n",
                "        non_linearity: Literal[\"relu\", \"sigmoid\", \"tanh\"] = \"relu\"\n",
                "    ):\n",
                "        super(Model, self).__init__()\n",
                "\n",
                "        modules = []\n",
                "        for input_dim, output_dim in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
                "            modules.append(nn.Linear(input_dim, output_dim))\n",
                "            # After each linear layer, apply a non-linearity.\n",
                "            modules.append(self.NAME_TO_NONLINEARITY[non_linearity]())\n",
                "\n",
                "        # Remove the last non-linearity, since the last layer is the output layer.\n",
                "        self.layers = nn.Sequential(*modules[:-1])\n",
                "\n",
                "    def forward(self, inputs):\n",
                "        outputs = self.layers(inputs)\n",
                "        # We want the model to predict 0 for one class and 1 for the other class.\n",
                "        # A sigmoid function maps the output from [-inf, inf] to [0, 1].\n",
                "        prediction = torch.sigmoid(outputs)\n",
                "        return prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the model.\n",
                "model = Model()\n",
                "\n",
                "# Choose the hyperparameters for the training loop.\n",
                "num_epochs = 10\n",
                "batch_size = 16\n",
                "\n",
                "# Choose the loss function. This one is a mean squared error (MSE) loss, where the\n",
                "# error refers to the difference between the output of the network and the target label.\n",
                "loss_function = nn.MSELoss()\n",
                "\n",
                "# Choose the optimizer. We use a stochastic gradient descent (SGD) optimizer\n",
                "# with a learning rate (lr) of 0.01.\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Writing the training loop\n",
                "More information can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) if needed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read this code line-by-line.\n",
                "# It's code you want to understand because it is central to ML.\n",
                "\n",
                "# tqdm is a library used to display progress bars. It's useful when training.\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "\n",
                "def train(\n",
                "    num_epochs: int,\n",
                "    batch_size: int,\n",
                "    loss_function: nn.Module,\n",
                "    optimizer: torch.optim.Optimizer,\n",
                "    model: nn.Module,\n",
                "    dataset: torch.utils.data.Dataset,\n",
                "    verbose: bool = False\n",
                "):\n",
                "    \"\"\"Train a model.\"\"\"\n",
                "    # Initialize a list to store the training losses.\n",
                "    train_losses = []\n",
                "    # Create a DataLoader to iterate over the dataset in batches.\n",
                "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
                "\n",
                "    for epoch in tqdm(range(num_epochs)):\n",
                "        epoch_average_loss = 0\n",
                "        # Each epoch, we iterate over the dataset once.\n",
                "        for x_batch, y_true in train_loader:\n",
                "            # Compute the predictions.\n",
                "            # Output shape is (batch_size, 1), so we squeeze the last dimension.\n",
                "            y_predicted = model(x_batch).squeeze(1)\n",
                "\n",
                "            # The loss is how far the predictions are from the true labels\n",
                "            loss = loss_function(y_predicted, y_true.float())\n",
                "\n",
                "            # Do gradient descent to minimize the loss.\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            # Record the average loss for this batch.\n",
                "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
                "\n",
                "        train_losses.append(epoch_average_loss)\n",
                "\n",
                "        if verbose:\n",
                "            print(f\"Epoch [{epoch + 1} / {num_epochs}], Loss: {epoch_average_loss:.4f}\")\n",
                "\n",
                "    return train_losses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the training loop.\n",
                "train_losses = train(num_epochs, batch_size, loss_function, optimizer, model, training_set, 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the training error w.r.t. the number of epochs.\n",
                "plt.plot(range(1, num_epochs + 1), train_losses)\n",
                "plt.xlabel(\"Number of epochs\")\n",
                "plt.ylabel(\"Training loss\")\n",
                "plt.title(\"Visualization of training convergence\")\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating the model\n",
                "\n",
                "We now evaluate the accuracy on a validation set, to see how the model performs on data it did not see during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read this code line-by-line.\n",
                "# It's code you want to understand as it is central to ML.\n",
                "\n",
                "# Generate 1000 validation datapoints.\n",
                "x_val, y_val = generate_data(1000)\n",
                "\n",
                "\n",
                "def get_accuracy(model, x=x_val, y=y_val):\n",
                "    \"\"\"Compute the accuracy of the model on a dataset.\"\"\"\n",
                "    # Compute the predictions without keeping track of the gradients.\n",
                "    with torch.inference_mode():\n",
                "        y_predicted = model(x).squeeze(1)\n",
                "\n",
                "    # The predictions are in [0, 1] and the labels are either 0 or 1.\n",
                "    # So we round the predictions to get the predicted labels.\n",
                "    y_predicted = torch.round(y_predicted)\n",
                "\n",
                "    # Compute the accuracy by counting the number of correct predictions.\n",
                "    accuracy = (y_predicted == y).sum().item() / len(y)\n",
                "    \n",
                "    return accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Accuracy on {len(y_val)} examples: {get_accuracy(model):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then we visualize what the model has learned by plotting all the predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You don't need to understand this code, just run it.\n",
                "\n",
                "\n",
                "def compare_predictions(model, x=x_val, y=y_val):\n",
                "    \"\"\"Compare the prediction with real labels.\"\"\"\n",
                "\n",
                "    with torch.inference_mode():\n",
                "        y_predicted = model(x).squeeze(1)\n",
                "\n",
                "    plt.figure(figsize=(10, 5))\n",
                "\n",
                "    reds = y > 0.5\n",
                "    plt.subplot(121)\n",
                "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
                "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
                "    plt.title(\"real data\")\n",
                "\n",
                "    reds = y_predicted > 0.5\n",
                "    plt.subplot(122)\n",
                "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
                "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
                "    plt.title(\"predicted data\")\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_predictions(model)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Searching for good hyperparameters\n",
                "\n",
                "We will now try to find the best combination of hyperparameters.\n",
                "\n",
                "*RECOMMENDATION: For this exercise to be maximally useful, you can make predictions about each experiment before running the code. Write down your predictions somewhere and check your predictions against what is observed.*\n",
                "\n",
                "Bonus:\n",
                "- If you want, you can make your predictions on [FateBook](https://fatebook.io), a nice website to easily make predictions, resolve them, and see your calibration.\n",
                "- Organize the results of the experiments in a clean way, so that a nice visualization would be produced if you reset the notebook and clicked 'Run All'.\n",
                "\n",
                "You have a few options for Exercise 1 and 2 below:\n",
                "\n",
                "1. Loop over different hyperparameter combinations and make plots that illustrate the effect of each hyperparameter [on the validation performance] while keeping the others fixed. You could also make some two-dimensional plots that illustrate the joint effect of two hyperparameters at a time.\n",
                "\n",
                "2. If short on time, or if you do not care about being too systematic or getting an initial feel for what things look like at the level of Python code, you can also use the [Neural Network Playground](https://playground.tensorflow.org#dataset=spiral) to more visually and informally experiment with hyperparameters as described below. The neural network architecture and the spiral dataset in the playground are similar to the setup in this notebook.\n",
                "\n",
                "3. If you are comfortable coding and feel like learning how to use a popular \"MLOps\" framework, you can also follow a tutorial like [this one from Weights & Biases](https://docs.wandb.ai/tutorials/sweeps/) and try to use their framework to search for hyperparameters here."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 1: Impact of the optimizer\n",
                "\n",
                "Retrain the model by using different hyperparameters. You can change them in the definitions in previous cells, but it is recommended that you put the code for modifying the values of hyperparameters in the cell below.\n",
                "\n",
                "Try to see the impact of the following factors:\n",
                "* Use different batch sizes from 16 to 1024.\n",
                "* Use different values of the learning rate (for example, between 0.001 and 10), and see how these impact the training process.\n",
                "* Change the duration of the training by increasing the number of epochs.\n",
                "* Use other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Impact of the architecture of the model\n",
                "\n",
                "Try to see the impact of the following factors:\n",
                "\n",
                "* Try to add more layers.\n",
                "* Try to change the number of neurons in each layer.\n",
                "* Try different activation functions ([sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [ReLU](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), and so on).\n",
                "\n",
                "These changes may interact with your previous choices of hyperparameters!\n",
                "\n",
                "You can thus also try to explore questions like:\n",
                "* Do all network architectures react the same way to different learning rates?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### (Bonus) Exercise: Impact of the loss function\n",
                "\n",
                "MSELoss is rarely used for classification and instead a cross-entropy loss is used.\n",
                "\n",
                "Using a cross-entropy loss involves interpreting the output of the network as a probability $p_{model}(y \\mid x)$; that is, the probability that the input $x$ belongs to the class $y$ according to the model.\n",
                "\n",
                "The goal of the neural network can be framed as maximizing the probability of the *correct* class for every datapoint; that is, in maximizing\n",
                "\n",
                "$$ \\prod_{(x,y) \\in data} p_{model}(y \\mid x) $$\n",
                "\n",
                "or, equivalently, in minimizing the result of applying $-\\log$ to the above:\n",
                "\n",
                "$$ \\sum_{(x,y) \\in data} - \\log p_{model}(y \\mid x) $$\n",
                "\n",
                "This is called the negative log-likelihood. (For those who have taken statistics courses, minimizing this means that we are doing maximum likelihood estimation.) In our case (where we are doing classification with binary labels), this is mathematically equivalent to the cross-entropy between the true data distribution and the model's predicted distribution, and hence we refer to it as the cross-entropy loss.\n",
                "\n",
                "(Note that the number of outputs in the final layer of a neural network trained with cross-entropy loss would normally equal the number of possible classes. Because we have a binary problem, it is easier to just have a single output and use [BCELoss](https://pytorch.org/docs/stable/nn.html?highlight=bce#torch.nn.BCELoss).)\n",
                "\n",
                "Counterintuitively, for numerical stability reasons, it is better to combine the sigmoid (done at the end of the forward function of the model) and the BCELoss into a single function.\n",
                "This can be done with [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html?highlight=bcewithlogit#torch.nn.BCEWithLogitsLoss).\n",
                "\n",
                "So explicitly, your task is:\n",
                "- Use `BCEWithLogitsLoss` instead of MSE and see how this changes the behavior in the network. This can also interact with the changes of the previous exercises.\n",
                "- Ensure you modify your network so there is no sigmoid on the final output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluating the model on a test set\n",
                "### Exercise 3: Prediction on a test set\n",
                "\n",
                "Once you have chosen your hyperparameters and trained your final model, you should evaluate it on a test dataset (that was never seen during training **or during validation**).\n",
                "\n",
                "Question: Why is this needed?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
