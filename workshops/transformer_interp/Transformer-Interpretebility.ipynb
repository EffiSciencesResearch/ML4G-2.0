{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Thanks to Callum McDougall for creating mauch of this notebooks content!\n",
    "\n",
    "This exercise is designed for you to get some hands on expereince with Transfomer Interpretebility. Transfoermers are the predominant arcitecture for LLMs, and much of the work that has been done on interpretability has been done on these models. We will look at and replicate some key findings, usin Neel Nanda's **TransformerLens** library.\n",
    "\n",
    "\n",
    "## Content & Learning Objectives\n",
    "\n",
    "#### 1ï¸âƒ£ TransformerLens: Introduction\n",
    "\n",
    "This section is designed to get you up to speed with the TransformerLens library. You'll learn how to load and run models, and learn about the shared architecture template for all of these models (the latter of which should be familiar to you if you've already done the exercises that come before these, since many of the same design principles are followed).\n",
    "\n",
    "> ##### Learning objectives\n",
    "> \n",
    "> - Load and run a `HookedTransformer` model\n",
    "> - Understand the basic architecture of these models\n",
    "> - Use the model's tokenizer to convert text to tokens, and vice versa\n",
    "> - Know how to cache activations, and to access activations from the cache\n",
    "\n",
    "#### 2ï¸âƒ£ Find Inductive Heads via attention patterns\n",
    "\n",
    "You'll plot attnetion patterns to check out theory of Inductive Heads, and see if you can find them in the model you're working with.\n",
    "> ##### Learning objectives\n",
    "> \n",
    "> - Use `circuitsvis` to visualise attention heads\n",
    "> - Understand what the theory of inductive heads predicts about attention patterns\n",
    "> - Use attention patterns to identify inductive heads\n",
    "\n",
    "#### 3ï¸âƒ£ Logit Attribution\n",
    "\n",
    "Here, you'll learn how to use TransfomerLens to implement LogitLens, a tool for attributing logit values to specific components of the model. You'll also learn how to use this tool to identify basic attention heads, that are imortant for Induction tasks\n",
    "> ##### Learning objectives\n",
    "> - Understand to use Hook functions\n",
    "> - Perform direct logit attribution to figure out which heads are writing to the residual stream in a significant way\n",
    "> - Learn how to use different transformerlens helper functions, which decompose the residual stream in different ways\n",
    "\n",
    "#### 4ï¸âƒ£  Activation Addition\n",
    "\n",
    "Next, you'll learn how to influence the behaviour of a Transfomrmer by writing into the residual stream. You'll learn how to construct steering vectors out of transformer activations, and how to measure the effect of these steering vectors on the model's output.\n",
    "\n",
    "> ##### Learning objectives\n",
    "> \n",
    "> - Understand how activation addition works\n",
    "> - Use activation addition to influence the model's output\n",
    "> - Use contrastive activation addition to construct steering vectors\n",
    "> - Measure the effect of steering vectors on the model's output\n",
    "\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import webbrowser\n",
    "import re\n",
    "import itertools\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import List, Optional, Callable, Tuple, Dict, Literal, Set, Union\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "import circuitsvis as cv\n",
    "from pathlib import Path\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "from transformer_lens.components import Embed, Unembed, LayerNorm, MLP\n",
    "\n",
    "t.set_grad_enabled(False);\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerLens: Introduction\n",
    "\n",
    "> ### Learning objectives\n",
    "> \n",
    "> - Load and run a `HookedTransformer` model\n",
    "> - Understand the basic architecture of these models\n",
    "> - Use the model's tokenizer to convert text to tokens, and vice versa\n",
    "> - Know how to cache activations, and to access activations from the cache\n",
    "> - Use `circuitsvis` to visualise attention heads\n",
    "                \n",
    "\n",
    "## Introduction\n",
    "\n",
    "*Note - this intro is written from the POV of Neel Nanda.*\n",
    "\n",
    "This is a demo notebook for [TransformerLens](https://github.com/neelnanda-io/TransformerLens), **a library I ([Neel Nanda](https://www.neelnanda.io/)) wrote for doing [mechanistic interpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models.** The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this! Mechanistic interpretability is a very young and small field, and there are a *lot* of open problems - if you would like to help, please try working on one! **Check out my [list of concrete open problems](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit#) to figure out where to start.**\n",
    "\n",
    "I wrote this library because after I left the Anthropic interpretability team and started doing independent research, I got extremely frustrated by the state of open source tooling. There's a lot of excellent infrastructure like HuggingFace and DeepSpeed to *use* or *train* models, but very little to dig into their internals and reverse engineer how they work. **This library tries to solve that**, and to make it easy to get into the field even if you don't work at an industry org with real infrastructure! The core features were heavily inspired by [Anthropic's excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html). Credit to Nelson Elhage and Chris Olah for building Garcon and showing me the value of good infrastructure for accelerating exploratory research!\n",
    "\n",
    "The core design principle I've followed is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state. This notebook demonstrates how the library works and how to use it, but if you want to see how well it works for exploratory research, check out [my notebook analysing Indirect Objection Identification](https://github.com/neelnanda-io/TransformerLens/blob/main/Exploratory_Analysis_Demo.ipynb) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!\n",
    "\n",
    "\n",
    "## Loading and Running Models\n",
    "\n",
    "TransformerLens comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. For this demo notebook we'll look at GPT-2 Small, an 80M parameter model, see the Available Models section for info on the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HookedTransformerConfig\n",
    "\n",
    "Alternatively, you can define a config object, then call `HookedTransformer.from_config(cfg)` to define your model. This is particularly useful when you want to have finer control over the architecture of your model. We'll see an example of this in the next section, when we define an attention-only model to study induction heads.\n",
    "\n",
    "Even if you don't define your model in this way, you can still access the config object through the `cfg` attribute of the model.\n",
    "\n",
    "\n",
    "### Exercise - inspect your model\n",
    "\n",
    "```yaml\n",
    "Difficulty: ðŸ”´âšªâšªâšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "```\n",
    "\n",
    "Use `gpt2_small.cfg` to find the following, for your GPT-2 Small model:\n",
    "\n",
    "* Number of layers\n",
    "* Number of heads per layer\n",
    "* Maximum context window\n",
    "\n",
    "You might have to check out the documentation page for some of these. If you're in VSCode then you can reach it by right-clicking on `HookedTransformerConfig` and choosing \"Go to definition\". If you're in Colab, then you can read the [GitHub page](https://github.com/neelnanda-io/TransformerLens).\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The following parameters in the config object give you the answers:\n",
    "\n",
    "```\n",
    "cfg.n_layers == 12\n",
    "cfg.n_heads == 12\n",
    "cfg.n_ctx == 1024\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### Running your model\n",
    "\n",
    "Models can be run on a single string or a tensor of tokens (shape: `[batch, position]`, all integers). The possible return types are: \n",
    "\n",
    "* `\"logits\"` (shape `[batch, position, d_vocab]`, floats), \n",
    "* `\"loss\"` (the cross-entropy loss when predicting the next token), \n",
    "* `\"both\"` (a tuple of `(logits, loss)`) \n",
    "* `None` (run the model, but don't calculate the logits - this is faster when we only want to use intermediate activations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description_text = '''## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!'''\n",
    "\n",
    "loss = gpt2_small(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer architecture\n",
    "\n",
    "HookedTransformer is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads:\n",
    "\n",
    "* The weights `W_K`, `W_Q`, `W_V` mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one.\n",
    "* The weight matrices `W_K`, `W_Q`, `W_V`, `W_O` and activations have separate `head_index` and `d_head` axes, rather than flattening them into one big axis.\n",
    "    * The activations all have shape `[batch, position, head_index, d_head]`.\n",
    "    * `W_K`, `W_Q`, `W_V` have shape `[head_index, d_model, d_head]` and `W_O` has shape `[head_index, d_head, d_model]`\n",
    "* **Important - we generally follow the convention that weight matrices multiply on the right rather than the left.** In other words, they have shape `[input, output]`, and we have `new_activation = old_activation @ weights + bias`.\n",
    "    * Click the dropdown below for examples of this, if it seems unintuitive.\n",
    "\n",
    "<details>\n",
    "<summary>Examples of matrix multiplication in our model</summary>\n",
    "\n",
    "* **Query matrices**\n",
    "    * Each query matrix `W_Q` for a particular layer and head has shape `[d_model, d_head]`. \n",
    "    * So if a vector `x` in the residual stream has length `d_model`, then the corresponding query vector is `x @ W_Q`, which has length `d_head`.\n",
    "* **Embedding matrix**\n",
    "    * The embedding matrix `W_E` has shape `[d_vocab, d_model]`. \n",
    "    * So if `A` is a one-hot-encoded vector of length `d_vocab` corresponding to a particular token, then the embedding vector for this token is `A @ W_E`, which has length `d_model`.\n",
    "\n",
    "</details>\n",
    "\n",
    "The actual code is a bit of a mess, as there's a variety of Boolean flags to make it consistent with the various different model families in TransformerLens - to understand it and the internal structure, I instead recommend reading the code in [CleanTransformerDemo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb).\n",
    "\n",
    "\n",
    "### Parameters and Activations\n",
    "\n",
    "It's important to distinguish between parameters and activations in the model.\n",
    "\n",
    "* **Parameters** are the weights and biases that are learned during training.\n",
    "    * These don't change when the model input changes.\n",
    "    * They can be accessed direction fromm the model, e.g. `model.W_E` for the embedding matrix.\n",
    "* **Activations** are temporary numbers calculated during a forward pass, that are functions of the input.\n",
    "    * We can think of these values as only existing for the duration of a single forward pass, and disappearing afterwards.\n",
    "    * We can use hooks to access these values during a forward pass (more on hooks later), but it doesn't make sense to talk about a model's activations outside the context of some particular input.\n",
    "    * Attention scores and patterns are activations (this is slightly non-intuitve because they're used in a matrix multiplication with another activation).\n",
    "\n",
    "The link below shows a diagram of a single layer (called a `TransformerBlock`) for an attention-only model with no biases. Each box corresponds to an **activation** (and also tells you the name of the corresponding hook point, which we will eventually use to access those activations). The red text below each box tells you the shape of the activation (ignoring the batch dimension). Each arrow corresponds to an operation on an activation; where there are **parameters** involved these are labelled on the arrows.\n",
    "\n",
    "[Link to diagram](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/small-merm.svg)\n",
    "\n",
    "The next link is to a diagram of a `TransformerBlock` with full features (including biases, layernorms, and MLPs). Don't worry if not all of this makes sense at first - we'll return to some of the details later. As we work with these transformers, we'll get more comfortable with their architecture.\n",
    "\n",
    "[Link to diagram](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/full-merm.svg)\n",
    "                \n",
    "There's also an annotated version of the larger diagram [here](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-full-updated.png).\n",
    "\n",
    "\n",
    "A few shortctus to make your lives easier when using these models:\n",
    "\n",
    "* You can index weights like `W_Q` directly from the model via e.g. `model.blocks[0].attn.W_Q` (which gives you the `[nheads, d_model, d_head]` query weights for all heads in layer 0).\n",
    "    * But an easier way is just to index with `model.W_Q`, which gives you the `[nlayers, nheads, d_model, d_head]` tensor containing **every** query weight in the model.\n",
    "* Similarly, there exist shortcuts `model.W_E`, `model.W_U` and `model.W_pos` for the embeddings, unembeddings and positional embeddings respectively.\n",
    "* With models containing MLP layers, you also have `model.W_in` and `model.W_out` for the linear layers.\n",
    "* The same is true for all biases (e.g. `model.b_Q` for all query biases).\n",
    "\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The tokenizer is stored inside the model, and you can access it using `model.tokenizer`. There are also a few helper methods that call the tokenizer under the hood, for instance:\n",
    "\n",
    "* `model.to_str_tokens(text)` converts a string into a list of tokens-as-strings (or a list of strings into a list of lists of tokens-as-strings).\n",
    "* `model.to_tokens(text)` converts a string into a tensor of tokens.\n",
    "* `model.to_string(tokens)` converts a tensor of tokens into a string.\n",
    "\n",
    "Examples of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_small.to_str_tokens(\"gpt2\"))\n",
    "print(gpt2_small.to_str_tokens([\"gpt2\", \"gpt2\"]))\n",
    "print(gpt2_small.to_tokens(\"gpt2\"))\n",
    "print(gpt2_small.to_string([50256, 70, 457, 17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Aside - <code><|endoftext|></code></summary>\n",
    "\n",
    "A weirdness you may have noticed in the above is that `to_tokens` and `to_str_tokens` added a weird `<|endoftext|>` to the start of each prompt. We encountered this in the previous set of exercises, and noted that this was the **Beginning of Sequence (BOS)** token (which for GPT-2 is also the same as the EOS and PAD tokens - index `50256`.\n",
    "\n",
    "TransformerLens appends this token by default, and it can easily trip up new users. Notably, **this includes** `model.forward` (which is what's implicitly used when you do eg `model(\"Hello World\")`). You can disable this behaviour by setting the flag `prepend_bos=False` in `to_tokens`, `to_str_tokens`, `model.forward` and any other function that converts strings to multi-token tensors.\n",
    "\n",
    "`prepend_bos` is a bit of a hack, and I've gone back and forth on what the correct default here is. The reason I do this is that transformers tend to treat the first token weirdly - this doesn't really matter in training (where all inputs are >1000 tokens), but this can be a big issue when investigating short prompts! The reason for this is that attention patterns are a probability distribution and so need to add up to one, so to simulate being \"off\" they normally look at the first token. Giving them a BOS token lets the heads rest by looking at that, preserving the information in the first \"real\" token.\n",
    "\n",
    "Further, *some* models are trained to need a BOS token (OPT and my interpretability-friendly models are, GPT-2 and GPT-Neo are not). But despite GPT-2 not being trained with this, empirically it seems to make interpretability easier.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "### Exercise - how many tokens does your model guess correctly?\n",
    "\n",
    "```yaml\n",
    "Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "\n",
    "You should spend up to ~10 minutes on this exercise.\n",
    "```\n",
    "\n",
    "Consider the `model_description_text` you fed into your model above. How many words did your model guess correctly? Which words were correct? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits: Tensor = gpt2_small(model_description_text, return_type=\"logits\")\n",
    "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "# YOUR CODE HERE - get the model's prediction on the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Use `return_type=\"logits\"` to get the model's predictions, then take argmax across the vocab dimension. Then, compare these predictions with the actual tokens, derived from the `model_description_text`.\n",
    "\n",
    "Remember, you should be comparing the `[:-1]`th elements of this tensor of predictions with the `[1:]`th elements of the input tokens (because your model's output represents a probability distribution over the *next* token, not the current one).\n",
    "\n",
    "Also, remember to handle the batch dimension (since `logits`, and the output of `to_tokens`, will both have batch dimensions by default).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "```python\n",
    "logits = gpt2_small(model_description_text, return_type=\"logits\")\n",
    "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "true_tokens = gpt2_small.to_tokens(model_description_text).squeeze()[1:]\n",
    "num_correct = (prediction == true_tokens).sum()\n",
    "\n",
    "print(f\"Model accuracy: {num_correct}/{len(true_tokens)}\")\n",
    "print(f\"Correct tokens: {gpt2_small.to_str_tokens(prediction[prediction == true_tokens])}\")\n",
    "```\n",
    "\n",
    "The output from this code is:\n",
    "\n",
    "```\n",
    "Model accuracy: 33/111\n",
    "Correct tokens: ['\\n', '\\n', 'former', ' with', ' models', '.', ' can', ' of', 'ooked', 'Trans', 'former', '_', 'NAME', '`.', ' model', ' the', 'Trans', 'former', ' to', ' be', ' and', '-', '.', '\\n', '\\n', ' at', 'PT', '-', ',', ' model', ',', \"'s\", ' the']\n",
    "```\n",
    "\n",
    "So the model got 33 out of 112 tokens correct. Not too bad!\n",
    "</details>\n",
    "\n",
    "\n",
    "## Caching all Activations\n",
    "\n",
    "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out, on the first sentence from the GPT-2 paper.\n",
    "\n",
    "<details>\n",
    "<summary>Aside - a note on <code>remove_batch_dim</code></summary>\n",
    "\n",
    "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it. \n",
    "\n",
    "`gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "gpt2_tokens = gpt2_small.to_tokens(gpt2_text)\n",
    "gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the `gpt2_cache` object, you should see that it contains a very large number of keys, each one corresponding to a different activation in the model. You can access the keys by indexing the cache directly, or by a more convenient indexing shorthand. For instance, the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_patterns_layer_0 = gpt2_cache[\"pattern\", 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns the same thing as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_patterns_layer_0_copy = gpt2_cache[\"blocks.0.attn.hook_pattern\"]\n",
    "\n",
    "t.testing.assert_close(attn_patterns_layer_0, attn_patterns_layer_0_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Aside: <code>utils.get_act_name</code></summary>\n",
    "\n",
    "The reason these are the same is that, under the hood, the first example actually indexes by `utils.get_act_name(\"pattern\", 0)`, which evaluates to `\"blocks.0.attn.hook_pattern\"`.\n",
    "\n",
    "In general, `utils.get_act_name` is a useful function for getting the full name of an activation, given its short name and layer number.\n",
    "\n",
    "You can use the diagram from the **Transformer Architecture** section to help you find activation names.\n",
    "</details>\n",
    "\n",
    "\n",
    "### Exercise - verify activations\n",
    "\n",
    "```yaml\n",
    "Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "\n",
    "You should spend up to 10-15 minutes on this exercise.\n",
    "\n",
    "If you're already comfortable implementing things like attention calculations (e.g. having gone through Neel's transformer walkthrough) you can skip this exercise. However, it might serve as a useful refresher.\n",
    "```\n",
    "\n",
    "Verify that `hook_q`, `hook_k` and `hook_pattern` are related to each other in the way implied by the diagram. Do this by computing `layer0_pattern_from_cache` (the attention pattern taken directly from the cache, for layer 0) and `layer0_pattern_from_q_and_k` (the attention pattern calculated from `hook_q` and `hook_k`, for layer 0). Remember that attention pattern is the probabilities, so you'll need to scale and softmax appropriately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0_pattern_from_cache = gpt2_cache[\"pattern\", 0]\n",
    "\n",
    "# YOUR CODE HERE - define `layer0_pattern_from_q_and_k` manually, by manually performing the steps of the attention calculation (dot product, masking, scaling, softmax)\n",
    "t.testing.assert_close(layer0_pattern_from_cache, layer0_pattern_from_q_and_k)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Attention Heads\n",
    "\n",
    "A key insight from the Mathematical Frameworks paper is that we should focus on interpreting the parts of the model that are intrinsically interpretable - the input tokens, the output logits and the attention patterns. Everything else (the residual stream, keys, queries, values, etc) are compressed intermediate states when calculating meaningful things. So a natural place to start is classifying heads by their attention patterns on various texts.\n",
    "\n",
    "When doing interpretability, it's always good to begin by visualising your data, rather than taking summary statistics. Summary statistics can be super misleading! But now that we have visualised the attention patterns, we can create some basic summary statistics and use our visualisations to validate them! (Accordingly, being good at web dev/data visualisation is a surprisingly useful skillset! Neural networks are very high-dimensional object.)\n",
    "\n",
    "Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on Anthropic's PySvelte library). If you did the previous set of exercises, you'll have seen this library before.\n",
    "\n",
    "We will use the function `cv.attention.attention_patterns`, which takes the following arguments:\n",
    "\n",
    "* `attention`: Attention head activations. \n",
    "    * This should be a tensor of shape `[nhead, seq_dest, seq_src]`, i.e. the `[i, :, :]`th element is the grid of attention patterns (probabilities) for the `i`th attention head.\n",
    "    * We get this by indexing our `gpt2_cache` object.\n",
    "* `tokens`: List of tokens (e.g. `[\"A\", \"person\"]`). \n",
    "    * Sequence length must match that inferred from `attention`.\n",
    "    * This is used to label the grid.\n",
    "    * We get this by using the `gpt2_small.to_str_tokens` method.\n",
    "* `attention_head_names`: Optional list of names for the heads.\n",
    "\n",
    "There are also other circuitsvis functions, e.g. `cv.attention.attention_pattern` (for just a single head) or `cv.attention.attention_heads` (which has the same syntax and but presents the information in a different form).\n",
    "\n",
    "<details>\n",
    "<summary>Help - my <code>attention_heads</code> plots are behaving weirdly (e.g. they continually shrink after I plot them).</summary>\n",
    "\n",
    "This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.\n",
    "\n",
    "Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:\n",
    "\n",
    "```python\n",
    "attn_heads = cv.attention.attention_heads(\n",
    "    tokens=gpt2_str_tokens, \n",
    "    attention=attention_pattern,\n",
    "    attention_head_names=[f\"L0H{i}\" for i in range(12)],\n",
    ")\n",
    "\n",
    "path = \"attn_heads.html\"\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(str(attn_heads))\n",
    "\n",
    "webbrowser.open(path)\n",
    "```\n",
    "\n",
    "To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.\n",
    "</details>\n",
    "\n",
    "This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backwards, so information can only move forwards in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gpt2_cache))\n",
    "attention_pattern = gpt2_cache[\"pattern\", 0]\n",
    "print(attention_pattern.shape)\n",
    "gpt2_str_tokens = gpt2_small.to_str_tokens(gpt2_text)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(cv.attention.attention_patterns(\n",
    "    tokens=gpt2_str_tokens, \n",
    "    attention=attention_pattern,\n",
    "    attention_head_names=[f\"L0H{i}\" for i in range(12)],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hover over heads to see the attention patterns; click on a head to lock it. Hover over each token to see which other tokens it attends to (or which other tokens attend to it - you can see this by changing the dropdown to `Destination <- Source`).\n",
    "\n",
    "<details>\n",
    "<summary>Other circuitsvis functions - neuron activations</summary>\n",
    "\n",
    "The `circuitsvis` library also has a number of cool visualisations for **neuron activations**. Here are some more of them (you don't have to understand them all now, but you can come back to them later).\n",
    "\n",
    "The function below visualises neuron activations. The example shows just one sequence, but it can also show multiple sequences (if `tokens` is a list of lists of strings, and `activations` is a list of tensors).\n",
    "\n",
    "```python\n",
    "neuron_activations_for_all_layers = t.stack([\n",
    "    gpt2_cache[\"post\", layer] for layer in range(gpt2_small.cfg.n_layers)\n",
    "], dim=1)\n",
    "# shape = (seq_pos, layers, neurons)\n",
    "\n",
    "cv.activations.text_neuron_activations(\n",
    "    tokens=gpt2_str_tokens,\n",
    "    activations=neuron_activations_for_all_layers\n",
    ")\n",
    "```\n",
    "\n",
    "The next function shows which words each of the neurons activates most / least on (note that it requires some weird indexing to work correctly).\n",
    "\n",
    "```python\n",
    "neuron_activations_for_all_layers_rearranged = utils.to_numpy(einops.rearrange(neuron_activations_for_all_layers, \"seq layers neurons -> 1 layers seq neurons\"))\n",
    "\n",
    "cv.topk_tokens.topk_tokens(\n",
    "    # Some weird indexing required here Â¯\\_(ãƒ„)_/Â¯\n",
    "    tokens=[gpt2_str_tokens], \n",
    "    activations=neuron_activations_for_all_layers_rearranged,\n",
    "    max_k=7, \n",
    "    first_dimension_name=\"Layer\", \n",
    "    third_dimension_name=\"Neuron\",\n",
    "    first_dimension_labels=list(range(12))\n",
    ")\n",
    "```\n",
    "</details>\n",
    "\n",
    "# Finding induction heads\n",
    "\n",
    "## Introducing Our Toy Attention-Only Model\n",
    "\n",
    "Here we introduce a toy 2L attention-only transformer trained specifically for today. Some changes to make them easier to interpret:\n",
    "- It has only attention blocks.\n",
    "- The positional embeddings are only added to the residual stream before each key and query vector in the attention layers as opposed to the token embeddings - i.e. we compute queries as `Q = (resid + pos_embed) @ W_Q + b_Q` and same for keys, but values as `V = resid @ W_V + b_V`. This means that **the residual stream can't directly encode positional information**.\n",
    "    - This turns out to make it *way* easier for induction heads to form, it happens 2-3x times earlier - [see the comparison of two training runs](https://wandb.ai/mechanistic-interpretability/attn-only/reports/loss_ewma-22-08-24-11-08-83---VmlldzoyNTI0MDMz?accessToken=8ap8ir6y072uqa4f9uinotdtrwmoa8d8k2je4ec0lyasf1jcm3mtdh37ouijgdbm) here. (The bump in each curve is the formation of induction heads.)\n",
    "    - The argument that does this below is `positional_embedding_type=\"shortformer\"`.\n",
    "- It has no MLP layers, no LayerNorms, and no biases.\n",
    "- There are separate embed and unembed matrices (i.e. the weights are not tied).\n",
    "\n",
    "We now define our model with a `HookedTransformerConfig` object. This is similar to the `Config` object we used in the previous set of exercises, although it has a lot more features. You can look at the documentation page (Right-click, \"Go to Definition\" in VSCode) to seee what the different arguments do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model=768,\n",
    "    d_head=64,\n",
    "    n_heads=12,\n",
    "    n_layers=2,\n",
    "    n_ctx=2048,\n",
    "    d_vocab=50278,\n",
    "    attention_dir=\"causal\",\n",
    "    attn_only=True, # defaults to False\n",
    "    tokenizer_name=\"EleutherAI/gpt-neox-20b\", \n",
    "    seed=398,\n",
    "    use_attn_result=True,\n",
    "    normalization_type=None, # defaults to \"LN\", i.e. layernorm with weights & biases\n",
    "    positional_embedding_type=\"shortformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>An aside about tokenizers</summary>\n",
    "\n",
    "In the last section, we defined a tokenizer explicitly, and passed it into our model. But here, we just pass a tokenizer name. The model automatically creates a tokenizer for us (under the hood, it calls `AutoTokenizer.from_pretrained(tokenizer_name)`).\n",
    "</details>\n",
    "\n",
    "Below, you'll load in your weights, with some boilerplate code to download your state dict from HuggingFace (you can do this for any model you've uploaded to HuggingFace yourself):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we'll create our model and load in the weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_path, map_location=device)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             \n",
    "Use the [diagram at this link](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/small-merm.svg) to remind yourself of the relevant hook names.\n",
    "\n",
    "\n",
    "### Exercise - visualise attention patterns\n",
    "\n",
    "```yaml\n",
    "Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "\n",
    "You should spend up to ~10 minutes on this exercise.\n",
    "\n",
    "It's important to be comfortable using circuitsvis, and the cache object.\n",
    "```\n",
    "\n",
    "*This exercise should be very quick - you can reuse code from the previous section. You should look at the solution if you're still stuck after 5-10 minutes.*\n",
    "\n",
    "Visualise the attention patterns for both layers of your model, on the following prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "\n",
    "logits, cache = model.run_with_cache(text, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*(Note that we've run the model on the string `text`, rather than on tokens like we did previously when creating a cache - this is something that `HookedTransformer` allows.)*\n",
    "\n",
    "Inspect the attention patterns. What do you notice about the attention heads? \n",
    "\n",
    "You should spot three relatively distinctive basic patterns, which occur in multiple heads. What are these patterns, and can you guess why they might be present?\n",
    "\n",
    "<details>\n",
    "<summary>Aside - what to do if your plots won't show up</summary>\n",
    "\n",
    "A common mistake is to fail to pass the tokens in as arguments. If you do this, your attention patterns won't render.\n",
    "\n",
    "If this isn't the problem, then it might be an issue with the Circuitsvis library.Rather than plotting inline, you can do the following, and then open in your browser from the left-hand file explorer menu of VSCode:\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - visualize attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution </summary>\n",
    "\n",
    "We visualise attention patterns with the following code:\n",
    "\n",
    "```python\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    attention_pattern = cache[\"pattern\", layer]\n",
    "    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
    "```\n",
    "\n",
    "We notice that there are three basic patterns which repeat quite frequently:\n",
    "\n",
    "* `prev_token_heads`, which attend mainly to the previous token (e.g. head `0.7`)\n",
    "* `current_token_heads`, which attend mainly to the current token (e.g. head `1.6`)\n",
    "* `first_token_heads`, which attend mainly to the first token (e.g. heads `0.3` or `1.4`, although these are a bit less clear-cut than the other two)\n",
    "\n",
    "The `prev_token_heads` and `current_token_heads` are perhaps unsurprising, because words that are close together in a sequence probably have a lot more mutual information (i.e. we could get quite far using bigram or trigram prediction). \n",
    "\n",
    "The `first_token_heads` are a bit more surprising. The basic intuition here is that the first token in a sequence is often used as a resting or null position for heads that only sometimes activate (since our attention probabilities always have to add up to 1).\n",
    "</details>\n",
    "\n",
    "\n",
    "Now that we've observed our three basic attention patterns, it's time to make detectors for those patterns!\n",
    "\n",
    "\n",
    "### Exercise - write your own detectors\n",
    "\n",
    "```yaml\n",
    "Difficulty: ðŸ”´ðŸ”´âšªâšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µâšªâšª\n",
    "\n",
    "You shouldn't spend more than 15-20 minutes on these exercises.\n",
    "\n",
    "These exercises shouldn't be too challenging, if you understand attention patterns. Use the hint if stuck on things like how to correctly index your tensors, or how to access the activation patterns from the cache.\n",
    "```\n",
    "\n",
    "You should fill in the functions below, which act as detectors for particular types of heads. Validate your detectors by comparing these results to the visual attention patterns above - summary statistics on their own can be dodgy, but are much more reliable if you can validate it by directly playing with the data.\n",
    "\n",
    "Tasks like this are useful, because we need to be able to take our observations / intuitions about what a model is doing, and translate these into quantitative measures. As the exercises proceed, we'll be creating some much more interesting tools and detectors!\n",
    "\n",
    "Note - there's no objectively correct answer for which heads are doing which tasks, and which detectors can spot them. You should just try and come up with something plausible-seeming, which identifies the kind of behaviour you're looking for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(cache)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(cache)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(cache)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Try and compute the average attention probability along the relevant tokens. For instance, you can get the tokens just below the diagonal by using `t.diagonal` with appropriate `offset` parameter:\n",
    "\n",
    "```python\n",
    ">>> arr = t.arange(9).reshape(3, 3)\n",
    ">>> arr\n",
    "tensor([[0, 1, 2],\n",
    "        [3, 4, 5],\n",
    "        [6, 7, 8]])\n",
    "\n",
    ">>> arr.diagonal()\n",
    "tensor([0, 4, 8])\n",
    "\n",
    ">>> arr.diagonal(-1)\n",
    "tensor([3, 7])\n",
    "```\n",
    "\n",
    "Remember that you should be using `cache[\"pattern\", layer]` to get all the attention probabilities for a given layer, and then indexing on the 0th dimension to get the correct head.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Solution </code></summary>\n",
    "\n",
    "```python\n",
    "def current_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            # take avg of diagonal elements\n",
    "            score = attention_pattern.diagonal().mean()\n",
    "            if score > 0.4:\n",
    "                attn_heads.append(f\"{layer}.{head}\")\n",
    "    return attn_heads\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            # take avg of sub-diagonal elements\n",
    "            score = attention_pattern.diagonal(-1).mean()\n",
    "            if score > 0.4:\n",
    "                attn_heads.append(f\"{layer}.{head}\")\n",
    "    return attn_heads\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            # take avg of 0th elements\n",
    "            score = attention_pattern[:, 0].mean()\n",
    "            if score > 0.4:\n",
    "                attn_heads.append(f\"{layer}.{head}\")\n",
    "    return attn_heads\n",
    "```\n",
    "\n",
    "Note - choosing `0.4` as a threshold is a bit arbitrary, but it seems to work well enough. In this particular case, a threshold of `0.5` results in no head being classified as a current-token head.\n",
    "</details>\n",
    "\n",
    "\n",
    "Compare the printouts to your attention visualisations above. Do they seem to make sense?\n",
    "\n",
    "# Logit Attribution\n",
    "A consequence of the residual stream is that the output logits are the sum of the contributions of each layer, and thus the sum of the results of each head. This means we can decompose the output logits into a term coming from each head and directly do attribution like this!\n",
    "\n",
    "<details>\n",
    "<summary>A concrete example</summary>\n",
    "\n",
    "Let's say that our model knows that the token Harry is followed by the token Potter, and we want to figure out how it does this. The logits on Harry are `residual @ W_U`. But this is a linear map, and the residual stream is the sum of all previous layers `residual = embed + attn_out_0 + attn_out_1`. So `logits = (embed @ W_U) + (attn_out @ W_U) + (attn_out_1 @ W_U)`\n",
    "\n",
    "We can be even more specific, and *just* look at the logit of the Potter token - this corresponds to a column of `W_U`, and so a direction in the residual stream - our logit is now a single number that is the sum of `(embed @ potter_U) + (attn_out_0 @ potter_U) + (attn_out_1 @ potter_U)`. Even better, we can decompose each attention layer output into the sum of the result of each head, and use this to get many terms.\n",
    "</details>\n",
    "\n",
    "Your mission here is to write a function to look at how much each component contributes to the correct logit. Your components are:\n",
    "\n",
    "* The direct path (i.e. the residual connections from the embedding to unembedding),\n",
    "* Each layer 0 head (via the residual connection and skipping layer 1)\n",
    "* Each layer 1 head\n",
    "\n",
    "To emphasise, these are not paths from the start to the end of the model, these are paths from the output of some component directly to the logits - we make no assumptions about how each path was calculated!\n",
    "\n",
    "A few important notes for this exercise:\n",
    "\n",
    "* Here we are just looking at the DIRECT effect on the logits, i.e. the thing that this component writes / embeds into the residual stream - if heads compose with other heads and affect logits like that, or inhibit logits for other tokens to boost the correct one we will not pick up on this!\n",
    "* By looking at just the logits corresponding to the correct token, our data is much lower dimensional because we can ignore all other tokens other than the correct next one (Dealing with a 50K vocab size is a pain!). But this comes at the cost of missing out on more subtle effects, like a head suppressing other plausible logits, to increase the log prob of the correct one.\n",
    "    * There are other situations where our job might be easier. For instance, in the IOI task (which we'll discuss shortly) we're just comparing the logits of the indirect object to the logits of the direct object, meaning we can use the **difference between these logits**, and ignore all the other logits.\n",
    "* When calculating correct output logits, we will get tensors with a dimension `(position - 1,)`, not `(position,)` - we remove the final element of the output (logits), and the first element of labels (tokens). This is because we're predicting the *next* token, and we don't know the token after the final token, so we ignore it.\n",
    "\n",
    "<details>\n",
    "<summary>Aside - centering <code>W_U</code></summary>\n",
    "\n",
    "While we won't worry about this for this exercise, logit attribution is often more meaningful if we first center `W_U` - i.e. ensure the mean of each row writing to the output logits is zero. Log softmax is invariant when we add a constant to all the logits, so we want to control for a head that just increases all logits by the same amount. We won't do this here for ease of testing.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Question - why don't we do this to the log probs instead?</summary>\n",
    "\n",
    "Because log probs aren't linear, they go through `log_softmax`, a non-linear function.\n",
    "</details>\n",
    "\n",
    "To do do logit atribution, we first need hooks\n",
    "## What are hooks?\n",
    "\n",
    "One of the great things about interpreting neural networks is that we have *full control* over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don't know what they mean!). And we can make precise, surgical edits and see how the model's behaviour and other internals change. This is an extremely powerful tool, because it can let us e.g. set up careful counterfactuals and causal intervention to easily understand model behaviour. \n",
    "\n",
    "Accordingly, being able to do this is a pretty core operation, and this is one of the main things TransformerLens supports! The key feature here is **hook points**. Every activation inside the transformer is surrounded by a hook point, which allows us to edit or intervene on it. \n",
    "\n",
    "We do this by adding a **hook function** to that activation, and then calling `model.run_with_hooks`.\n",
    "\n",
    "*(Terminology note - because basically all the activations in our model have an associated hook point, we'll sometimes use the terms \"hook\" and \"activation\" interchangeably.)*\n",
    "\n",
    "\n",
    "### Hook functions\n",
    "\n",
    "Hook functions take two arguments: `activation_value` and `hook_point`. The `activation_value` is a tensor representing some activation in the model, just like the values in our `ActivationCache`. The `hook_point` is an object which gives us methods like `hook.layer()` or attributes like `hook.name` that are sometimes useful to call within the function.\n",
    "\n",
    "If we're using hooks to edit activations, then the hook function should return a tensor of the same shape as the activation value. But we can also just have our hook function access the activation, do some processing, and write the results to some external variable (in which case our hook function should just not return anything).\n",
    "\n",
    "An example hook function for changing the attention patterns at a particular layer might look like:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_function(\n",
    "    attn_pattern: Float[Tensor, \"batch heads seqQ seqK\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[Tensor, \"batch heads seqQ seqK\"]:\n",
    "\n",
    "    # modify attn_pattern (can be inplace)\n",
    "    return attn_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Running with hooks\n",
    "\n",
    "Once you've defined a hook function (or functions), you should call `model.run_with_hooks`. A typical call to this function might look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.run_with_hooks(\n",
    "    tokens, \n",
    "    return_type=\"loss\",\n",
    "    fwd_hooks=[\n",
    "        ('blocks.1.attn.hook_pattern', hook_function)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this code down.\n",
    "\n",
    "* `tokens` represents our model's input.\n",
    "* `return_type=\"loss\"` is used here because we're modifying our activations and seeing how this affects the loss.\n",
    "    * We could also return the logits, or just use `return_type=None` if we only want to access the intermediate activations and we don't care about the output.\n",
    "* `fwd_hooks` is a list of 2-tuples of (hook name, hook function).\n",
    "    * The hook name is a string that specifies which activation we want to hook. \n",
    "    * The hook function gets run with the corresponding activation as its first argument.\n",
    "\n",
    "### A bit more about hooks\n",
    "\n",
    "Here are a few extra notes for how to squeeze even more functionality out of hooks. If you'd prefer, you can [jump ahead](#hooks-accessing-activations) to see an actual example of hooks being used, and come back to this section later.\n",
    "\n",
    "<details>\n",
    "<summary>Resetting hooks</summary>\n",
    "\n",
    "`model.run_with_hooks` has the default parameter `reset_hooks_end=True` which resets all hooks at the end of the run (including both those that were added before and during the run).\n",
    "\n",
    "Despite this, it's possible to shoot yourself in the foot with hooks, e.g. if there's an error in one of your hooks so the function never finishes. In this case, you can use `model.reset_hooks()` to reset all hooks.\n",
    "\n",
    "Further, if you *do* want to keep the hooks that you add, you can do this by calling `add_hook` on the relevant `HookPoint`.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Adding multiple hooks at once</summary>\n",
    "\n",
    "Including more than one tuple in the `fwd_hooks` list is one way to add multiple hooks:\n",
    "\n",
    "```python\n",
    "loss = model.run_with_hooks(\n",
    "    tokens, \n",
    "    return_type=\"loss\",\n",
    "    fwd_hooks=[\n",
    "        ('blocks.0.attn.hook_pattern', hook_function),\n",
    "        ('blocks.1.attn.hook_pattern', hook_function)\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Another way is to use a **name filter** rather than a single name:\n",
    "\n",
    "```python\n",
    "loss = model.run_with_hooks(\n",
    "    tokens,\n",
    "    return_type=\"loss\",\n",
    "    fwd_hooks=[\n",
    "        (lambda name: name.endswith(\"pattern\"), hook_function)\n",
    "    ]\n",
    ")\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><code>utils.get_act_name</code></summary>\n",
    "\n",
    "When we were indexing the cache in the previous section, we found we could use strings like `cache['blocks.0.attn.hook_pattern']`, or use the shorthand of `cache['pattern', 0]`. The reason the second one works is that it calls the function `utils.get_act_name` under the hood, i.e. we have:\n",
    "\n",
    "```python\n",
    "utils.get_act_name('pattern', 0) == 'blocks.0.attn.hook_pattern'\n",
    "```\n",
    "\n",
    "Using `utils.get_act_name` in your forward hooks is often easier than using the full string, since the only thing you need to remember is the activation name (you can refer back to the diagram in the previous section for this).\n",
    "</details>\n",
    "<details>\n",
    "<summary>Using <code>functools.partial</code> to create variations on hooks</summary>\n",
    "\n",
    "A useful trick is to define a hook function with more arguments than it needs, and then use `functools.partial` to fill in the extra arguments. For instance, if you want a hook function which only modifies a particular head, but you want to run it on all heads separately (rather than just adding all the hooks and having them all run on the next forward pass), then you can do something like:\n",
    "\n",
    "```python\n",
    "def hook_all_attention_patterns(\n",
    "    attn_pattern: Float[Tensor, \"batch heads seqQ seqK\"],\n",
    "    hook: HookPoint,\n",
    "    head_idx: int\n",
    ") -> Float[Tensor, \"batch heads seqQ seqK\"]:\n",
    "    # modify attn_pattern inplace, at head_idx\n",
    "    return attn_pattern\n",
    "\n",
    "for head_idx in range(12):\n",
    "    temp_hook_fn = functools.partial(hook_all_attention_patterns, head_idx=head_idx)\n",
    "    model.run_with_hooks(tokens, fwd_hooks=[('blocks.1.attn.hook_pattern', temp_hook_fn)])\n",
    "```\n",
    "</details>\n",
    "\n",
    "And here are some points of interest, which aren't vital to understand:\n",
    "\n",
    "<details>\n",
    "<summary>Relationship to PyTorch hooks</summary>\n",
    "\n",
    "[PyTorch hooks](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that **Hook points** act on *activations* not layers. This means that you can intervene within a layer on each activation, and don't need to care about the precise layer structure of the transformer. And it's immediately clear exactly how the hook's effect is applied. This adjustment was shamelessly inspired by [Garcon's use of ProbePoints](https://transformer-circuits.pub/2021/garcon/index.html).\n",
    "\n",
    "They also come with a range of other quality of life improvements. PyTorch's hooks are global state, which can be a massive pain if you accidentally leave a hook on a model. TransformerLens hooks are also global state, but `run_with_hooks` tries to create an abstraction where these are local state by removing all hooks at the end of the function (and they come with a helpful `model.reset_hooks()` method to remove all hooks).\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>How are TransformerLens hooks actually implemented?</summary>\n",
    "\n",
    "They are implemented as modules with the identity function as their forward method:\n",
    "\n",
    "```python\n",
    "class HookPoint(nn.Module):\n",
    "    ...\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "```\n",
    "\n",
    "but also with special features for adding and removing hook functions. This is why you see hooks when you print a HookedTransformer model, because all its modules are recursively printed.\n",
    "\n",
    "When you run the model normally, hook modules won't change the model's behaviour (since applying the identity function does nothing). It's only once you add functions to the hook modules (e.g. a function which ablates any inputs into the hook module) that the model's behaviour changes.\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "## Hooks: Accessing Activations\n",
    "\n",
    "In later sections, we'll write some code to intervene on hooks, which is really the core feature that makes them so useful for interpretability. But for now, let's just look at how to access them without changing their value. This can be achieved by having the hook function write to a global variable, and return nothing (rather than modifying the activation in place).\n",
    "\n",
    "Why might we want to do this? It turns out to be useful for things like:\n",
    "\n",
    "* Extracting activations for a specific task\n",
    "* Doing some long-running calculation across many inputs, e.g. finding the text that most activates a specific neuron\n",
    "\n",
    "Note that, in theory, this could all be done using the `run_with_cache` function we used in the previous section, combined with post-processing of the cache result. But using hooks can be more intuitive and memory efficient.\n",
    "\n",
    "### Exercise - build logit attribution tool\n",
    "\n",
    "```yaml\n",
    "Difficulty: ðŸ”´ðŸ”´ðŸ”´âšªâšª\n",
    "Importance: ðŸ”µðŸ”µðŸ”µðŸ”µâšª\n",
    "\n",
    "You shouldn't spend more than 10-15 minutes on this exercise.\n",
    "\n",
    "This exercise is important, but has quite a few messy einsums, so you might get more value from reading the solution than doing the exercises.\n",
    "```\n",
    "\n",
    "You should implement the `logit_attribution` function below. This should return the contribution of each component in the \"correct direction\". We've already given you the unembedding vectors for the correct direction, `W_U_correct_tokens` (note that we take the `[1:]` slice of tokens, for reasons discussed above).\n",
    "\n",
    "The code below this function will check your logit attribution function is working correctly, by taking the sum of logit attributions and comparing it to the actual values in the residual stream at the end of your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_attribution(\n",
    "    embed: Float[Tensor, \"seq d_model\"],\n",
    "    l1_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "    l2_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "    W_U: Float[Tensor, \"d_model d_vocab\"],\n",
    "    tokens: Int[Tensor, \"seq\"]\n",
    ") -> Float[Tensor, \"seq-1 n_components\"]:\n",
    "    '''\n",
    "    Inputs:\n",
    "        embed: the embeddings of the tokens (i.e. token + position embeddings)\n",
    "        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dimensions)\n",
    "        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dimensions)\n",
    "        W_U: the unembedding matrix\n",
    "        tokens: the token ids of the sequence\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components)\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "    '''\n",
    "    W_U_correct_tokens = W_U[:, tokens[1:]]\n",
    "    pass\n",
    "\n",
    "\n",
    "text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "tokens = model.to_tokens(text)\n",
    "\n",
    "with t.inference_mode():\n",
    "    embed = cache[\"embed\"]\n",
    "    l1_results = cache[\"result\", 0]\n",
    "    l2_results = cache[\"result\", 1]\n",
    "    logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "    # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token\n",
    "    correct_token_logits = logits[0, t.arange(len(tokens[0]) - 1), tokens[0, 1:]]\n",
    "    t.testing.assert_close(logit_attr.sum(1), correct_token_logits, atol=1e-3, rtol=0)\n",
    "    print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "def logit_attribution(\n",
    "    embed: Float[Tensor, \"seq d_model\"],\n",
    "    l1_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "    l2_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "    W_U: Float[Tensor, \"d_model d_vocab\"],\n",
    "    tokens: Int[Tensor, \"seq\"]\n",
    ") -> Float[Tensor, \"seq-1 n_components\"]:\n",
    "    '''\n",
    "    Inputs:\n",
    "        embed: the embeddings of the tokens (i.e. token + position embeddings)\n",
    "        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dimensions)\n",
    "        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dimensions)\n",
    "        W_U: the unembedding matrix\n",
    "        tokens: the token ids of the sequence\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components)\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "    '''\n",
    "    W_U_correct_tokens = W_U[:, tokens[1:]]\n",
    "    # SOLUTION\n",
    "    direct_attributions = einops.einsum(W_U_correct_tokens, embed[:-1], \"emb seq, seq emb -> seq\")\n",
    "    l1_attributions = einops.einsum(W_U_correct_tokens, l1_results[:-1], \"emb seq, seq nhead emb -> seq nhead\")\n",
    "    l2_attributions = einops.einsum(W_U_correct_tokens, l2_results[:-1], \"emb seq, seq nhead emb -> seq nhead\")\n",
    "    return t.concat([direct_attributions.unsqueeze(-1), l1_attributions, l2_attributions], dim=-1)\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "Once you've got the tests working, you can visualise the logit attributions for each path through the model. We've provided you with the helper function `plot_logit_attribution`, which presents the results in a nice way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"embed\"]\n",
    "l1_results = cache[\"result\", 0]\n",
    "l2_results = cache[\"result\", 1]\n",
    "logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "\n",
    "plot_logit_attribution(model, logit_attr, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question - what is the interpretation of this plot?\n",
    "\n",
    "You should find that the most variation in the logit attribution comes from the direct path. In particular, some of the tokens in the direct path have a very high logit attribution (e.g. tokens 12, 24 and 46). Can you guess what gives them in particular such a high logit attribution? \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "The tokens with very high logit attribution are the ones which \"offer very probable bigrams\". For instance, the highest contribution on the direct path comes from `| manip|`, because this is very likely to be followed by `|ulative|` (or presumably a different stem like `| ulation|`). `| super|` -> `|human|` is another example of a bigram formed when the tokenizer splits one word into multiple tokens.\n",
    "\n",
    "There are also examples that come from two different words, rather than a single word split by the tokenizer. These include:\n",
    "\n",
    "* `| more|` -> `| likely|`\n",
    "* `| machine|` -> `| learning|`\n",
    "* `| by|` -> `| default|`\n",
    "* `| how|` -> `| to|`\n",
    "\n",
    "See later for a discussion of all the ~infuriating~ fun quirks of tokenization!\n",
    "</details>\n",
    "\n",
    "Another feature of the plot - the heads in the second layer seem to have much higher contributions than the heads in the first layer. Why do you think this might be?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Think about what this graph actually represents, in terms of paths through the transformer.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "This is because of a point we discussed earlier - this plot doesn't pick up on things like a head's effect in composition with another head. So the attribution for layer-0 heads won't involve any composition, whereas the attributions for layer-1 heads will involve not only the single-head paths through those attention heads, but also the 2-layer compositional paths through heads in layer 0 and layer 1.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
