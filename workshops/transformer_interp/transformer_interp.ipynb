{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer_interp/transformer_interp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "# GPT-2 interpretability\n",
                "\n",
                "## Logit lens\n",
                "\n",
                "### First step: understanding Logit lens\n",
                "\n",
                "Read: https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens\n",
                "\n",
                "### Second step: Reproducing the results\n",
                "\n",
                "Reimplement the Logit lens in a minimal way by reproducing the figure at the end of this section.\n",
                "This exercice is quite unguided because being able to use the transformer library autonomously is very important.\n",
                "\n",
                "Resources if you are stuck:\n",
                "- Read about hooks here https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks\n",
                "- pip install transformer-utils and use the function _plot_logit_lens https://github.dev/nostalgebraist/transformer-utils/tree/main/src/transformer_utils/logit_lens\n",
                "\n",
                "\n",
                "You should optain this figure:\n",
                "\n",
                "<!-- ![results from logit lens](./results.png) -->\n",
                "\n",
                "![results from the logit lens](https://github.com/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer_interp/results.png?raw=true)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup: Don't read, just run\n",
                "\n",
                "try:\n",
                "    import google.colab\n",
                "\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    # Install packages\n",
                "    %pip install transformers jaxtyping einops typeguard==2.13.3 -q\n",
                "\n",
                "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
                "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/transformer_interp/*'\n",
                "    !mv --no-clobber ML4G-2.0-master/workshops/transformer_interp/* .\n",
                "    !rm -r ML4G-2.0-master\n",
                "\n",
                "    print(\"Imports & installations complete!\")\n",
                "\n",
                "else:\n",
                "    from IPython import get_ipython\n",
                "\n",
                "    ipython = get_ipython()\n",
                "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
                "    ipython.run_line_magic(\"autoreload\", \"2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from functools import partial\n",
                "\n",
                "import einops\n",
                "import torch\n",
                "import transformers\n",
                "from jaxtyping import Float, Int, jaxtyped\n",
                "from torch import Tensor\n",
                "from typeguard import typechecked\n",
                "\n",
                "from utils import plot_logit_lens_low_level"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hints:\n",
                "- GPT-2 has tied embeddings, so the embedding and unembedding matrices are the same. You can access them with `model.base_model.wte.weight`\n",
                "- Do not forget to apply the final LayerNorm to normalise the residual stream before applying the softmax. You can access it with `model.base_model.ln_f`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loading GPT2 and its tokenizer\n",
                "gpt2 = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
                "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
                "gpt2.eval()\n",
                "\n",
                "# Looking at the model to see the name of the different layers\n",
                "print(gpt2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@jaxtyped(typechecker=typechecked)\n",
                "def plot_logit_lens(\n",
                "    per_layer_logits: Float[Tensor, \"layer nb_tokens vocab=50257\"],\n",
                "    per_layer_token_to_show: Int[Tensor, \"layer nb_tokens\"],\n",
                "    input_ids: Int[Tensor, \"batch=1 nb_tokens\"],\n",
                "    tokenizer=gpt2_tokenizer,\n",
                "):\n",
                "    plot_logit_lens_low_level(\n",
                "        per_layer_logits.detach(),\n",
                "        per_layer_token_to_show.detach(),\n",
                "        per_layer_logits.softmax(dim=-1).detach(),\n",
                "        tokenizer,\n",
                "        # Hack: add the end-of-text token to avoid crash in _plot_logit_lens\n",
                "        input_ids=torch.cat([input_ids, torch.tensor([[50256]])], dim=1),\n",
                "        # input_ids=input_ids,\n",
                "        start_ix=0,\n",
                "        layer_names=None,\n",
                "        probs=True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"Happy birthday to you, happy birthday to\"\n",
                "\n",
                "...  # Implement logit lens\n",
                "\n",
                "plot_logit_lens(\n",
                "    per_layer_logits,\n",
                "    per_layer_token_to_show,\n",
                "    input_ids,\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "  <summary>Hint: steps</summary>\n",
                "\n",
                "```python\n",
                "# 1. Define a hook that stores the output of the layer\n",
                "# 2. Add the hook to each layer, use partial to pass the layer index\n",
                "# 3. Run the model on the input, then remove the hooks\n",
                "# 4. For each layer\n",
                "# 4.1. Normalize the output using the final layer norm\n",
                "# 4.2. Compute the word distribution using the word embeddings\n",
                "# 4.3. Find the most likely token\n",
                "```\n",
                "</details>\n",
                "\n",
                "<details>\n",
                "  <summary>Click to see the solution</summary>\n",
                "\n",
                "```python\n",
                "n_layers_gpt = len(gpt2.base_model.h)\n",
                "outputs = [None] * n_layers_gpt\n",
                "\n",
                "input_ids = gpt2_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
                "\n",
                "# 1. Define a hook that stores the output of the layer\n",
                "def memorize_output_layer_hook(self, input, output, layer):\n",
                "    # Remark: the `global` keyword is not necessary, because we are modifying \n",
                "    # the content of a list. `global` would have been necessary if we were\n",
                "    # overwriting the list (i.e. outputs = ...)\n",
                "    outputs[layer] = output[0].detach()\n",
                "\n",
                "\n",
                "# 2. Add the hook to each layer, use partial to pass the layer index\n",
                "handles = [\n",
                "    gpt_block.register_forward_hook(partial(memorize_output_layer_hook, layer=layer))\n",
                "    for layer, gpt_block in enumerate(gpt2.base_model.h)\n",
                "]\n",
                "\n",
                "# 3. Run the model on the input, then remove the hooks\n",
                "try:\n",
                "    with torch.no_grad():\n",
                "        gpt2(input_ids)\n",
                "finally:\n",
                "    for handle in handles:\n",
                "        handle.remove()\n",
                "\n",
                "\n",
                "last_layer_norm = gpt2.base_model.ln_f\n",
                "word_embeddings = gpt2.base_model.wte.weight\n",
                "\n",
                "\n",
                "per_layer_token_to_show = []\n",
                "per_layer_logits = []\n",
                "\n",
                "# 4. For each layer\n",
                "for layer, output in enumerate(outputs):\n",
                "    # 4.1. Normalize the output using the final layer norm\n",
                "    normalized_output = last_layer_norm(output)\n",
                "\n",
                "    # 4.2. Compute the word distribution using the word embeddings\n",
                "    word_distribution = einops.einsum(\n",
                "        normalized_output, word_embeddings, \n",
                "        \"batch token d_model, vocab d_model -> token vocab\"\n",
                "    )\n",
                "    # 4.3. Find the most likely token\n",
                "    best_token = torch.argmax(word_distribution, dim=-1)\n",
                "    output_text = gpt2_tokenizer.decode(best_token)\n",
                "    print(output_text)\n",
                "    \n",
                "    per_layer_token_to_show.append(best_token) \n",
                "    per_layer_logits.append(word_distribution) \n",
                "    \n",
                "\n",
                "per_layer_logits = torch.stack(per_layer_logits)\n",
                "per_layer_token_to_show = torch.stack(per_layer_token_to_show)\n",
                "```\n",
                "</details>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Probing\n",
                "\n",
                "Look at this youtube video to introduce you to probing: https://www.youtube.com/watch?v=HJn-OTNLnoE\n",
                "\n",
                "We will use the fetch_20newsgroups dataset, and we will classify the news according to which journal they come from.\n",
                "We will try to implement a small probe and analyse each layer of GPT-2. Which layer contains most of the information we are insterested with?\n",
                "\n",
                "Questions: \n",
                "- What is your strategy to use the internal states of gpt-2 as features for classification?\n",
                "- Propose 2 other strategies that won't work.\n",
                "- Try to predict the score of each layer at classifying the fetch_20newsgroups dataset.\n",
                "- Implement and check your prediction.\n",
                "\n",
                "Bonu read this: https://arxiv.org/pdf/1704.01444.pdf\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import fetch_20newsgroups\n",
                "\n",
                "categories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n",
                "\n",
                "twenty_train = fetch_20newsgroups(\n",
                "    subset=\"train\", categories=categories, shuffle=True, random_state=42\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(twenty_train.data[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "twenty_train.target[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "twenty_train.target_names[twenty_train.target[0]]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(twenty_train.target)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "torch.cuda.empty_cache()\n",
                "embed_dim = 768\n",
                "n_layers = 12\n",
                "N = len(twenty_train.data)\n",
                "\n",
                "# We only take the last token\n",
                "# If you do not put everything in a single array, the memory explodes\n",
                "hidden_states = np.zeros((N, n_layers, embed_dim))\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "activations = np.zeros((N, n_layers, embed_dim))\n",
                "\n",
                "gpt2 = gpt2.to(device)\n",
                "gpt2.eval()\n",
                "\n",
                "\n",
                "# Fill hidden_states\n",
                "..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.save(\"hidden_states\", hidden_states)\n",
                "hidden_states.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# If you do not have a gpu, use this line.\n",
                "# hidden_states = np.load(\"hidden_states.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the documentation of sklearn and use those imports to score each layer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.pipeline import make_pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "scores = []\n",
                "for layer in range(n_layers):\n",
                "    ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.plot(list(range(len(scores))), scores)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Activation Atlas"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "With out dataset we can also try to minimally reproduce the activation atlas paper:\n",
                "https://openai.com/blog/introducing-activation-atlases/\n",
                "\n",
                "\n",
                "Questions:\n",
                "- How to implement dimensionality reduction?\n",
                "- Install umap-learn.\n",
                "- Visualize the umap of the best previously selected layer. Comment.\n",
                "\n",
                "Bonus: Plot the sentences alongside the point in the UMAP plot. Chack that everything makes sense."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install umap-learn -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "%matplotlib inline\n",
                "\n",
                "# Dimension reduction and clustering library\n",
                "import umap as umap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose the best_layer\n",
                "best_layer = ...\n",
                "X = hidden_states[:, best_layer, :]\n",
                "y = twenty_train.target\n",
                "\n",
                "standard_embedding = umap.UMAP(random_state=42).fit_transform(X_train)\n",
                "plt.scatter(\n",
                "    standard_embedding[:, 0], standard_embedding[:, 1], c=y.astype(int), s=0.1, cmap=\"Spectral\"\n",
                ");"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
