{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Only execute on Colab\n",
                "try:\n",
                "    import google.colab\n",
                "\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    # Install packages\n",
                "    %pip install einops\n",
                "    %pip install transformer_lens\n",
                "\n",
                "    # Code to make sure output widgets display\n",
                "    from google.colab import output\n",
                "\n",
                "    output.enable_custom_widget_manager()\n",
                "\n",
                "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
                "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/transformer_interp/*'\n",
                "    !mv --no-clobber ML4G-2.0-master/workshops/transformer_interp/* .\n",
                "    !rm -r ML4G-2.0-master\n",
                "\n",
                "    print(\"Imports & installations complete!\")\n",
                "\n",
                "else:\n",
                "    from IPython import get_ipython\n",
                "\n",
                "    ipython = get_ipython()\n",
                "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
                "    ipython.run_line_magic(\"autoreload\", \"2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/transformer_interp/exercise_hard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "## Introduction\n",
                "\n",
                "In this exercise, we will take a closer look at some of the methods that deal with features in transformers. In particular, we will implement a part of [logit lens](https://arxiv.org/pdf/2303.08112.pdf) and [activation addition](https://arxiv.org/pdf/2308.10248.pdf).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
                "\n",
                "import torch as t\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from einops import repeat\n",
                "\n",
                "# now, we import pca\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "from transformer_lens import HookedTransformer\n",
                "\n",
                "t.set_grad_enabled(False)\n",
                "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
                "\n",
                "from tests import test_logit_lens_analysis, test_prompt_to_residual_stream_activations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "## Loading and Running Models\n",
                "\n",
                "TransformerLens comes loaded with more than 40 open-source GPT-style models. You can load any of them with HookedTransformer.from_pretrained(MODEL_NAME). For this demo notebook, we'll look at GPT-2 Small, an 80M parameter model. See the Available Models section for info on the rest.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gpt2_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
                "gpt2_tokenizer = gpt2_small.tokenizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Caching all Activations\n",
                "\n",
                "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out, on the first sentence from the GPT-2 paper.\n",
                "\n",
                "<details>\n",
                "<summary>Aside - a note on <code>remove_batch_dim</code></summary>\n",
                "\n",
                "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it. \n",
                "\n",
                "`gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
                "gpt2_tokens = gpt2_small.to_tokens(gpt2_text)\n",
                "gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here, `cache` is a `Cache` object, which acts as a dictionary containing all the activations in the model. The keys are the names of the layers, and the values are the activations. The activations are represented as `torch.Tensor`s, allowing for a wide range of operations to be performed on them, such as plotting, saving them to disk, etc.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for position in gpt2_cache.keys():\n",
                "    print(f\"Layer {position} has shape {gpt2_cache[position].shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here is an example of how one could unembed the residual stream from the cache, should that ever prove to be useful:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "residual_stream_activation = gpt2_cache[\"blocks.11.ln2.hook_normalized\"]\n",
                "unembedded_output = gpt2_small.unembed(residual_stream_activation[None])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Logit Lens\n",
                "Logit Lens is a method to look into the processes going on inside a transformer model. Normally, we just take the unembedding of the final layer of the model as its prediction of the next token. The idea behind Logit Lens is to look at the unembeddings of all the layers of the model and see how the model's best guess of what the next token is changes as we go through the layers.\n",
                "When looking at the unembeddings of the residual stream, we visualize two things:\n",
                "- What is the most likely token at each layer.\n",
                "- How far does the prediction of this layer deviate from the final prediction of the model (as measured by the KL divergence).\n",
                "<details>\n",
                "<summary>KL Divergence</summary>\n",
                "       \n",
                "The KL divergence is a measure of how much two probability distributions differ. In this case, we are comparing the distribution of the final layer's unembedding to the distribution of the unembedding of the layer we are looking at.\n",
                "Mathematically, the KL divergence is defined as:\n",
                "$$\n",
                "D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\n",
                "$$\n",
                "Here \\(P\\) and \\(Q\\) are the two distributions we are comparing, and \\(i\\) is the index of the token we are looking at.\n",
                "The way that the KL divergence is implemented in torch can be quite confusing:\n",
                "```python\n",
                "torch.nn.functional.kl_div(p, q)\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_logit_lens(tokenizer, token_list, top_tokens, kl_div):\n",
                "    seq_len, layers = top_tokens.shape\n",
                "    # Create a figure and axis\n",
                "    fig, ax = plt.subplots(figsize=(10, 8))\n",
                "    # Create a color map based on the KL divergence values\n",
                "    cmap = plt.cm.Reds\n",
                "    norm = plt.Normalize(vmin=t.min(kl_div), vmax=t.max(kl_div))\n",
                "    # Set the x-axis limits to include the last row\n",
                "    ax.set_xlim(0, seq_len)\n",
                "    ax.set_ylim(0, layers)\n",
                "    # Create a grid of boxes\n",
                "    for i in range(seq_len):\n",
                "        for j in range(layers):\n",
                "            # Get the top token and KL divergence value for the current box\n",
                "            token_id = top_tokens[i, j]\n",
                "            kl_value = kl_div[i, j]\n",
                "            # Decode the token ID to get the corresponding token\n",
                "            token = tokenizer.decode([token_id])\n",
                "            # Set the color of the box based on the KL divergence value\n",
                "            color = cmap(norm(kl_value))\n",
                "            # Create a rectangle patch for the box\n",
                "            rect = plt.Rectangle((i, j), 1, 1, edgecolor=\"black\", facecolor=color)\n",
                "            ax.add_patch(rect)\n",
                "            # Add the decoded token text to the box\n",
                "            ax.text(i + 0.5, j + 0.5, token, ha=\"center\", va=\"center\", fontsize=8)\n",
                "    # Set the x-axis labels (decoded tokens from the token list)\n",
                "    ax.set_xticks(np.arange(seq_len) + 0.5)\n",
                "    ax.set_xticklabels(\n",
                "        [tokenizer.decode([token_id]) for token_id in token_list], rotation=45, ha=\"right\"\n",
                "    )\n",
                "    # Set the y-axis labels (shifted token list)\n",
                "    ax.set_yticks(np.arange(layers))\n",
                "    ax.set_yticklabels([str(i) for i in range(layers)], fontsize=8)\n",
                "    # Set the plot title and labels\n",
                "    ax.set_title(\"Token Grid with KL Divergence\")\n",
                "    ax.set_xlabel(\"Sequence\")\n",
                "    ax.set_ylabel(\"Layers\")\n",
                "    # Add a color bar to represent the KL divergence values\n",
                "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
                "    sm.set_array([])\n",
                "    cbar = plt.colorbar(sm, ax=ax)\n",
                "    cbar.set_label(\"KL Divergence\")\n",
                "    # Adjust the plot layout and display it\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now it is your turn to implement the analysis, underlying logit lens.\n",
                " - retrieve the residual stream activations, given the prompt\n",
                " - unembedd the residual stream activations, and get the probabilityd distribution of the next token at each layer\n",
                " - make a tensor of the the most likely token at each layer, for each token\n",
                " - calculate the KL divergence between the final prediction, and the prediction at each layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def logit_lens_analysis(input_prompt: str):\n",
                "    \"\"\"\n",
                "    Performs logit lens analysis on the given input prompt.\n",
                "\n",
                "    Args:\n",
                "        input_prompt (str): The input prompt to analyze.\n",
                "\n",
                "    Returns:\n",
                "        tuple: A tuple containing the following:\n",
                "            - tokens (list): The tokenized input prompt.\n",
                "            - top_unembedded_residual_stream_caches (torch.Tensor): The top unembedded residual stream caches.\n",
                "            - kl_divergences (torch.Tensor): The KL divergences between the unembedded residual stream caches and the final logits.\n",
                "    \"\"\"\n",
                "    # Tokenize the input prompt\n",
                "    tokens = gpt2_small.to_tokens(input_prompt)\n",
                "\n",
                "    # Run the model with caching\n",
                "\n",
                "    # Get the number of layers in the model\n",
                "\n",
                "    # Extract the residual stream caches from the cache\n",
                "\n",
                "    # Stack the residual stream caches along the layer dimension\n",
                "\n",
                "    # Unembed the residual stream caches\n",
                "\n",
                "    # Transform the unembedded residual stream caches into log probabilities\n",
                "    # Get the top unembedded residual stream caches\n",
                "\n",
                "    # Create a tensor of the same shape as the final logits of the model at each position\n",
                "\n",
                "    # Convert the final logits to probabilities\n",
                "\n",
                "    # Calculate the KL divergences between the unembedded residual stream caches and the final logits\n",
                "\n",
                "    return tokens, top_unembedded_residual_stream_caches, kl_divergences\n",
                "\n",
                "\n",
                "test_logit_lens_analysis(logit_lens_analysis, gpt2_small)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "\n",
                "\n",
                "def logit_lens_analysis(input_prompt: str):\n",
                "    \"\"\"\n",
                "    Performs logit lens analysis on the given input prompt.\n",
                "\n",
                "    Args:\n",
                "        input_prompt (str): The input prompt to analyze.\n",
                "\n",
                "    Returns:\n",
                "        tuple: A tuple containing the following:\n",
                "            - tokens (list): The tokenized input prompt.\n",
                "            - top_unembedded_residual_stream_caches (torch.Tensor): The top unembedded residual stream caches.\n",
                "            - kl_divergences (torch.Tensor): The KL divergences between the unembedded residual stream caches and the final logits.\n",
                "    \"\"\"\n",
                "    # Tokenize the input prompt\n",
                "    tokens = gpt2_small.to_tokens(input_prompt)\n",
                "\n",
                "    # Run the model with caching\n",
                "    final_logits, cache = gpt2_small.run_with_cache(tokens, remove_batch_dim=True)\n",
                "\n",
                "    # Get the number of layers in the model\n",
                "    num_layers = len(gpt2_small.blocks)\n",
                "\n",
                "    # Extract the residual stream caches from the cache\n",
                "    residual_stream_caches = [cache[f\"blocks.{i}.ln2.hook_normalized\"] for i in range(num_layers)]\n",
                "\n",
                "    # Stack the residual stream caches along the layer dimension\n",
                "    residual_stream_caches = t.stack(residual_stream_caches, dim=1)\n",
                "\n",
                "    # Unembed the residual stream caches\n",
                "    unembedded_residual_stream_caches = gpt2_small.unembed(residual_stream_caches)\n",
                "\n",
                "    # Transform the unembedded residual stream caches into log probabilities\n",
                "    unembedded_residual_stream_caches = unembedded_residual_stream_caches.log_softmax(dim=-1)\n",
                "\n",
                "    # Get the top unembedded residual stream caches\n",
                "    top_unembedded_residual_stream_caches = t.argmax(unembedded_residual_stream_caches, dim=-1)\n",
                "\n",
                "    # Create a tensor of the same shape as the final logits of the model at each position\n",
                "    final_logits = repeat(final_logits[0], 'nb_tokens vocab -> nb_tokens layer vocab', layer=num_layers)\n",
                "\n",
                "    # Convert the final logits to probabilities\n",
                "    final_probs = final_logits.softmax(dim=-1)\n",
                "\n",
                "    # Calculate the KL divergences between the unembedded residual stream caches and the final logits\n",
                "    kl_divergences = t.nn.functional.kl_div(unembedded_residual_stream_caches, final_probs, reduction='none').sum(-1)\n",
                "\n",
                "    return tokens, top_unembedded_residual_stream_caches, kl_divergences\n",
                "test_logit_lens_analysis(logit_lens_analysis, gpt2_small)\n",
                "\n",
                "\n",
                "```\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"James Potter went to school. James\"\n",
                "tokens, top_unebedded_residual_stream_caches, kl_divergences = logit_lens_analysis(text)\n",
                "plot_logit_lens(gpt2_tokenizer, tokens[0], top_unebedded_residual_stream_caches, kl_divergences)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = \"Alice and Bob went to the store. Alice gave the milk to\"\n",
                "tokens, top_unebedded_residual_stream_caches, kl_divergences = logit_lens_analysis(text)\n",
                "plot_logit_lens(gpt2_tokenizer, tokens[0], top_unebedded_residual_stream_caches, kl_divergences)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Do you see anything interesting in these diagrams?</summary>\n",
                "In which layer does the model start to predict \"Bob\" vs \"Potter\"? Do you have any idea why?\n",
                "Try to play around with different prompts and see what happens.\n",
                "</details>\n",
                "\n",
                "## Linear Separability of the Activations\n",
                "The second thing we will look at is the linear separability of the activations. This is a measure of how well the activations of a layer can be used to predict what is going on in the transformer.\n",
                "Here, we have a dataset with different prompts, corresponding to different simple tasks, that a transformer might be doing:\n",
                "  - inverting words to their opposite\n",
                "  - translating words into German\n",
                "This linear separability is necessary for linear probes and activation addition to work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "steering_prompt_pairs = [\n",
                "    [\"fast : slow\", \"fast : schnell\"],\n",
                "    [\"cold : warm\", \"cold : kalt\"],\n",
                "    [\"big : small\", \"big : groß\"],\n",
                "    [\"happy : sad\", \"happy : froh\"],\n",
                "    [\"light : dark\", \"light : hell\"],\n",
                "    [\"loud : quiet\", \"loud : laut\"],\n",
                "    [\"new : old\", \"new : neu\"],\n",
                "    [\"rich : poor\", \"rich : reich\"],\n",
                "    [\"strong : weak\", \"strong : stark\"],\n",
                "    [\"young : old\", \"young : jung\"],\n",
                "    [\"clean : dirty\", \"clean : rein\"],\n",
                "    [\"dry : wet\", \"dry : trocken\"],\n",
                "    [\"full : empty\", \"full : voll\"],\n",
                "    [\"good : bad\", \"good : gut\"],\n",
                "    [\"hard : soft\", \"hard : hart\"],\n",
                "    [\"heavy : light\", \"heavy : schwer\"],\n",
                "    [\"high : low\", \"high : hoch\"],\n",
                "    [\"long : short\", \"long : lang\"],\n",
                "    [\"right : wrong\", \"right : richtig\"],\n",
                "    [\"rough : smooth\", \"rough : rau\"],\n",
                "    [\"sharp : dull\", \"sharp : scharf\"],\n",
                "    [\"thick : thin\", \"thick : dick\"],\n",
                "    [\"wide : narrow\", \"wide : breit\"],\n",
                "    [\"brave : cowardly\", \"brave : mutig\"],\n",
                "    [\"calm : angry\", \"calm : ruhig\"],\n",
                "    [\"clever : stupid\", \"clever : klug\"],\n",
                "    [\"cruel : kind\", \"cruel : grausam\"],\n",
                "    [\"deep : shallow\", \"deep : tief\"],\n",
                "    [\"early : late\", \"early : früh\"],\n",
                "    [\"easy : hard\", \"easy : leicht\"],\n",
                "    [\"fake : real\", \"fake : falsch\"],\n",
                "    [\"far : near\", \"far : weit\"],\n",
                "    [\"first : last\", \"first : erster\"],\n",
                "    [\"free : captive\", \"free : frei\"],\n",
                "    [\"front : back\", \"front : vorne\"],\n",
                "    [\"great : terrible\", \"great : toll\"],\n",
                "    [\"hot : cold\", \"hot : heiß\"],\n",
                "    [\"large : small\", \"large : groß\"],\n",
                "    [\"left : right\", \"left : links\"],\n",
                "    [\"loose : tight\", \"loose : locker\"],\n",
                "    [\"lost : found\", \"lost : verloren\"],\n",
                "    [\"low : high\", \"low : niedrig\"],\n",
                "    [\"major : minor\", \"major : wichtig\"],\n",
                "    [\"many : few\", \"many : viele\"],\n",
                "    [\"open : closed\", \"open : offen\"],\n",
                "    [\"past : future\", \"past : vergangen\"],\n",
                "    [\"peace : war\", \"peace : frieden\"],\n",
                "    [\"quick : slow\", \"quick : schnell\"],\n",
                "    [\"raw : cooked\", \"raw : roh\"],\n",
                "    [\"safe : dangerous\", \"safe : sicher\"],\n",
                "    [\"same : different\", \"same : gleich\"],\n",
                "    [\"short : tall\", \"short : kurz\"],\n",
                "    [\"sick : healthy\", \"sick : krank\"],\n",
                "    [\"simple : complex\", \"simple : einfach\"],\n",
                "    [\"single : married\", \"single : ledig\"],\n",
                "    [\"slow : fast\", \"slow : langsam\"],\n",
                "    [\"small : big\", \"small : klein\"],\n",
                "    [\"smart : dumb\", \"smart : klug\"],\n",
                "    [\"soft : hard\", \"soft : weich\"],\n",
                "    [\"sour : sweet\", \"sour : sauer\"],\n",
                "    [\"start : end\", \"start : anfang\"],\n",
                "    [\"stop : go\", \"stop : halt\"],\n",
                "    [\"sweet : sour\", \"sweet : süß\"],\n",
                "    [\"tall : short\", \"tall : groß\"],\n",
                "    [\"tame : wild\", \"tame : zahm\"],\n",
                "    [\"thin : fat\", \"thin : dünn\"],\n",
                "    [\"top : bottom\", \"top : oben\"],\n",
                "    [\"tough : tender\", \"tough : zäh\"],\n",
                "    [\"true : false\", \"true : wahr\"],\n",
                "    [\"ugly : beautiful\", \"ugly : hässlich\"],\n",
                "    [\"urban : rural\", \"urban : städtisch\"],\n",
                "    [\"used : new\", \"used : gebraucht\"],\n",
                "    [\"useful : useless\", \"useful : nützlich\"],\n",
                "    [\"valid : invalid\", \"valid : gültig\"],\n",
                "    [\"vast : tiny\", \"vast : riesig\"],\n",
                "    [\"vegan : carnivore\", \"vegan : vegan\"],\n",
                "    [\"visible : invisible\", \"visible : sichtbar\"],\n",
                "    [\"vital : trivial\", \"vital : wichtig\"],\n",
                "    [\"vivid : dull\", \"vivid : lebhaft\"],\n",
                "    [\"warm : cool\", \"warm : warm\"],\n",
                "    [\"waste : save\", \"waste : abfall\"],\n",
                "    [\"weak : strong\", \"weak : schwach\"],\n",
                "    [\"wealth : poverty\", \"wealth : reichtum\"],\n",
                "    [\"weird : normal\", \"weird : seltsam\"],\n",
                "    [\"wet : dry\", \"wet : nass\"],\n",
                "    [\"white : black\", \"white : weiß\"],\n",
                "    [\"whole : part\", \"whole : ganz\"],\n",
                "    [\"wide : narrow\", \"wide : breit\"],\n",
                "    [\"wild : tame\", \"wild : wild\"],\n",
                "    [\"win : lose\", \"win : sieg\"],\n",
                "    [\"wise : foolish\", \"wise : weise\"],\n",
                "    [\"woman : man\", \"woman : frau\"],\n",
                "    [\"work : play\", \"work : arbeit\"],\n",
                "    [\"wrong : right\", \"wrong : falsch\"],\n",
                "    [\"yes : no\", \"yes : ja\"],\n",
                "    [\"young : old\", \"young : jung\"],\n",
                "    [\"zero : one\", \"zero : null\"],\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this dataset, the relevant token, where the task becomes clear is the first token after prompt.\n",
                "For creating steering vectors for example, it is important to average over those tokens, that carrie the information, that one wants to infuse into the network.\n",
                "\n",
                "Now it is your turn to retrieve the relevant activations from the datasset of different tasks:\n",
                " - find out the relevant token, where the task becomes clear (this is already implemented)\n",
                " - get the activations in the residual stream, for the relevant token for each layer\n",
                " - combine all of the activations into a single tensor, and return it\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prompt_to_residual_stream_activations(prompt, demarcation_token):\n",
                "    \"\"\"\n",
                "    Computes the residual stream activations for the first token after the demarkation token in the prompt.\n",
                "\n",
                "    Args:\n",
                "        prompt (str): The input prompt string.\n",
                "        demarcation_token (int): The token ID representing the demarcation point in the prompt.\n",
                "\n",
                "    Returns:\n",
                "        torch.Tensor: The residual stream activations for the final token in the prompt.\n",
                "    \"\"\"\n",
                "    # Tokenize the prompt and convert it to a list\n",
                "    tokens = gpt2_small.to_tokens(prompt)[0].tolist()\n",
                "\n",
                "    # Find the position of the demarcation token in the tokenized prompt\n",
                "    demarcation_position = tokens.index(demarcation_token)\n",
                "\n",
                "    # Truncate the tokens up to the demarcation token and include two additional tokens\n",
                "    tokens = tokens[: demarcation_position + 2]\n",
                "\n",
                "    # Convert the tokens to a tensor and add a batch dimension\n",
                "\n",
                "    # Run the model with the truncated tokens and obtain the final logits and cache\n",
                "    # Get the number of layers in the model\n",
                "\n",
                "    # Extract the residual stream activations from the cache for each layer\n",
                "    # Stack the residual stream activations along a new dimension\n",
                "\n",
                "    # Extract the residual stream activations for the final token\n",
                "\n",
                "    return final_token_residual_stream_caches\n",
                "\n",
                "\n",
                "test_prompt_to_residual_stream_activations(prompt_to_residual_stream_activations, gpt2_small)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Solution</summary>\n",
                "\n",
                "```python\n",
                "def prompt_to_residual_stream_activations(prompt, demarcation_token):\n",
                "    \"\"\"\n",
                "    Computes the residual stream activations for the first token after the demarkation token in the prompt.\n",
                "\n",
                "    Args:\n",
                "        prompt (str): The input prompt string.\n",
                "        demarcation_token (int): The token ID representing the demarcation point in the prompt.\n",
                "\n",
                "    Returns:\n",
                "        torch.Tensor: The residual stream activations for the final token in the prompt.\n",
                "    \"\"\"\n",
                "    # Tokenize the prompt and convert it to a list\n",
                "    tokens = gpt2_small.to_tokens(prompt)[0].tolist()\n",
                "\n",
                "    # Find the position of the demarcation token in the tokenized prompt\n",
                "    demarcation_position = tokens.index(demarcation_token)\n",
                "\n",
                "    # Truncate the tokens up to the demarcation token and include two additional tokens\n",
                "    tokens = tokens[:demarcation_position + 2]\n",
                "\n",
                "    # Convert the tokens to a tensor and add a batch dimension\n",
                "    tokens = t.tensor(tokens).unsqueeze(0)\n",
                "\n",
                "    # Run the model with the truncated tokens and obtain the final logits and cache\n",
                "    final_logits, cache = gpt2_small.run_with_cache(tokens, remove_batch_dim=True)\n",
                "\n",
                "    # Get the number of layers in the model\n",
                "    num_layers = len(gpt2_small.blocks)\n",
                "\n",
                "    # Extract the residual stream activations from the cache for each layer\n",
                "    residual_stream_caches = [cache[f\"blocks.{i}.ln2.hook_normalized\"] for i in range(num_layers)]\n",
                "\n",
                "    # Stack the residual stream activations along a new dimension\n",
                "    residual_stream_caches = t.stack(residual_stream_caches, dim=1)\n",
                "\n",
                "    # Extract the residual stream activations for the final token\n",
                "    final_token_residual_stream_caches = residual_stream_caches[-1]\n",
                "\n",
                "    return final_token_residual_stream_caches\n",
                "```\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def crate_activation_dataset(prompt_pairs, demarkation_string):\n",
                "    a_activations = []\n",
                "    b_activations = []\n",
                "\n",
                "    demarkation_token = gpt2_tokenizer.encode(demarkation_string)[-1]\n",
                "\n",
                "    for truth_prompt, lie_prompt in prompt_pairs:\n",
                "        a_activations.append(prompt_to_residual_stream_activations(truth_prompt, demarkation_token))\n",
                "        b_activations.append(prompt_to_residual_stream_activations(lie_prompt, demarkation_token))\n",
                "\n",
                "    a_activations = t.stack(a_activations)\n",
                "    b_activations = t.stack(b_activations)\n",
                "    return a_activations, b_activations\n",
                "\n",
                "\n",
                "demarkation_string = \" :\"\n",
                "a_activations, b_activations = crate_activation_dataset(steering_prompt_pairs, demarkation_string)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_activation_seperability(a_activations, b_activations, layer, pca_com1=0, pca_com2=1):\n",
                "    all_activations = t.cat([a_activations, b_activations], dim=0)\n",
                "    all_activations = all_activations[:, layer, :]\n",
                "    pca = PCA(n_components=2)\n",
                "    pca.fit(all_activations)\n",
                "    a_activations_pca = pca.transform(a_activations[:, layer, :])\n",
                "    b_activations_pca = pca.transform(b_activations[:, layer, :])\n",
                "    plt.scatter(\n",
                "        a_activations_pca[:, pca_com1], a_activations_pca[:, pca_com2], label=\"a\", alpha=0.5\n",
                "    )\n",
                "    plt.scatter(\n",
                "        b_activations_pca[:, pca_com1], b_activations_pca[:, pca_com2], label=\"b\", alpha=0.5\n",
                "    )\n",
                "    plt.legend()\n",
                "    plt.title(f\"Layer {layer} Activation Separability\")\n",
                "    plt.xlabel(f\"PCA Component {pca_com1}\")\n",
                "    plt.ylabel(f\"PCA Component {pca_com2}\")\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "plot_activation_seperability(a_activations, b_activations, layer=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary>Why do we plot the primary components instead of training a linear classifier?</summary>\n",
                "When the number of dimensions exceeds the number of datapoints, you can just find a linear separation for the data without any generalisation to other datapoints. This does not tell us anything about the different representations of the data in the network.\n",
                "By restricting ourselves to the dimensions along which the dataset has the most variance, we only see the most important dimensions along which the data varies.\n",
                "</details>\n",
                "<details>\n",
                "<summary>Play around with the layers and PCA components</summary>\n",
                "Why do you think the activations are more separable in some layers than in others?\n",
                "</details>"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
